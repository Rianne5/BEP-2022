{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4073cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c126562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf74f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import arxiv\n",
    "\n",
    "import urllib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fa09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbd0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #not sure if this one is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a044fd",
   "metadata": {},
   "source": [
    "# Preparation not always needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026ac470",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_name_Arxiv = \"pdftest\\Term_Expansion_and_FinBERT_fine_tuning_for_Hypernym_and_Synonym_Ranking_of_Financial_Terms.pdf\"\n",
    "loc_name_Anth = \"pdftest\\Anth-Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms.pdf\"\n",
    "\n",
    "\n",
    "doc_Arxiv = fitz.open(loc_name_Arxiv)\n",
    "doc_Anth = fitz.open(loc_name_Anth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a1c39",
   "metadata": {},
   "source": [
    "IN the case of blocks:  extractBLOCKS()\n",
    "\n",
    "    Textpage content as a list of text lines grouped by block. Each list items looks like this:\n",
    "\n",
    "    (x0, y0, x1, y1, \"lines in the block\", block_no, block_type)\n",
    "\n",
    "    The first four entries are the block’s bbox coordinates, block_type is 1 for an image block, 0 for text. block_no is the block sequence number. Multiple text lines are joined via line breaks.\n",
    "\n",
    "    For an image block, its bbox and a text line with some image meta information is included – not the image content.\n",
    "\n",
    "    This is a high-speed method with just enough information to output plain text in desired reading sequence.\n",
    "    Return type:\tlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed4f61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20193635\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (6,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = pd.read_csv('combined_table.csv').drop('Unnamed: 0', axis =1)\n",
    "test = table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2973683c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>address</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>language</th>\n",
       "      <th>month</th>\n",
       "      <th>note</th>\n",
       "      <th>number</th>\n",
       "      <th>pages</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>clean</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>10.18653/v1/2021.woah-1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1--5</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Exploiting Auxiliary Data for Offensive Langua...</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>Exploiting Auxiliary Data for Offensive Langua...</td>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>10.18653/v1/2021.woah-1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6--16</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v2</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>10.18653/v1/2021.woah-1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17--25</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>HateBERT  Retraining BERT for Abusive Language...</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v2</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Online</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>10.18653/v1/2021.woah-1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26--35</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>Memes in the Wild  Assessing the Generalizabil...</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ISBN address                                          booktitle  \\\n",
       "0  NaN  Online                                                NaN   \n",
       "1  NaN  Online  Proceedings of the 5th Workshop on Online Abus...   \n",
       "2  NaN  Online  Proceedings of the 5th Workshop on Online Abus...   \n",
       "3  NaN  Online  Proceedings of the 5th Workshop on Online Abus...   \n",
       "4  NaN  Online  Proceedings of the 5th Workshop on Online Abus...   \n",
       "\n",
       "                         doi journal language   month note  number   pages  \\\n",
       "0                        NaN     NaN      NaN  August  NaN     NaN     NaN   \n",
       "1  10.18653/v1/2021.woah-1.1     NaN      NaN  August  NaN     NaN    1--5   \n",
       "2  10.18653/v1/2021.woah-1.2     NaN      NaN  August  NaN     NaN   6--16   \n",
       "3  10.18653/v1/2021.woah-1.3     NaN      NaN  August  NaN     NaN  17--25   \n",
       "4  10.18653/v1/2021.woah-1.4     NaN      NaN  August  NaN     NaN  26--35   \n",
       "\n",
       "                                   publisher  \\\n",
       "0  Association for Computational Linguistics   \n",
       "1  Association for Computational Linguistics   \n",
       "2  Association for Computational Linguistics   \n",
       "3  Association for Computational Linguistics   \n",
       "4  Association for Computational Linguistics   \n",
       "\n",
       "                                               title  \\\n",
       "0  Proceedings of the 5th Workshop on Online Abus...   \n",
       "1  Exploiting Auxiliary Data for Offensive Langua...   \n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...   \n",
       "\n",
       "                                      url  volume  year  \\\n",
       "0  https://aclanthology.org/2021.woah-1.0     NaN  2021   \n",
       "1  https://aclanthology.org/2021.woah-1.1     NaN  2021   \n",
       "2  https://aclanthology.org/2021.woah-1.2     NaN  2021   \n",
       "3  https://aclanthology.org/2021.woah-1.3     NaN  2021   \n",
       "4  https://aclanthology.org/2021.woah-1.4     NaN  2021   \n",
       "\n",
       "                                               clean  \\\n",
       "0  Proceedings of the 5th Workshop on Online Abus...   \n",
       "1  Exploiting Auxiliary Data for Offensive Langua...   \n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "3  HateBERT  Retraining BERT for Abusive Language...   \n",
       "4  Memes in the Wild  Assessing the Generalizabil...   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "0                                                 NF   \n",
       "1                                                 NF   \n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "0                                NaN                        NaN   \n",
       "1                                NaN                        NaN   \n",
       "2  http://arxiv.org/pdf/2106.07505v2  2021-06-14 15:34:37+00:00   \n",
       "3  http://arxiv.org/pdf/2010.12472v2  2020-10-23 15:14:14+00:00   \n",
       "4  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "\n",
       "                        result_Arxiv  \n",
       "0                                NaN  \n",
       "1                                NaN  \n",
       "2  http://arxiv.org/abs/2106.07505v2  \n",
       "3  http://arxiv.org/abs/2010.12472v2  \n",
       "4  http://arxiv.org/abs/2107.04313v1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da6ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN               5\n",
       "address            0\n",
       "booktitle          1\n",
       "doi                1\n",
       "journal            5\n",
       "language           5\n",
       "month              0\n",
       "note               5\n",
       "number             5\n",
       "pages              1\n",
       "publisher          0\n",
       "title              0\n",
       "url                0\n",
       "volume             5\n",
       "year               0\n",
       "clean              0\n",
       "title_Arxiv        0\n",
       "pdf_url_Arxiv      2\n",
       "published_Arxiv    2\n",
       "result_Arxiv       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874eba7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ISBN', 'address', 'booktitle', 'doi', 'journal', 'language', 'month',\n",
       "       'note', 'number', 'pages', 'publisher', 'title', 'url', 'volume',\n",
       "       'year', 'clean', 'title_Arxiv', 'pdf_url_Arxiv', 'published_Arxiv',\n",
       "       'result_Arxiv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b130ce4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v1</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v1</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...  2021  August   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "3  https://aclanthology.org/2021.woah-1.3   \n",
       "4  https://aclanthology.org/2021.woah-1.4   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "2  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
       "3  http://arxiv.org/pdf/2010.12472v1  2020-10-23 15:14:14+00:00   \n",
       "4  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "\n",
       "                        result_Arxiv  \n",
       "2  http://arxiv.org/abs/2106.07505v2  \n",
       "3  http://arxiv.org/abs/2010.12472v2  \n",
       "4  http://arxiv.org/abs/2107.04313v1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = prepare_table(test)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6b730e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pdf sim/arx-HateBERT: Retraining BERT for Abusive Language Detection in English.pdf',\n",
       " <http.client.HTTPMessage at 0x26f9c1e3dc0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(tab['pdf_url_Arxiv'][3], 'pdf sim/arx-'+tab['title'][3]+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d7c52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20100/3881673951.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnot_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embed' is not defined"
     ]
    }
   ],
   "source": [
    "not_sample, sample = pdf_sim(embed, tab,max_iter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8e56fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v1</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "2  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
       "\n",
       "                        result_Arxiv  \n",
       "2  http://arxiv.org/abs/2106.07505v2  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bfd315bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "      <th>Succes</th>\n",
       "      <th>w_Anth</th>\n",
       "      <th>w_Arxiv</th>\n",
       "      <th>w_both</th>\n",
       "      <th>pages_Anth</th>\n",
       "      <th>pages_Arxiv</th>\n",
       "      <th>cosin</th>\n",
       "      <th>len_blocks</th>\n",
       "      <th>len_Anth</th>\n",
       "      <th>len_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>4840.0</td>\n",
       "      <td>4801.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v1</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>3763.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.960287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "4  Memes in the Wild: Assessing the Generalizabil...  2021  August   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "4  https://aclanthology.org/2021.woah-1.4   \n",
       "3  https://aclanthology.org/2021.woah-1.3   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "4  Memes in the Wild: Assessing the Generalizabil...   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "4  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "3  http://arxiv.org/pdf/2010.12472v1  2020-10-23 15:14:14+00:00   \n",
       "\n",
       "                        result_Arxiv  Succes  w_Anth  w_Arxiv  w_both  \\\n",
       "4  http://arxiv.org/abs/2107.04313v1     1.0  4865.0   4840.0  4801.0   \n",
       "3  http://arxiv.org/abs/2010.12472v2     1.0  4914.0   3763.0    52.0   \n",
       "\n",
       "   pages_Anth  pages_Arxiv     cosin  len_blocks  len_Anth  len_Arxiv  \n",
       "4        10.0         10.0  0.999624         1.0      12.0        3.0  \n",
       "3         9.0          8.0  0.960287         1.0      10.0        8.0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "78470341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    Modeling Profanity and Hate Speech in Social M...\n",
      "Name: title, dtype: object\n",
      "done url\n",
      "start load\n",
      "start blocks\n",
      "remove_bar skipped\n",
      "elem_both_list\n",
      "length same element:  207\n",
      "Join blocks\n",
      "start embed/sim\n",
      "start sim\n",
      "Length Anthology: 14\n",
      "Length Arxiv:6\n",
      "Updated length Anthology: 46244\n",
      "Updated length Arxiv:45732\n",
      "                                                Anth  \\\n",
      "0  Proceedings of the Fifth Workshop on Online Ab...   \n",
      "\n",
      "                                               Arxiv  \n",
      "0  Modeling Profanity and Hate Speech in Social M...  \n",
      "start cosin\n",
      "0.999282 7037 6950 1 14 6\n",
      "                                               title  year   month  \\\n",
      "2  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
      "\n",
      "                                      url  \\\n",
      "2  https://aclanthology.org/2021.woah-1.2   \n",
      "\n",
      "                                         title_Arxiv  \\\n",
      "2  Modeling Profanity and Hate Speech in Social M...   \n",
      "\n",
      "                       pdf_url_Arxiv            published_Arxiv  \\\n",
      "2  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
      "\n",
      "                        result_Arxiv  Succes  w_Anth  w_Arxiv  w_both  \\\n",
      "2  http://arxiv.org/abs/2106.07505v2     0.0  7037.0   6950.0  6306.0   \n",
      "\n",
      "   pages_Anth  pages_Arxiv     cosin  len_blocks  len_Anth  len_Arxiv  \n",
      "2        11.0         11.0  0.999282         1.0      14.0        6.0  \n"
     ]
    }
   ],
   "source": [
    "not_sample, sample = pdf_sim(embed=embed, tab_old = not_sample, result=sample,max_iter = 1, r_state=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e9f97001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, year, month, url, title_Arxiv, pdf_url_Arxiv, published_Arxiv, result_Arxiv]\n",
       "Index: []"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1bb3e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "      <th>Succes</th>\n",
       "      <th>w_Anth</th>\n",
       "      <th>w_Arxiv</th>\n",
       "      <th>w_both</th>\n",
       "      <th>pages_Anth</th>\n",
       "      <th>pages_Arxiv</th>\n",
       "      <th>cosin</th>\n",
       "      <th>len_blocks</th>\n",
       "      <th>len_Anth</th>\n",
       "      <th>len_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v1</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7037.0</td>\n",
       "      <td>6950.0</td>\n",
       "      <td>6306.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>4840.0</td>\n",
       "      <td>4801.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v1</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>3763.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.960287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "0  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
       "1  Memes in the Wild: Assessing the Generalizabil...  2021  August   \n",
       "2  HateBERT: Retraining BERT for Abusive Language...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "0  https://aclanthology.org/2021.woah-1.2   \n",
       "1  https://aclanthology.org/2021.woah-1.4   \n",
       "2  https://aclanthology.org/2021.woah-1.3   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "0  Modeling Profanity and Hate Speech in Social M...   \n",
       "1  Memes in the Wild: Assessing the Generalizabil...   \n",
       "2  HateBERT: Retraining BERT for Abusive Language...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "0  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
       "1  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "2  http://arxiv.org/pdf/2010.12472v1  2020-10-23 15:14:14+00:00   \n",
       "\n",
       "                        result_Arxiv  Succes  w_Anth  w_Arxiv  w_both  \\\n",
       "0  http://arxiv.org/abs/2106.07505v2     1.0  7037.0   6950.0  6306.0   \n",
       "1  http://arxiv.org/abs/2107.04313v1     1.0  4865.0   4840.0  4801.0   \n",
       "2  http://arxiv.org/abs/2010.12472v2     1.0  4914.0   3763.0    52.0   \n",
       "\n",
       "   pages_Anth  pages_Arxiv     cosin  len_blocks  len_Anth  len_Arxiv  \n",
       "0        11.0         11.0  0.999282         1.0      14.0        6.0  \n",
       "1        10.0         10.0  0.999624         1.0      12.0        3.0  \n",
       "2         9.0          8.0  0.960287         1.0      10.0        8.0  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c35b8",
   "metadata": {},
   "source": [
    "# End function \n",
    "---\n",
    "---\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "63791f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(n), pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2f456fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "n=pd.DataFrame()\n",
    "if type(n) ==pd.DataFrame:\n",
    "    print('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87188b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed, tab_old, result=None, max_iter=1, r_state = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9cd9b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "      <th>Succes</th>\n",
       "      <th>w_Anth</th>\n",
       "      <th>w_Arxiv</th>\n",
       "      <th>w_both</th>\n",
       "      <th>pages_Anth</th>\n",
       "      <th>pages_Arxiv</th>\n",
       "      <th>cosin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>4840.0</td>\n",
       "      <td>4801.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.999624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v1</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>3763.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.960287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "0  Memes in the Wild: Assessing the Generalizabil...  2021  August   \n",
       "1  HateBERT: Retraining BERT for Abusive Language...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "0  https://aclanthology.org/2021.woah-1.4   \n",
       "1  https://aclanthology.org/2021.woah-1.3   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "0  Memes in the Wild: Assessing the Generalizabil...   \n",
       "1  HateBERT: Retraining BERT for Abusive Language...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "0  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "1  http://arxiv.org/pdf/2010.12472v1  2020-10-23 15:14:14+00:00   \n",
       "\n",
       "                        result_Arxiv  Succes  w_Anth  w_Arxiv  w_both  \\\n",
       "0  http://arxiv.org/abs/2107.04313v1     NaN  4865.0   4840.0  4801.0   \n",
       "1  http://arxiv.org/abs/2010.12472v2     NaN  4914.0   3763.0    52.0   \n",
       "\n",
       "   pages_Anth  pages_Arxiv     cosin  \n",
       "0        10.0         10.0  0.999624  \n",
       "1         9.0          8.0  0.960287  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1.merge(try1,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "994317a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v1</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "2  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
       "\n",
       "                        result_Arxiv  \n",
       "2  http://arxiv.org/abs/2106.07505v2  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0429fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfe996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim(embed, loc_name_Arxiv, loc_name_Anth):\n",
    "#sometimes download not available. Then download works but does not give a good pdf so reading does not work\n",
    "#         embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        print('start load')\n",
    "        doc_Arxiv = fitz.open(loc_name_Arxiv)\n",
    "        doc_Anth = fitz.open(loc_name_Anth)\n",
    "        \n",
    "        print('start blocks')\n",
    "        b_Anth = get_blocks(doc_Anth)\n",
    "        b_Arxiv = get_blocks(doc_Arxiv)\n",
    "        rect = (5,180,40,570)\n",
    "        remove_bar = doc_Arxiv.load_page(0).get_text(\"text\",clip=rect)\n",
    "        try:\n",
    "            b_Arxiv.remove(remove_bar)\n",
    "        except:\n",
    "            print('Unable to remove bar from arxiv')\n",
    "    \n",
    "        elem_both_list = set(b_Anth)&set(b_Arxiv)\n",
    "        print('elem_both_list')\n",
    "        print(elem_both_list)\n",
    "        print('start join')\n",
    "        #join blocks\n",
    "#         p_Anth = join_diff(b_Anth, elem_both_list)\n",
    "#         p_Arxiv = join_diff(b_Arxiv, elem_both_list)\n",
    "        p_Anth = join_blocks(b_Anth, elem_both_list)\n",
    "        p_Arxiv = join_blocks(b_Arxiv, elem_both_list)\n",
    "    \n",
    "        print('start embed')\n",
    "        #similarity\n",
    "            \n",
    "        result = sim(p_Anth,p_Arxiv,embed)\n",
    "        return result, b_Arxiv, b_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be9aaa8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start load\n",
      "start blocks\n",
      "Unable to remove bar from arxiv\n",
      "elem_both_list\n",
      "{'1\\nIntroduction\\n', 'References\\n', 'Abstract\\n'}\n",
      "start join\n",
      "start embed\n",
      "start sim\n",
      "Length Anthology: 4\n",
      "Length Arxiv:4\n",
      "                                                Anth  \\\n",
      "0  Contributions of Transformer Attention Heads i...   \n",
      "1  Abstract This paper studies the relative impor...   \n",
      "2  1 Introduction Prior research on mono-lingual ...   \n",
      "3  References Xilun Chen, Ahmed Hassan Awadallah,...   \n",
      "\n",
      "                                               Arxiv  \n",
      "0  Proceedings of the 2021 Conference on Empirica...  \n",
      "1  Abstract The impressive capabilities of recent...  \n",
      "2  1 Introduction Recent text generation models (...  \n",
      "3  References David Ifeoluwa Adelani, Haotian Mai...  \n"
     ]
    }
   ],
   "source": [
    "test_sim_result = test_sim(embed, 'pdf Anth/Anth-Artificial Text Detection via Examining the Topology of Attention Maps.pdf',\n",
    "                          'pdf Arxiv/2108.08375v1.Contributions_of_Transformer_Attention_Heads_in_Multi_and_Cross_lingual_Tasks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b7e10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Arxiv = test_sim_result[1]\n",
    "b_Anth = test_sim_result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "843e90e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.617386, 7288, 8302)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "37ee6267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\\n'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rect = (5,180,40,570)\n",
    "doc_Arxiv.load_page(0).get_text(\"text\",clip=rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4bc83241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doc_Anth\n",
    "# b_Anth = get_blocks(doc_Anth)\n",
    "# b_Arxiv = get_blocks(doc_Arxiv)\n",
    "\n",
    "rect = (5,180,40,570)\n",
    "remove_bar = doc_Arxiv.load_page(0).get_text(\"text\",clip=rect)\n",
    "try:\n",
    "    b_Arxiv.remove(remove_bar)\n",
    "except:\n",
    "    print('Unable to remove bar from arxiv')\n",
    "                \n",
    "# elem_both_list = set(b_Anth)&set(b_Arxiv)\n",
    "# print('start join')\n",
    "#join blocks\n",
    "# p_Anth = join_blocks(b_Anth, elem_both_list)\n",
    "# p_Arxiv = join_blocks(b_Arxiv, elem_both_list)\n",
    "d_Anth = join_diff(b_Anth, elem_both_list)\n",
    "d_Arxiv = join_diff(b_Arxiv, elem_both_list)\n",
    "# print('start embed')\n",
    "\n",
    "#similarity\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "# res = sim(d_Anth,d_Arxiv,embedding=embed)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cf5a189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    http://arxiv.org/pdf/2106.07505v1\n",
       "3    http://arxiv.org/pdf/2010.12472v1\n",
       "4    http://arxiv.org/pdf/2107.04313v1\n",
       "Name: pdf_url_Arxiv, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab['pdf_url_Arxiv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ee40276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x26fec337880>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d39b9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>title_Arxiv</th>\n",
       "      <th>pdf_url_Arxiv</th>\n",
       "      <th>published_Arxiv</th>\n",
       "      <th>result_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>http://arxiv.org/pdf/2106.07505v1</td>\n",
       "      <td>2021-06-14 15:34:37+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2106.07505v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>HateBERT: Retraining BERT for Abusive Language...</td>\n",
       "      <td>http://arxiv.org/pdf/2010.12472v1</td>\n",
       "      <td>2020-10-23 15:14:14+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2010.12472v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>http://arxiv.org/pdf/2107.04313v1</td>\n",
       "      <td>2021-07-09 09:04:05+00:00</td>\n",
       "      <td>http://arxiv.org/abs/2107.04313v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year   month  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...  2021  August   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...  2021  August   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...  2021  August   \n",
       "\n",
       "                                      url  \\\n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "3  https://aclanthology.org/2021.woah-1.3   \n",
       "4  https://aclanthology.org/2021.woah-1.4   \n",
       "\n",
       "                                         title_Arxiv  \\\n",
       "2  Modeling Profanity and Hate Speech in Social M...   \n",
       "3  HateBERT: Retraining BERT for Abusive Language...   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...   \n",
       "\n",
       "                       pdf_url_Arxiv            published_Arxiv  \\\n",
       "2  http://arxiv.org/pdf/2106.07505v1  2021-06-14 15:34:37+00:00   \n",
       "3  http://arxiv.org/pdf/2010.12472v1  2020-10-23 15:14:14+00:00   \n",
       "4  http://arxiv.org/pdf/2107.04313v1  2021-07-09 09:04:05+00:00   \n",
       "\n",
       "                        result_Arxiv  \n",
       "2  http://arxiv.org/abs/2106.07505v2  \n",
       "3  http://arxiv.org/abs/2010.12472v2  \n",
       "4  http://arxiv.org/abs/2107.04313v1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e409eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contributions of Transformer Attention Heads\\nin Multi- and Cross-lingual Tasks\\n',\n",
       " 'Weicheng Ma1*, Kai Zhang2*†, Renze Lou3†, Lili Wang1, and Soroush Vosoughi4\\n',\n",
       " '1,4Department of Computer Science, Dartmouth College\\n2Department of Computer Science and Technology, Tsinghua University\\n3Department of Computer Science, Zhejiang University City College\\n1{first.last}.gr@dartmouth.edu\\n2drogozhang@gmail.com\\n3marionojump0722@gmail.com\\n4soroush.vosoughi@dartmouth.edu\\n',\n",
       " 'Abstract\\n',\n",
       " 'This paper studies the relative importance of\\nattention heads in Transformer-based models\\nto aid their interpretability in cross-lingual and\\nmulti-lingual tasks. Prior research has found\\nthat only a few attention heads are important\\nin each mono-lingual Natural Language Pro-\\ncessing (NLP) task and pruning the remaining\\nheads leads to comparable or improved per-\\nformance of the model. However, the impact\\nof pruning attention heads is not yet clear in\\ncross-lingual and multi-lingual tasks. Through\\nextensive experiments, we show that (1) prun-\\ning a number of attention heads in a multi-\\nlingual Transformer-based model has, in gen-\\neral, positive effects on its performance in\\ncross-lingual and multi-lingual tasks and (2)\\nthe attention heads to be pruned can be ranked\\nusing gradients and identiﬁed with a few trial\\nexperiments.\\nOur experiments focus on se-\\nquence labeling tasks, with potential applica-\\nbility on other cross-lingual and multi-lingual\\ntasks.\\nFor comprehensiveness, we examine\\ntwo pre-trained multi-lingual models, namely\\nmulti-lingual BERT (mBERT) and XLM-R,\\non three tasks across 9 languages each. We\\nalso discuss the validity of our ﬁndings and\\ntheir extensibility to truly resource-scarce lan-\\nguages and other task settings.\\n',\n",
       " '1\\nIntroduction\\n',\n",
       " 'Prior research on mono-lingual Transformer-based\\n(Vaswani et al., 2017) models reveals that a subset\\nof their attention heads makes key contributions\\nto each task, and the models perform comparably\\nwell (Voita et al., 2019; Michel et al., 2019) or even\\nbetter (Kovaleva et al., 2019) with the remaining\\nheads pruned 1. While multi-lingual Transformer-\\n',\n",
       " '*Equal contribution.\\n†Work done when interning at the Minds, Machines, and\\nSociety Lab at Dartmouth College.\\n1We regard single-source machine translation as a mono-\\nlingual task since the inputs to the models are mono-lingual.\\n',\n",
       " 'based models, e.g. mBERT (Devlin et al., 2019)\\nand XLM-R (Conneau et al., 2020), are widely ap-\\nplied in cross-lingual and multi-lingual NLP tasks\\n2 (Wang et al., 2019; Keung et al., 2019; Eskander\\net al., 2020), no attempt has been made to extend\\nthe ﬁndings on the aforementioned mono-lingual\\nresearch to this context. In this paper, we explore\\nthe roles of attention heads in cross-lingual and\\nmulti-lingual tasks for two reasons. First, better\\nunderstanding and interpretability of Transformer-\\nbased models leads to efﬁcient model designs and\\nparameter tuning. Second, head-pruning makes\\nTransformer-based models more applicable to truly\\nresource-scarce languages if it does not negatively\\naffect model performance signiﬁcantly.\\nThe biggest challenge we face when studying\\nthe roles of attention heads in cross-lingual and\\nmulti-lingual tasks is locating the heads to prune.\\nExisting research has shown that each attention\\nhead is specialized to extract a collection of linguis-\\ntic features, e.g., the middle layers of BERT mainly\\nextract syntactic features (Vig and Belinkov, 2019;\\nHewitt and Manning, 2019) and the fourth head\\non the ﬁfth layer of BERT greatly contributes to\\nthe coreference resolution task (Clark et al., 2019).\\nThus, we hypothesize that important feature extrac-\\ntors for a task should be shared across languages\\nand the remaining heads can be pruned. We eval-\\nuate two approaches used to rank attention heads,\\nthe ﬁrst of which is layer-wise relevance propaga-\\ntion (LRP, Ding et al. (2017)). Voita et al. (2019)\\ninterpreted the adaptation of LRP in Transformer-\\nbased models on machine translation. Motivated by\\nFeng et al. (2018) and Serrano and Smith (2019),\\nwe design a second ranking method based on gra-\\ndients since the gradients on each attention head\\n',\n",
       " '2We deﬁne a cross-lingual task as a task whose test set is in\\na different language from its training set. A multi-lingual task\\nis a task whose training set is multi-lingual and the languages\\nof its test set belong to the languages of the training set.\\n',\n",
       " 'arXiv:2108.08375v1  [cs.CL]  18 Aug 2021\\n',\n",
       " 'reﬂect its contribution to the predictions.\\nWe study the effects of pruning attention heads\\non three sequence labeling tasks, namely part-of-\\nspeech tagging (POS), named entity recognition\\n(NER), and slot ﬁlling (SF). We focus on sequence\\nlabeling tasks since they are more difﬁcult to an-\\nnotate than document- or sentence-level classiﬁca-\\ntion datasets and require more treatment in cross-\\nlingual and multi-lingual research. We choose POS\\nand NER datasets in 9 languages, where English\\n(EN), Chinese (ZH), and Arabic (AR) are candidate\\nsource languages. The MultiAtis++ corpus (Xu\\net al., 2020) is used in the SF evaluations with EN\\nas the source language. We do not include syntactic\\nchunking and semantic role labeling tasks due to\\nlack of availability of manually written and anno-\\ntated corpora. In these experiments, we rank atten-\\ntion heads based only on the source language(s) to\\nensure the extensibility of the learned knowledge\\nto cross-lingual tasks and resource-poor languages.\\nIn our preliminary experiments comparing the\\ngradient-based method and LRP, the average F1\\nscore improvements on NER with mBERT are 0.69\\n(cross-lingual) and 0.24 (multi-lingual) for LRP\\nand 0.81 (cross-lingual) and 0.31 (multi-lingual)\\nfor the gradient-based method, though both meth-\\nods rank attention heads similarly.\\nThus we choose the gradient-based method to\\nrank attention heads in all our experiments.\\nOur evaluations conﬁrm that only a subset of\\nattention heads in each Transformer-based model\\nmakes key contributions to each cross-lingual or\\nmulti-lingual task and that these heads are shared\\nacross languages.\\nPerformance of models generally drop when\\nthe highest-ranked or randomly selected heads are\\npruned, validating the head rankings generated by\\nour gradient-based method. We also observe perfor-\\nmance improvements on tasks with multiple source\\nlanguages by pruning attention heads. Our ﬁnd-\\nings potentially apply to truly resource-scarce lan-\\nguages since we show that the models perform bet-\\nter with attention heads pruned when fewer training\\ninstances are available in the target languages.\\nThe contributions of this paper are three-fold:\\n',\n",
       " '• We explore the roles of attention heads in multi-\\nlingual Transformer-based models and ﬁnd that\\npruning certain heads leads to comparable or\\nbetter performance in cross-lingual and multi-\\nlingual sequence labeling tasks.\\n',\n",
       " '• We adapt a gradient-based method to locate atten-\\n',\n",
       " 'LC\\nLanguage Family\\nTraining Size\\nPOS\\nNER\\n',\n",
       " 'EN\\nIE, Germanic\\n12,543\\n14,987\\n',\n",
       " 'DE\\nIE, Germanic\\n13,814\\n12,705\\n',\n",
       " 'NL\\nIE, Germanic\\n12,264\\n15,806\\n',\n",
       " 'AR\\nAfro-Asiatic, Semitic\\n6,075\\n1,329\\n',\n",
       " 'HE\\nAfro-Asiatic, Semitic\\n5,241\\n2,785\\n',\n",
       " 'ZH\\nSino-Tibetan\\n3,997\\n20,905\\n',\n",
       " 'JA\\nJapanese\\n7,027\\n800\\n',\n",
       " 'UR\\nIE, Indic\\n4,043\\n289,741\\n',\n",
       " 'FA\\nIE, Iranian\\n4,798\\n18,463\\n',\n",
       " 'Table 1: Details of POS and NER datasets in our ex-\\nperiments. LC refers to language code. Training size\\ndenotes the number of training instances.\\n',\n",
       " 'tion heads that can be pruned without exhaustive\\nexperiments on all possible combinations.\\n',\n",
       " '• We show the correctness, robustness, and ex-\\ntensibility of the ﬁndings and our head ranking\\nmethod under a wide range of settings through\\ncomprehensive experiments.\\n',\n",
       " '2\\nDatasets\\n',\n",
       " 'We use human-written and manually annotated\\ndatasets in experiments to avoid noise from ma-\\nchine translation and automatic label projection.\\nWe choose POS and NER datasets in 9 lan-\\nguages, namely EN, ZH, AR, Hebrew (HE),\\nJapanese (JA), Persian (FA), German (DE), Dutch\\n(NL), and Urdu (UR). As Table 1 shows, these lan-\\nguages fall in diverse language families and the\\ndatasets are very different in size. EN, ZH, and AR\\nare used as candidate source languages since they\\nare resource-rich in many NLP tasks. Our POS\\ndatasets are all from Universal Dependencies (UD)\\nv2.7 3. These datasets are labeled with a common\\nlabel set containing 17 POS tags.\\nFor NER, we use NL, EN, and DE datasets\\nfrom CoNLL-2002 and 2003 challenges (Tjong\\nKim Sang, 2002; Tjong Kim Sang and De Meulder,\\n2003). Additionally, we use the People’s Daily\\ndataset 4, iob2corpus 5, AQMAR (Mohit et al.,\\n2012), ArmanPerosNERCorpus (Poostchi et al.,\\n2016), MK-PUCIT (Kanwal et al., 2020), and a\\nnews-based NER dataset (Mordecai and Elhadad,\\n2012) for the languages CN, JA, AR, FA, UR, and\\n',\n",
       " '3http://universaldependencies.org/\\n4http://github.com/OYE93/Chinese-NLP-C\\norpus/tree/master/NER/People’sDaily\\n',\n",
       " '5http://github.com/Hironsan/IOB2Corpus\\n',\n",
       " 'HE, respectively. Since the NER datasets are in-\\ndividually constructed in each language, their la-\\nbel sets do not fully agree. As there are four NE\\ntypes (PER, ORG, LOC, MISC) in the three source-\\nlanguage datasets, we merge other NE types into\\nthe MISC class to allow cross-lingual evaluations.\\nWe evaluate SF models on MultiAtis++ with\\nEN as the source language and Spanish (ES), Por-\\ntuguese (PT), DE, French (FR), ZH, JA, Hindi (HI),\\nand Turkish (TR) as target languages. There are 71\\nslot types in the TR dataset, 75 in the HI dataset,\\nand 84 in the other datasets. We do not use the\\nintent labels in our evaluations since we study only\\nsequence labeling tasks. Thus our results are not\\ndirectly comparable with Xu et al. (2020).\\n',\n",
       " '3\\nMethodology\\n',\n",
       " 'Here, we introduce the gradient-based method we\\nuse in the experiments to rank the attention heads.\\nFeng et al. (2018) claim that gradients measure the\\nimportance of features to predictions. Since each\\nhead functions similarly as a standalone feature\\nextractor in a Transformer-based model, we use\\ngradients to approximate the importance of the fea-\\nture set extracted by each head and rank the heads\\naccordingly. Michel et al. (2019) determine im-\\nportance of heads with accumulated gradients at\\neach head in a training epoch. Different from their\\napproach, we ﬁne-tune the model on the training\\nset and rank the heads using gradients on the de-\\nvelopment set to ensure that the head importance\\nrankings are not signiﬁcantly correlated with the\\ntraining instances in one source language.\\nSpeciﬁcally, our method generates head rank-\\nings for each language in three steps:\\n(1) We ﬁne-tune a Transformer-based model on a\\nmono-lingual task for three epochs.\\n(2) We re-run the ﬁne-tuned model on the develop-\\nment partition of the dataset with back-propagation\\nbut not parameter updates to obtain gradients.\\n(3) We sum up the absolute gradients on each\\nhead, layer-wise normalize the accumulated gra-\\ndients, and scale them into the range [0, 1] globally.\\n',\n",
       " 'We show Spearman’s rank correlation coefﬁ-\\ncients (Spearman’s ρ) between head rankings of\\neach language pair generated by our method on\\nPOS, NER, and SF in Figure 1.\\nThe highest-\\nranked heads largely overlap in all three tasks,\\nwhile the rankings of unimportant heads vary more\\nin mBERT than XLM-R.\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " '1\\n0.81 0.77 0.82 0.85 0.82 0.75 0.87 0.79\\n',\n",
       " '0.81\\n1\\n0.72 0.71 0.81 0.83 0.75 0.82 0.69\\n',\n",
       " '0.77 0.72\\n1\\n0.85 0.72 0.83 0.73 0.72 0.72\\n',\n",
       " '0.82 0.71 0.85\\n1\\n0.78 0.81 0.72 0.77 0.76\\n',\n",
       " '0.85 0.81 0.72 0.78\\n1\\n0.81 0.68 0.86 0.72\\n',\n",
       " '0.82 0.83 0.83 0.81 0.81\\n1\\n0.78 0.82 0.70\\n',\n",
       " '0.75 0.75 0.73 0.72 0.68 0.78\\n1\\n0.67 0.69\\n',\n",
       " '0.87 0.82 0.72 0.77 0.86 0.82 0.67\\n1\\n0.82\\n',\n",
       " '0.79 0.69 0.72 0.76 0.72 0.70 0.69 0.82\\n1\\n',\n",
       " '(a) POS-mBERT\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " '1\\n0.95 0.96 0.97 0.97 0.97 0.95 0.97 0.97\\n',\n",
       " '0.95\\n1\\n0.95 0.94 0.95 0.97 0.97 0.96 0.95\\n',\n",
       " '0.96 0.95\\n1\\n0.97 0.96 0.97 0.94 0.96 0.96\\n',\n",
       " '0.97 0.94 0.97\\n1\\n0.97 0.97 0.95 0.97 0.96\\n',\n",
       " '0.97 0.95 0.96 0.97\\n1\\n0.97 0.96 0.97 0.96\\n',\n",
       " '0.97 0.97 0.97 0.97 0.97\\n1\\n0.97 0.97 0.97\\n',\n",
       " '0.95 0.97 0.94 0.95 0.96 0.97\\n1\\n0.95 0.96\\n',\n",
       " '0.97 0.96 0.96 0.97 0.97 0.97 0.95\\n1\\n0.96\\n',\n",
       " '0.97 0.95 0.96 0.96 0.96 0.97 0.96 0.96\\n1\\n',\n",
       " '(b) POS-XLM-R\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " '1\\n0.75\\n0.78\\n0.85\\n0.83\\n0.76\\n0.69\\n0.87\\n0.47\\n',\n",
       " '0.75\\n1\\n0.69\\n0.79\\n0.81\\n0.70\\n0.65\\n0.84\\n0.66\\n',\n",
       " '0.78\\n0.69\\n1\\n0.84\\n0.77\\n0.87\\n0.75\\n0.79\\n0.43\\n',\n",
       " '0.85\\n0.79\\n0.84\\n1\\n0.83\\n0.81\\n0.73\\n0.84\\n0.56\\n',\n",
       " '0.83\\n0.81\\n0.77\\n0.83\\n1\\n0.70\\n0.69\\n0.87\\n0.65\\n',\n",
       " '0.76\\n0.70\\n0.87\\n0.81\\n0.70\\n1\\n0.69\\n0.78\\n0.38\\n',\n",
       " '0.69\\n0.65\\n0.75\\n0.73\\n0.69\\n0.69\\n1\\n0.71\\n0.48\\n',\n",
       " '0.87\\n0.84\\n0.79\\n0.84\\n0.87\\n0.78\\n0.71\\n1\\n0.57\\n',\n",
       " '0.47\\n0.66\\n0.43\\n0.56\\n0.65\\n0.38\\n0.48\\n0.57\\n1\\n',\n",
       " '(c) NER-mBERT\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " 'EN\\nZH\\nAR\\nFA\\nDE\\nHE\\nJA\\nNL\\nUR\\n',\n",
       " '1\\n0.92\\n0.93\\n0.90\\n0.92\\n0.95\\n0.91\\n0.95\\n0.92\\n',\n",
       " '0.92\\n1\\n0.87\\n0.97\\n0.92\\n0.94\\n0.96\\n0.96\\n0.97\\n',\n",
       " '0.93\\n0.87\\n1\\n0.87\\n0.92\\n0.92\\n0.86\\n0.92\\n0.88\\n',\n",
       " '0.90\\n0.97\\n0.87\\n1\\n0.93\\n0.93\\n0.94\\n0.97\\n0.95\\n',\n",
       " '0.92\\n0.92\\n0.92\\n0.93\\n1\\n0.92\\n0.92\\n0.96\\n0.91\\n',\n",
       " '0.95\\n0.94\\n0.92\\n0.93\\n0.92\\n1\\n0.93\\n0.95\\n0.93\\n',\n",
       " '0.91\\n0.96\\n0.86\\n0.94\\n0.92\\n0.93\\n1\\n0.93\\n0.94\\n',\n",
       " '0.95\\n0.96\\n0.92\\n0.97\\n0.96\\n0.95\\n0.93\\n1\\n0.96\\n',\n",
       " '0.92\\n0.97\\n0.88\\n0.95\\n0.91\\n0.93\\n0.94\\n0.96\\n1\\n',\n",
       " '(d) NER-XLM-R\\n',\n",
       " 'EN\\nZH\\nDE\\nHI\\nFR\\nES\\nJA\\nPT\\nTR\\n',\n",
       " 'EN\\nZH\\nDE\\nHI\\nFR\\nES\\nJA\\nPT\\nTR\\n',\n",
       " '1\\n0.83 0.80 0.78 0.82 0.84 0.83 0.85 0.62\\n',\n",
       " '0.83\\n1\\n0.79 0.80 0.82 0.78 0.82 0.83 0.62\\n',\n",
       " '0.80 0.79\\n1\\n0.75 0.81 0.83 0.81 0.85 0.65\\n',\n",
       " '0.78 0.80 0.75\\n1\\n0.72 0.77 0.81 0.77 0.71\\n',\n",
       " '0.82 0.82 0.81 0.72\\n1\\n0.80 0.87 0.89 0.67\\n',\n",
       " '0.84 0.78 0.83 0.77 0.80\\n1\\n0.85 0.84 0.72\\n',\n",
       " '0.83 0.82 0.81 0.81 0.87 0.85\\n1\\n0.89 0.68\\n',\n",
       " '0.85 0.83 0.85 0.77 0.89 0.84 0.89\\n1\\n0.67\\n',\n",
       " '0.62 0.62 0.65 0.71 0.67 0.72 0.68 0.67\\n1\\n',\n",
       " '(e) SF-mBERT\\n',\n",
       " 'EN\\nZH\\nDE\\nHI\\nFR\\nES\\nJA\\nPT\\nTR\\n',\n",
       " 'EN\\nZH\\nDE\\nHI\\nFR\\nES\\nJA\\nPT\\nTR\\n',\n",
       " '1\\n0.95 0.97 0.94 0.98 0.95 0.95 0.96 0.94\\n',\n",
       " '0.95\\n1\\n0.94 0.96 0.95 0.92 0.97 0.95 0.95\\n',\n",
       " '0.97 0.94\\n1\\n0.94 0.98 0.95 0.95 0.97 0.94\\n',\n",
       " '0.94 0.96 0.94\\n1\\n0.94 0.90 0.95 0.94 0.96\\n',\n",
       " '0.98 0.95 0.98 0.94\\n1\\n0.96 0.95 0.97 0.93\\n',\n",
       " '0.95 0.92 0.95 0.90 0.96\\n1\\n0.94 0.95 0.91\\n',\n",
       " '0.95 0.97 0.95 0.95 0.95 0.94\\n1\\n0.95 0.95\\n',\n",
       " '0.96 0.95 0.97 0.94 0.97 0.95 0.95\\n1\\n0.95\\n',\n",
       " '0.94 0.95 0.94 0.96 0.93 0.91 0.95 0.95\\n1\\n',\n",
       " '(f) SF-XLM-R\\n',\n",
       " 'Figure 1:\\nSpearman’s ρ of head ranking matrices\\nbetween languages in the POS, NER, and SF tasks.\\nDarker colors indicate higher correlations.\\n',\n",
       " 'After ranking the attention heads, we ﬁne-tune\\nthe model, with the lowest-ranked head in the\\nsource language pruned. We keep increasing the\\nnumber of heads to prune until it reaches a pre-\\nset limit or when the performance starts to drop.\\nWe limit the number of trials to 12 since the mod-\\nels mostly show improved performance within 12\\nattempts 6.\\n',\n",
       " '4\\nExperiments and Analysis\\n',\n",
       " 'This section displays and explains experimental re-\\nsults on cross-lingual and multi-lingual POS, NER,\\nand SF tasks. Training sets in target languages are\\nnot used to train the model under the cross-lingual\\nsetting. Our experiments are based on the Hugging-\\nface (Wolf et al., 2020) implementations of mBERT\\n',\n",
       " '6On average 7.52 and 6.58 heads are pruned for POS, 7.54\\nand 7.28 heads for NER, and 6.19 and 6.31 heads for SF,\\nrespectively in mBERT and XLM-R models.\\n',\n",
       " 'SL\\nTL\\n',\n",
       " 'mBERT\\nXLM-R\\n',\n",
       " 'Unpruned\\nPruned\\nUnpruned\\nPruned\\n',\n",
       " 'CrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\n',\n",
       " 'EN\\n',\n",
       " 'ZH\\n59.88\\n95.10\\n59.99\\n95.31\\n41.10\\n95.87\\n46.18\\n95.99\\n',\n",
       " 'AR\\n55.98\\n95.64\\n56.71\\n95.68\\n66.75\\n96.07\\n67.02\\n96.13\\n',\n",
       " 'FA\\n57.94\\n94.48\\n58.34\\n94.81\\n66.60\\n96.85\\n66.50\\n97.09\\n',\n",
       " 'DE\\n88.86\\n94.81\\n89.13\\n94.94\\n89.41\\n94.81\\n89.78\\n95.19\\n',\n",
       " 'HE\\n77.91\\n96.45\\n78.01\\n96.58\\n77.48\\n97.26\\n80.37\\n97.30\\n',\n",
       " 'JA\\n44.73\\n96.84\\n45.95\\n96.97\\n30.98\\n97.52\\n33.64\\n97.62\\n',\n",
       " 'NL\\n87.45\\n96.47\\n87.48\\n96.69\\n88.06\\n97.04\\n88.03\\n97.02\\n',\n",
       " 'UR\\n53.21\\n91.92\\n54.78\\n92.17\\n55.45\\n92.94\\n56.04\\n93.07\\n',\n",
       " 'ZH\\n',\n",
       " 'EN\\n55.63\\n96.52\\n57.05\\n96.64\\n42.35\\n97.19\\n43.38\\n97.32\\n',\n",
       " 'AR\\n38.41\\n95.62\\n41.03\\n95.66\\n36.71\\n95.99\\n38.19\\n96.07\\n',\n",
       " 'FA\\n43.68\\n94.55\\n45.29\\n94.63\\n33.43\\n97.07\\n34.64\\n97.09\\n',\n",
       " 'DE\\n63.50\\n94.62\\n64.36\\n94.75\\n46.58\\n95.06\\n47.47\\n95.22\\n',\n",
       " 'HE\\n57.14\\n96.51\\n57.94\\n96.58\\n51.26\\n97.06\\n50.42\\n97.19\\n',\n",
       " 'JA\\n43.63\\n96.73\\n44.69\\n97.01\\n49.12\\n97.32\\n49.74\\n97.34\\n',\n",
       " 'NL\\n59.95\\n96.78\\n61.10\\n96.97\\n40.78\\n97.30\\n42.50\\n97.43\\n',\n",
       " 'UR\\n43.82\\n92.21\\n44.07\\n92.26\\n30.08\\n92.90\\n29.26\\n93.01\\n',\n",
       " 'AR\\n',\n",
       " 'EN\\n54.77\\n96.50\\n56.90\\n96.53\\n61.73\\n97.21\\n63.63\\n97.31\\n',\n",
       " 'ZH\\n46.19\\n95.16\\n47.14\\n95.31\\n25.12\\n95.16\\n34.71\\n96.04\\n',\n",
       " 'FA\\n63.82\\n94.52\\n64.02\\n94.64\\n70.92\\n97.15\\n71.55\\n97.20\\n',\n",
       " 'DE\\n56.88\\n94.82\\n57.85\\n94.98\\n65.21\\n95.16\\n68.28\\n95.29\\n',\n",
       " 'HE\\n60.33\\n96.44\\n61.88\\n96.70\\n67.45\\n97.23\\n67.72\\n97.34\\n',\n",
       " 'JA\\n44.32\\n97.02\\n44.18\\n97.15\\n22.11\\n97.52\\n29.21\\n97.65\\n',\n",
       " 'NL\\n58.86\\n96.87\\n60.31\\n97.03\\n62.93\\n96.87\\n64.80\\n97.50\\n',\n",
       " 'UR\\n49.31\\n92.00\\n49.76\\n92.16\\n54.79\\n92.74\\n56.06\\n92.88\\n',\n",
       " 'Table 2: F-1 scores of mBERT and XLM on POS. SL and TL refer to source and target languages and CrLing and\\nMulLing stand for cross-lingual and multi-lingual settings, respectively. Unpruned results are produced by the full\\nmodels and pruned results are the best scores each model produces with up to 12 lowest-ranked heads pruned. The\\nhigher performance in each pair of pruned and unpruned experiments is in bold.\\n',\n",
       " 'and XLM-R. Speciﬁcally, we use the pre-trained\\nbert-base-multilingual-cased and xlm-roberta-base\\nmodels for their comparable model sizes. The mod-\\nels are ﬁne-tuned for 3 epochs with a learning rate\\nof 5e-5 in all the experiments. We use the ofﬁcial\\ndataset splits and load training instances with se-\\nquential data samplers, so the reported evaluation\\nscores are robust to randomness.\\n',\n",
       " '4.1\\nPOS\\n',\n",
       " 'Table 2 shows the evaluation scores on POS with\\nthree source language choices. In the majority (88\\nout of 96 pairs) of experiments, pruning up to 12\\nattention heads improves mBERT and XLM-R per-\\nformance. Results are comparable in the other 8\\nexperiments with and without head pruning. Aver-\\nage F-1 score improvements are 0.91 for mBERT\\nand 1.78 for XLM-R in cross-lingual tasks, and\\n0.15 for mBERT and 0.17 for XLM-R in multi-\\n',\n",
       " 'lingual tasks. These results support that pruning\\nheads generally has positive effects on model per-\\nformance in cross-lingual and multi-lingual tasks,\\nand that our method correctly ranks the heads.\\n',\n",
       " 'Consistent with Conneau et al. (2020), XLM-\\nR usually outperforms mBERT, with exceptions\\nin cross-lingual experiments where ZH and JA\\ndatasets are involved. Word segmentation in ZH\\nand JA is different from the other languages we\\nchoose, e.g. words are not separated by white\\nspaces and unpaired adjacent word pieces often\\nmake up a new word.\\nAs XLM-R applies the\\nSentencePiece tokenization method (Kudo and\\nRichardson, 2018), it is more likely to detect wrong\\nword boundaries and make improper predictions\\nthan mBERT in cross-lingual experiments involv-\\ning ZH or JA datasets. We note that the perfor-\\nmance improvements are solid regardless of the\\n',\n",
       " 'SL\\nTL\\n',\n",
       " 'mBERT\\nXLM-R\\n',\n",
       " 'Unpruned\\nPruned\\nUnpruned\\nPruned\\n',\n",
       " 'CrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\n',\n",
       " 'EN\\n',\n",
       " 'ZH\\n47.64\\n93.24\\n51.61\\n93.71\\n29.97\\n90.99\\n32.33\\n91.11\\n',\n",
       " 'AR\\n38.81\\n70.55\\n38.93\\n73.32\\n41.21\\n71.77\\n43.78\\n74.28\\n',\n",
       " 'FA\\n40.12\\n96.70\\n39.81\\n96.97\\n54.90\\n96.62\\n55.72\\n96.98\\n',\n",
       " 'DE\\n56.43\\n79.11\\n58.27\\n79.19\\n63.71\\n82.31\\n66.48\\n83.10\\n',\n",
       " 'HE\\n46.92\\n89.18\\n46.55\\n88.49\\n56.96\\n88.02\\n56.87\\n89.67\\n',\n",
       " 'JA\\n42.45\\n84.91\\n44.14\\n84.34\\n33.87\\n81.48\\n37.88\\n82.35\\n',\n",
       " 'NL\\n64.51\\n84.90\\n65.56\\n85.17\\n77.15\\n90.21\\n77.66\\n90.38\\n',\n",
       " 'UR\\n37.34\\n99.29\\n40.60\\n99.22\\n58.25\\n99.15\\n58.68\\n99.07\\n',\n",
       " 'ZH\\n',\n",
       " 'EN\\n38.58\\n87.65\\n41.40\\n87.99\\n56.40\\n90.72\\n58.55\\n91.05\\n',\n",
       " 'AR\\n36.43\\n72.27\\n36.99\\n72.86\\n34.31\\n74.84\\n36.11\\n75.68\\n',\n",
       " 'FA\\n45.68\\n96.21\\n46.57\\n96.23\\n51.60\\n95.63\\n51.51\\n95.66\\n',\n",
       " 'DE\\n29.07\\n79.04\\n33.81\\n78.67\\n56.22\\n82.33\\n55.51\\n82.54\\n',\n",
       " 'HE\\n47.14\\n88.20\\n47.68\\n89.35\\n48.52\\n85.95\\n48.94\\n87.79\\n',\n",
       " 'JA\\n49.21\\n82.02\\n51.69\\n83.20\\n46.18\\n80.19\\n47.06\\n82.63\\n',\n",
       " 'NL\\n29.75\\n84.61\\n31.46\\n85.28\\n49.59\\n89.56\\n52.27\\n90.56\\n',\n",
       " 'UR\\n44.61\\n99.26\\n46.33\\n99.28\\n48.98\\n98.99\\n55.95\\n99.10\\n',\n",
       " 'AR\\n',\n",
       " 'EN\\n19.29\\n87.86\\n20.07\\n87.82\\n51.33\\n90.37\\n51.00\\n91.01\\n',\n",
       " 'ZH\\n41.70\\n93.46\\n40.43\\n93.54\\n25.78\\n90.51\\n31.03\\n91.00\\n',\n",
       " 'FA\\n46.57\\n96.82\\n46.87\\n96.87\\n53.35\\n96.55\\n52.60\\n96.74\\n',\n",
       " 'DE\\n24.47\\n75.78\\n25.62\\n78.04\\n50.87\\n82.63\\n50.00\\n82.73\\n',\n",
       " 'HE\\n47.15\\n86.77\\n46.72\\n87.64\\n49.52\\n87.37\\n50.85\\n89.28\\n',\n",
       " 'JA\\n41.49\\n79.90\\n42.11\\n83.17\\n36.98\\n81.72\\n38.87\\n80.92\\n',\n",
       " 'NL\\n26.00\\n84.83\\n26.34\\n85.24\\n49.27\\n90.73\\n48.87\\n91.11\\n',\n",
       " 'UR\\n46.47\\n99.26\\n45.66\\n99.31\\n48.48\\n99.10\\n53.51\\n99.15\\n',\n",
       " 'Table 3: F-1 scores of mBERT and XLM on NER. SL and TL refer to source and target languages and CrLing and\\nMulLing stand for cross-lingual and multi-lingual settings, respectively. Unpruned results are produced by the full\\nmodels and pruned results are the best scores each model produces with up to 12 lowest-ranked heads pruned.\\n',\n",
       " 'source language selection and severe differences\\nof training data sizes in EN, ZH, and AR. This\\ndemonstrates the correctness of the head rankings\\nour method generates and that the important atten-\\ntion heads for a task are almost language invariant.\\n',\n",
       " 'We also examine to what extent the score im-\\nprovements are affected by the relationships be-\\ntween source and target languages, e.g. language\\nfamilies, URIEL language distance scores (Littell\\net al., 2017), and the similarity of the head ranking\\nmatrices. There are three non-exclusive clusters\\nof language families (containing more than one\\nlanguage) in our choice of languages, namely Indo-\\nEuropean (IE), Germanic, and Semitic languages.\\nAverage score improvements between models with\\nand without head pruning are 0.40 (IE), 0.16 (Ger-\\nmanic), and 0.91 (Semitic) for mBERT and 0.19\\n(IE), 0.18 (Germanic), and 0.19 (Semitic) for XLM-\\nR. In comparison, the overall average score im-\\n',\n",
       " 'provements are 0.53 for mBERT and 0.97 for XLM-\\nR. Despite the generally higher performance of\\nmodels when the source and target languages are\\nin the same family, the score improvements by\\npruning heads are not necessarily associated with\\nlanguage families. Additionally, we use Spear-\\nman’s ρ to measure the correlations between im-\\nproved F-1 scores and URIEL language distances.\\nThe correlation scores are 0.11 (cross-lingual) and\\n0.12 (multi-lingual) for mBERT, and -0.40 (cross-\\nlingual) and 0.23 (multi-lingual) for XLM-R. Sim-\\nilarly, the Spearman’s ρ between score improve-\\nments and similarities in head ranking matrices\\nshown in Figure 1 are -0.34 (cross-lingual) and\\n0.25 (multi-lingual) for mBERT, and -0.52 (cross-\\nlingual) and -0.10 (multi-lingual) for XLM-R. This\\nindicate that except in the cross-lingual XLM-R\\nmodel which faces word segmentation issues on\\nZH or JA experiments, pruning attention heads\\n',\n",
       " 'SL\\nTL\\n',\n",
       " 'mBERT\\nXLM-R\\n',\n",
       " 'Unpruned\\nPruned\\nUnpruned\\nPruned\\n',\n",
       " 'CrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\n',\n",
       " 'EN\\n',\n",
       " 'ZH\\n69.83\\n94.11\\n71.84\\n94.25\\n62.58\\n93.97\\n67.98\\n94.29\\n',\n",
       " 'DE\\n60.69\\n94.60\\n66.97\\n94.95\\n82.85\\n94.81\\n83.50\\n95.35\\n',\n",
       " 'HI\\n44.28\\n85.93\\n45.84\\n87.08\\n58.32\\n86.72\\n66.39\\n87.16\\n',\n",
       " 'FR\\n60.44\\n93.96\\n67.13\\n94.18\\n76.53\\n93.51\\n77.59\\n93.77\\n',\n",
       " 'ES\\n72.27\\n87.71\\n73.96\\n88.17\\n81.70\\n89.10\\n81.88\\n88.83\\n',\n",
       " 'JA\\n68.28\\n93.73\\n68.32\\n93.78\\n32.39\\n93.65\\n36.68\\n93.71\\n',\n",
       " 'PT\\n59.37\\n90.83\\n63.23\\n90.82\\n77.42\\n90.76\\n77.54\\n91.24\\n',\n",
       " 'TR\\n28.11\\n83.41\\n32.21\\n84.31\\n45.91\\n83.20\\n52.64\\n84.30\\n',\n",
       " 'EN\\n95.43\\n95.27\\n94.59\\n94.87\\n',\n",
       " 'Table 4: Slot F-1 scores on the MultiAtis++ corpus. CrLing and MulLing refer to cross-lingual and multi-lingual\\nsettings, respectively. SL and TL refer to source and target languages, respectively. English mono-lingual results\\nare reported for validity check purposes.\\n',\n",
       " 'improves model performance regardless of the dis-\\ntances between source and target languages. Thus\\nour ﬁndings are potentially applicable to all cross-\\nlingual and multi-lingual POS tasks.\\n',\n",
       " '4.2\\nNER\\n',\n",
       " 'As Table 3 shows, pruning attention heads gener-\\nally has positive effects on our cross-lingual and\\nmulti-lingual NER models.\\nEven in the multi-\\nlingual AR-UR experiment where the full mBERT\\nmodel achieves an F-1 score of 99.26, the score is\\nraised to 99.31 by pruning heads. Scores are com-\\nparable with and without head pruning in the 19\\ncases where model performances are not improved.\\nThis also lends support to the specialized role of\\nimportant attention heads and the consistency of\\nhead rankings across languages. In NER exper-\\niments, performance drops mostly happen when\\nthe source and target languages are from different\\nfamilies. This is likely caused by the difference\\nbetween named entity (NE) representations across\\nlanguage families. We show in Section 5.2 that the\\ngap is largely bridged when a language from the\\nsame family as the target language is added to the\\nsource languages.\\nAverage score improvements are comparable on\\nmBERT (0.81 under cross-lingual and 0.31 under\\nmulti-lingual settings) and XLM-R (1.08 under\\ncross-lingual and 0.67 under multi-lingual settings)\\nin NER experiments. The results indicate that the\\nperformance improvements introduced by head-\\npruning are not sensitive to the pre-training corpora\\nof models. The correlations between F-1 score\\nimprovements and URIEL language distances are\\nsmall, with Spearman’s ρ of -0.05 (cross-lingual)\\n',\n",
       " 'and -0.27 (multi-lingual) for mBERT and 0.10\\n(cross-lingual) and 0.12 (multi-lingual) for XLM-R.\\nSimilarities between head ranking matrices do not\\ngreatly affect score improvements either, the Spear-\\nman’s ρ of which are -0.08 (cross-lingual) and 0.06\\n(multi-lingual) for mBERT and 0.05 (cross-lingual)\\nand 0.12 (multi-lingual) for XLM-R. The ﬁndings\\nin POS and NER experiments are consistent, sup-\\nporting our hypothesis that important heads for a\\ntask are shared by arbitrary source-target language\\nselections.\\n',\n",
       " '4.3\\nSlot Filling\\n',\n",
       " 'We report SF evaluation results in Table 4. In 31\\nout of 34 pairs of experiments, pruning up to 12\\nheads results in performance improvements, while\\nthe scores are comparable in the other three cases.\\nThese results agree with those in POS and NER\\nexperiments, showing that only a subset of heads\\nin each model makes key contributions to cross-\\nlingual or multi-lingual tasks.\\nWe also evaluate the correlations between score\\nchanges and the closeness of source and target\\nlanguages.\\nIn terms of URIEL language dis-\\ntances, the Spearman’s ρ are 0.69 (cross-lingual)\\nand 0.14 (multi-lingual) for mBERT and -0.59\\n(cross-lingual) and 0.14 (multi-lingual) for XLM-\\nR. The coefﬁcients are -0.25 (cross-lingual) and\\n-0.73 (multi-lingual) for mBERT and -0.70 (cross-\\nlingual) and -0.14 (multi-lingual) between score\\nimprovements and similarities in head ranking ma-\\ntrices. While these coefﬁcients are generally higher\\nthan those in POS and NER evaluations, their p-\\nvalues are also high (0.55 to 0.74), indicating the\\ncorrelations between the score changes and source-\\n',\n",
       " 'NER\\n',\n",
       " 'Max-Pruning\\nRand-Pruning\\nTL\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\n',\n",
       " 'ZH\\n-1.74\\n+0.08\\n-2.44\\n+0.26\\n',\n",
       " 'AR\\n-3.17\\n-2.42\\n-2.09\\n-0.43\\n',\n",
       " 'DE\\n+0.88\\n-0.62\\n+0.57\\n-0.38\\n',\n",
       " 'NL\\n-2.76\\n-0.23\\n+0.29\\n+0.36\\n',\n",
       " 'FA\\n-0.86\\n-0.31\\n-2.52\\n-0.74\\n',\n",
       " 'HE\\n-2.50\\n-2.15\\n-0.49\\n-4.21\\n',\n",
       " 'JA\\n-1.48\\n-1.08\\n-2.65\\n-2.40\\n',\n",
       " 'UR\\n-0.15\\n-0.10\\n-0.60\\n-0.12\\n',\n",
       " 'POS\\n',\n",
       " 'Max-Mask\\nRand-Mask\\nTL\\nCrLing\\nMulLing\\nCrLing\\nMulLing\\n',\n",
       " 'ZH\\n+0.03\\n-0.39\\n-0.14\\n-0.20\\n',\n",
       " 'AR\\n-0.65\\n-0.04\\n-0.66\\n-0.12\\n',\n",
       " 'DE\\n-0.64\\n-0.04\\n-0.64\\n-0.14\\n',\n",
       " 'NL\\n-0.13\\n-0.13\\n-0.11\\n-0.16\\n',\n",
       " 'FA\\n-0.75\\n-0.03\\n-0.53\\n-0.25\\n',\n",
       " 'HE\\n-1.27\\n-0.28\\n-1.06\\n+0.05\\n',\n",
       " 'JA\\n-22.29\\n-0.05\\n-1.23\\n-0.05\\n',\n",
       " 'UR\\n-1.78\\n-0.11\\n-0.77\\n-0.07\\n',\n",
       " 'Table 5: F-1 score differences from the full mBERT\\nmodel on NER (upper) and POS (lower) by prun-\\ning highest ranked (Max-Pruning) or random (Rand-\\nPruning) heads in the ranking matrices. The source lan-\\nguage is EN. Blue and red cells indicate score drops\\nand improvements, respectively.\\n',\n",
       " 'target language closeness are not statistically sig-\\nniﬁcant. 7\\n',\n",
       " '5\\nDiscussions\\n',\n",
       " 'In this section, we perform case studies to conﬁrm\\nthe validity of our head ranking method. We also il-\\nlustrate the extensibility of the knowledge we learn\\nfrom the main experiments to a wider range of set-\\ntings, e.g. when the training dataset is limited in\\nsize or constructed over multiple source languages.\\n',\n",
       " '5.1\\nCorrectness of Head Rankings\\n',\n",
       " 'We evaluate the correctness of our head ranking\\nmethod through comparisons between results in\\nTables 2 and 3 and those produced by pruning (1)\\nrandomly sampled heads and (2) highest ranked\\nheads. Speciﬁcally, we repeat the head-pruning\\nexperiments with mBERT on NER and POS using\\n',\n",
       " '7The p-values for all the other Spearman’s ρ we report\\nare lower than 0.01, showing that those correlation scores are\\nstatistically signiﬁcant.\\n',\n",
       " 'EN as the source language and display the score dif-\\nferences from the the full models in Table 5. Same\\nas in the main experiments, we pick the best score\\nfrom pruning 1 to 12 heads in each experiment. A\\nrandom seed of 42 is used for sampling attention\\nheads to prune under the random sampling setting.\\nIn 14 out of 16 NER experiments, pruning the\\nheads ranked highest by our method results in no-\\nticeable performance drops compared to the full\\nmodel. Consistently, pruning the highest-ranked\\nattention heads harms the performance of mBERT\\nin 15 out of 16 POS experiments. Though score\\nchanges are slightly positive for cross-lingual EN-\\nDE and multi-lingual EN-ZH NER tasks and in the\\ncross-lingual EN-ZH POS experiment, improve-\\nments introduced by pruning lowest-ranked heads\\nare more signiﬁcant, as Table 2 and Table 3 show.\\nPruning random attention heads also has mainly\\nnegative effects on the performance of mBERT.\\nThese results indicate that while pruning attention\\nheads potentially boosts the performance of models,\\nreasonably choosing the heads to prune is impor-\\ntant. Our gradient-based method properly ranks the\\nheads by their priority to prune.\\n',\n",
       " '5.2\\nMultiple Source Languages\\n',\n",
       " 'Training cross-lingual models on multiple source\\nlanguages is a practical way to improve their per-\\nformance, due to enlarged training data size and\\nsupervision from source-target languages closer to\\neach other (Wu et al., 2020; Moon et al., 2019;\\nChen et al., 2019; Rahimi et al., 2019; T¨ackstr¨om,\\n2012). We also explore the effects of pruning atten-\\ntion heads under the multi-source settings. In this\\nsection, we experiment with mBERT on EN, DE,\\nAR, HE, and ZH datasets for both NER and POS\\ntasks. These languages fall into three mutually ex-\\nclusive language families, enabling our analysis on\\nthe inﬂuence of training cross-lingual models with\\nsource languages belonging to the same family as\\nthe target language. Similar to related research, the\\nmodel is ﬁne-tuned on the concatenation of training\\ndatasets in all the languages but the one on which\\nthe model is tested.\\nSince the head ranking matrices are not identical\\nacross languages, we design three heuristics to rank\\nthe heads in the multi-source experiments. The ﬁrst\\nmethod merges the head ranking matrices of all the\\nsource languages into one matrix and re-generates\\nthe rankings. The second method ranks the at-\\ntention heads after summing up the head ranking\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '68.8\\n',\n",
       " '70.6\\n',\n",
       " '72.4\\n',\n",
       " '74.2\\n',\n",
       " '76.0\\n',\n",
       " '77.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(a) EN-DE\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '76.4\\n',\n",
       " '78.2\\n',\n",
       " '79.9\\n',\n",
       " '81.7\\n',\n",
       " '83.4\\n',\n",
       " '85.2\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(b) EN-NL\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '51.5\\n',\n",
       " '55.1\\n',\n",
       " '58.7\\n',\n",
       " '62.3\\n',\n",
       " '65.9\\n',\n",
       " '69.5\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(c) EN-AR\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '70.8\\n',\n",
       " '74.0\\n',\n",
       " '77.3\\n',\n",
       " '80.6\\n',\n",
       " '83.9\\n',\n",
       " '87.2\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(d) EN-HE\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '87.5\\n',\n",
       " '88.7\\n',\n",
       " '89.9\\n',\n",
       " '91.1\\n',\n",
       " '92.2\\n',\n",
       " '93.4\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(e) EN-ZH\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '72.7\\n',\n",
       " '74.9\\n',\n",
       " '77.1\\n',\n",
       " '79.3\\n',\n",
       " '81.5\\n',\n",
       " '83.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(f) EN-JA\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '75.8\\n',\n",
       " '79.8\\n',\n",
       " '83.8\\n',\n",
       " '87.8\\n',\n",
       " '91.8\\n',\n",
       " '95.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(g) EN-FA\\n',\n",
       " 'Figure 2: F-1 scores of mBERT on multi-lingual NER with 10% - 90% target language training data usage. Dashed\\nblue lines indicate scores without head pruning and solid red lines show scores with head pruning.\\n',\n",
       " 'NER\\n',\n",
       " 'EN\\nDE\\nAR\\nHE\\nZH\\n',\n",
       " 'FL\\n60.77\\n59.16\\n35.90\\n51.19\\n44.18\\n',\n",
       " 'MD\\n62.63\\n61.10\\n40.78\\n55.15\\n47.59\\n',\n",
       " 'SD\\n63.38\\n61.66\\n41.53\\n54.20\\n47.08\\n',\n",
       " 'EC\\n64.63\\n61.71\\n40.78\\n56.26\\n47.24\\n',\n",
       " 'POS\\n',\n",
       " 'EN\\nDE\\nAR\\nHE\\nZH\\n',\n",
       " 'FL\\n81.97\\n88.82\\n74.07\\n75.62\\n61.31\\n',\n",
       " 'MD\\n82.99\\n89.19\\n74.65\\n77.00\\n61.74\\n',\n",
       " 'SD\\n82.62\\n88.74\\n74.41\\n77.30\\n61.29\\n',\n",
       " 'EC\\n83.49\\n89.20\\n75.86\\n78.04\\n62.33\\n',\n",
       " 'Table 6: Cross-lingual NER (upper) and POS (lower)\\nevaluation results with multiple source languages. FL\\nindicates unpruning. MD, SD, and EC are the three\\nheuristics we examine.\\n',\n",
       " 'matrices. We also examine the efﬁcacy of pruning\\nheads based on the head rankings from a single\\nlanguage. For this heuristic, we run experiments\\nusing the head ranking matrix from each language\\nand report the highest score. We refer to the three\\nheuristics as MD, SD, and EC, respectively.\\nTable 6 displays the results. We note that in the\\nNER evaluations, the performance of mBERT on\\nall the languages but ZH are higher than those in\\nthe single-source experiments. This supports our\\nhypothesis that supervision from languages in the\\n',\n",
       " 'same family as the target language helps improve\\nmodel performance. Different from NER, the eval-\\nuation results on POS are not much higher than the\\nsingle-source evaluation scores, implying that syn-\\ntactic features are more consistent across languages\\nthan appearances of named entities. However, it\\nis consistent on both tasks that pruning attention\\nheads brings performance boosts to all the multi-\\nsource experiments. While the EC heuristic pro-\\nvides the largest improvement margin in 3 out of\\n5 experiments, it requires a lot more trial experi-\\nments. MD and SD perform comparably well in\\nmost cases so they are also promising heuristics\\nfor ranking attention heads under the multi-source\\nsetting. The results support that pruning attention\\nheads is beneﬁcial to Transformer-based models\\nin cross-lingual tasks even if the training dataset is\\nalready large and diverse in languages.\\n',\n",
       " '5.3\\nExtension to Resource-poor Languages\\n',\n",
       " 'While the languages we use in the main experi-\\nments are not truly resource-poor, we examine our\\nﬁndings when training sets in the target languages\\nare smaller. We design experiments under the multi-\\nlingual setting with subsampled training datasets\\nin target languages. Speciﬁcally, we randomly di-\\nvide the training set of each target language into 10\\ndisjoint subsets and compare model performance,\\nwith and without head pruning, using 1 to 9 sub-\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '68.8\\n',\n",
       " '70.6\\n',\n",
       " '72.4\\n',\n",
       " '74.2\\n',\n",
       " '76.0\\n',\n",
       " '77.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(a) EN-DE\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '95.0\\n',\n",
       " '95.4\\n',\n",
       " '95.8\\n',\n",
       " '96.2\\n',\n",
       " '96.6\\n',\n",
       " '97.0\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(b) EN-NL\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '51.5\\n',\n",
       " '55.1\\n',\n",
       " '58.7\\n',\n",
       " '62.3\\n',\n",
       " '65.9\\n',\n",
       " '69.5\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(c) EN-AR\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '70.8\\n',\n",
       " '74.0\\n',\n",
       " '77.3\\n',\n",
       " '80.6\\n',\n",
       " '83.9\\n',\n",
       " '87.2\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(d) EN-HE\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '87.5\\n',\n",
       " '88.7\\n',\n",
       " '89.9\\n',\n",
       " '91.1\\n',\n",
       " '92.2\\n',\n",
       " '93.4\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(e) EN-ZH\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '72.7\\n',\n",
       " '74.9\\n',\n",
       " '77.1\\n',\n",
       " '79.3\\n',\n",
       " '81.5\\n',\n",
       " '83.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(f) EN-JA\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '75.8\\n',\n",
       " '79.8\\n',\n",
       " '83.8\\n',\n",
       " '87.8\\n',\n",
       " '91.8\\n',\n",
       " '95.8\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(g) EN-FA\\n',\n",
       " '0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n',\n",
       " 'target data usage\\n',\n",
       " '88.0\\n',\n",
       " '88.8\\n',\n",
       " '89.6\\n',\n",
       " '90.4\\n',\n",
       " '91.2\\n',\n",
       " '92.0\\n',\n",
       " 'F-1\\n',\n",
       " 'pruned\\nunpruned\\n',\n",
       " '(h) EN-UR\\n',\n",
       " 'Figure 3: F-1 scores of mBERT on multi-lingual the POS task with 10% - 90% target language training data usage.\\nDashed blue lines indicate scores without head pruning and solid red lines show scores with head pruning.\\n',\n",
       " 'sets. We do not use 0 or 10 subsets since they\\ncorrespond to cross-lingual and fully multi-lingual\\nsettings, respectively. We run the evaluations on\\nNER and POS tasks. These datasets vary greatly in\\nsize, allowing us to validate our ﬁndings on target-\\nlanguage datasets with as few as 80 training exam-\\nples. The UR NER dataset is excluded from this\\ncase study since its training set is overly large. We\\nnote that the score differences with and without\\nhead pruning are, in the main experiments, con-\\nsistent for all the choices of models and source\\nlanguages. Thus, we only display the mBERT per-\\nformance with EN as the source language on NER\\nin Figure 2 and that on POS in Figure 3.\\n',\n",
       " 'The evaluation results are consistent with those\\nin our main experiments, where the model with\\nup to 12 attention heads pruned generally outper-\\nforms the full mBERT model. This further supports\\nour hypothesis that pruning lower-ranked attention\\nheads has positive effects on the performance of\\nTransformer-based models in truly resource-scarce\\nlanguages. It is also worth noting that pruning at-\\ntention heads often causes the mBERT model to\\nreach peak evaluation scores with less training data\\nin the target language. For example, in the EN-JA\\nNER experiments, the full model achieves the high-\\nest F-1 score when all the 800 training instances\\nin the JA dataset are used while the model with\\nheads pruned achieves a comparable score with\\n',\n",
       " '20% less data. This suggests that pruning attention\\nheads makes deep Transformer-based models eas-\\nier to train with less training data and thus more\\napplicable to truly resource-poor languages.\\n',\n",
       " '6\\nConclusion and Future Work\\n',\n",
       " 'This paper studied the contributions of attention\\nheads in Transformer-based models. Past research\\nhas shown that in mono-lingual tasks, pruning a\\nlarge number of attention heads can achieve com-\\nparable or higher performance than the full models.\\nHowever, we were the ﬁrst to extend these ﬁnd-\\nings to cross-lingual and multi-lingual sequence\\nlabeling tasks. Using a gradient-based method, we\\nidentiﬁed the heads to prune and showed that prun-\\ning attention heads generally has positive effects\\non mBERT and XLM-R performances. Additional\\ncase studies empirically demonstrated the valid-\\nity of our ﬁndings and showed further extensibil-\\nity of them to a wider range of task settings. In\\naddition to better understanding of Transformer-\\nbased models under cross- and multi-lingual set-\\ntings, our ﬁndings can be applied to existing models\\nto achieve better performance with reduced training\\ndata and resource consumption. Future work could\\ninclude improving model interpretability in other\\ncross-lingual and multi-lingual tasks, e.g. XNLI\\n(Conneau et al., 2018) and other passage-level clas-\\nsiﬁcation tasks.\\n',\n",
       " 'References\\n',\n",
       " 'Xilun Chen, Ahmed Hassan Awadallah, Hany Has-\\nsan, Wei Wang, and Claire Cardie. 2019.\\nMulti-\\nsource cross-lingual model transfer: Learning what\\nto share. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguis-\\ntics, pages 3098–3112, Florence, Italy. Association\\nfor Computational Linguistics.\\n',\n",
       " 'Kevin Clark, Urvashi Khandelwal, Omer Levy, and\\nChristopher D. Manning. 2019. What does BERT\\nlook at? an analysis of BERT’s attention. In Pro-\\nceedings of the 2019 ACL Workshop BlackboxNLP:\\nAnalyzing and Interpreting Neural Networks for\\nNLP, pages 276–286, Florence, Italy. Association\\nfor Computational Linguistics.\\n',\n",
       " 'Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\\ncross-lingual representation learning at scale.\\nIn\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 8440–\\n8451, Online. Association for Computational Lin-\\nguistics.\\n',\n",
       " 'Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\\nina Williams, Samuel R. Bowman, Holger Schwenk,\\nand Veselin Stoyanov. 2018.\\nXNLI: Evaluating\\ncross-lingual sentence representations. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing. Association for\\nComputational Linguistics.\\n',\n",
       " 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding.\\nIn Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers),\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\n',\n",
       " 'Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong\\nSun. 2017.\\nVisualizing and understanding neural\\nmachine translation. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 1150–\\n1159, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\n',\n",
       " 'Ramy Eskander, Smaranda Muresan, and Michael\\nCollins. 2020. Unsupervised cross-lingual part-of-\\nspeech tagging for truly low-resource scenarios. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 4820–4831, Online. Association for Computa-\\ntional Linguistics.\\n',\n",
       " 'Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\\nPathologies of neural models make interpretations\\n',\n",
       " 'difﬁcult. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 3719–3728, Brussels, Belgium. Association\\nfor Computational Linguistics.\\n',\n",
       " 'John Hewitt and Christopher D. Manning. 2019.\\nA\\nstructural probe for ﬁnding syntax in word repre-\\nsentations. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers),\\npages 4129–4138, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\n',\n",
       " 'Saﬁa Kanwal, Kamran Malik, Khurram Shahzad,\\nFaisal Aslam, and Zubair Nawaz. 2020.\\nUrdu\\nnamed entity recognition: Corpus generation and\\ndeep learning applications. ACM Trans. Asian Low\\nResour. Lang. Inf. Process., 19(1):8:1–8:13.\\n',\n",
       " 'Phillip Keung, Yichao Lu, and Vikas Bhardwaj. 2019.\\n',\n",
       " 'Adversarial learning with contextual embeddings for\\nzero-resource cross-lingual classiﬁcation and NER.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP), pages 1355–\\n1360, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\n',\n",
       " 'Olga Kovaleva, Alexey Romanov, Anna Rogers, and\\nAnna Rumshisky. 2019. Revealing the dark secrets\\nof BERT. In Proceedings of the 2019 Conference on\\nEmpirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natu-\\nral Language Processing (EMNLP-IJCNLP), pages\\n4365–4374, Hong Kong, China. Association for\\nComputational Linguistics.\\n',\n",
       " 'Taku Kudo and John Richardson. 2018. SentencePiece:\\nA simple and language independent subword tok-\\nenizer and detokenizer for neural text processing. In\\nProceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, pages 66–71, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\n',\n",
       " 'Patrick Littell, David R. Mortensen, Ke Lin, Kather-\\nine Kairis, Carlisle Turner, and Lori Levin. 2017.\\nUriel and lang2vec: Representing languages as typo-\\nlogical, geographical, and phylogenetic vectors. In\\nProceedings of the 15th Conference of the European\\nChapter of the Association for Computational Lin-\\nguistics: Volume 2, Short Papers, pages 8–14. Asso-\\nciation for Computational Linguistics.\\n',\n",
       " 'Paul Michel, Omer Levy, and Graham Neubig. 2019.\\n',\n",
       " 'Are sixteen heads really better than one?\\nIn Ad-\\nvances in Neural Information Processing Systems,\\nvolume 32, pages 14014–14024. Curran Associates,\\nInc.\\n',\n",
       " 'Behrang Mohit, Nathan Schneider, Rishav Bhowmick,\\nKemal Oﬂazer, and Noah A. Smith. 2012. Recall-\\noriented learning of named entities in Arabic\\n',\n",
       " 'Wikipedia.\\nIn Proceedings of the 13th Confer-\\nence of the European Chapter of the Association\\nfor Computational Linguistics, pages 162–173, Avi-\\ngnon, France. Association for Computational Lin-\\nguistics.\\n',\n",
       " 'Taesun Moon, Parul Aswathy, Jian Ni, and Radu\\nFlorian. 2019.\\nTowards lingua franca named\\nentity recognition with bert.\\narXiv preprint\\narXiv:1912.01389.\\n',\n",
       " 'Naama Mordecai and Michael Elhadad. 2012. Hebrew\\nnamed entity recognition.\\n',\n",
       " 'Hanieh Poostchi, Ehsan Zare Borzeshi, Mohammad\\nAbdous, and Massimo Piccardi. 2016. PersoNER:\\nPersian named-entity recognition.\\nIn Proceedings\\nof COLING 2016, the 26th International Confer-\\nence on Computational Linguistics: Technical Pa-\\npers, pages 3381–3389, Osaka, Japan. The COLING\\n2016 Organizing Committee.\\n',\n",
       " 'Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\\nsively multilingual transfer for NER.\\nIn Proceed-\\nings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 151–164, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\n',\n",
       " 'Soﬁa Serrano and Noah A. Smith. 2019. Is attention\\ninterpretable?\\nIn Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 2931–2951, Florence, Italy. Associa-\\ntion for Computational Linguistics.\\n',\n",
       " 'Oscar T¨ackstr¨om. 2012.\\nNudging the envelope of\\ndirect transfer methods for multilingual named en-\\ntity recognition.\\nIn Proceedings of the NAACL-\\nHLT Workshop on the Induction of Linguistic Struc-\\nture, pages 55–63, Montr´eal, Canada. Association\\nfor Computational Linguistics.\\n',\n",
       " 'Erik F. Tjong Kim Sang. 2002.\\nIntroduction to the\\nCoNLL-2002 shared task: Language-independent\\nnamed entity recognition.\\nIn COLING-02: The\\n6th Conference on Natural Language Learning 2002\\n(CoNLL-2002).\\n',\n",
       " 'Erik F. Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the CoNLL-2003 shared task:\\nLanguage-independent named entity recognition. In\\nProceedings of the Seventh Conference on Natu-\\nral Language Learning at HLT-NAACL 2003, pages\\n142–147.\\n',\n",
       " 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems 30: Annual Conference on Neural\\nInformation Processing Systems 2017, December 4-\\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\\n',\n",
       " 'Jesse Vig and Yonatan Belinkov. 2019.\\nAnalyzing\\nthe structure of attention in a transformer language\\nmodel. In Proceedings of the 2019 ACL Workshop\\n',\n",
       " 'BlackboxNLP: Analyzing and Interpreting Neural\\nNetworks for NLP, pages 63–76, Florence, Italy. As-\\nsociation for Computational Linguistics.\\n',\n",
       " 'Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-\\nnrich, and Ivan Titov. 2019. Analyzing multi-head\\nself-attention: Specialized heads do the heavy lift-\\ning, the rest can be pruned. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics, pages 5797–5808, Florence,\\nItaly. Association for Computational Linguistics.\\n',\n",
       " 'Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu,\\nand Ting Liu. 2019.\\nCross-lingual BERT trans-\\nformation for zero-shot dependency parsing.\\nIn\\nProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP), pages 5721–\\n5727, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\n',\n",
       " 'Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. 2020.\\nTransformers: State-of-the-art natural language pro-\\ncessing. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing:\\nSystem Demonstrations, pages 38–45, Online. Asso-\\nciation for Computational Linguistics.\\n',\n",
       " 'Qianhui Wu, Zijia Lin, B¨orje Karlsson, Jian-Guang\\nLou, and Biqing Huang. 2020. Single-/multi-source\\ncross-lingual NER via teacher-student learning on\\nunlabeled data in target language. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 6505–6514, On-\\nline. Association for Computational Linguistics.\\n',\n",
       " 'Weijia Xu, Batool Haider, and Saab Mansour. 2020.\\n',\n",
       " 'End-to-end slot alignment and recognition for cross-\\nlingual NLU. In Proceedings of the 2020 Confer-\\nence on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 5052–5063, Online. As-\\nsociation for Computational Linguistics.\\n']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f645a5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Contributions of Transformer Attention Heads\\n...\n",
       "Name: x, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pand = pd.DataFrame()\n",
    "pand['x']=[niew]\n",
    "pand['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53700537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pand['x'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3bb85291",
   "metadata": {},
   "outputs": [],
   "source": [
    "pand['y']=['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "376b5fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x     y\n",
       "0  NaN  test"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "116cf7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(niew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0891ea9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47325"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niew = \" \".join(b_Anth)\n",
    "len(niew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5340332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cd63e8",
   "metadata": {},
   "source": [
    "# use table of contents\n",
    "possible to search table of contents in text\n",
    "\n",
    "\n",
    "maybe assign blocks to toc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a120e6f",
   "metadata": {},
   "source": [
    "!!! selected arxiv code !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c41585",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23424/3379052886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! gotten arxiv code selected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'page' is not defined"
     ]
    }
   ],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! gotten arxiv code selected\n",
    "# rect = (0,0,75,500)\n",
    "# page.get_text(\"text\",clip=rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07d6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page.get_textbox(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e96e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('pdf Arxiv\\Term_Expansion_and_FinBERT_fine_tuning_for_Hypernym_and_Synonym_Ranking_of_Financial_Terms.pdf')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_Arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499724b",
   "metadata": {},
   "source": [
    "# after testing giving it a good try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8640f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Anth=[]\n",
    "for i in doc_Anth.pages():\n",
    "    l_Anth = l_Anth + i.get_text('blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a0927a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57.92100143432617,\n",
       "  86.11181640625,\n",
       "  554.0775146484375,\n",
       "  120.71693420410156,\n",
       "  'Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       "  0,\n",
       "  0),\n",
       " (217.04800415039062,\n",
       "  131.30752563476562,\n",
       "  394.4539794921875,\n",
       "  150.76959228515625,\n",
       "  'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       "  1,\n",
       "  0),\n",
       " (192.7919921875,\n",
       "  153.19454956054688,\n",
       "  419.2077331542969,\n",
       "  191.8687744140625,\n",
       "  'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       "  2,\n",
       "  0),\n",
       " (153.25799560546875,\n",
       "  214.48614501953125,\n",
       "  197.7425537109375,\n",
       "  230.03961181640625,\n",
       "  'Abstract\\n',\n",
       "  3,\n",
       "  0),\n",
       " (73.92498779296875,\n",
       "  235.214111328125,\n",
       "  277.0807189941406,\n",
       "  532.1502075195312,\n",
       "  'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmenta-\\ntion in conjunction with semantic similarity is ben-\\neﬁcial for this task and could be beneﬁcial for\\nthe other tasks that deal with short phrases. Our\\nbest performing model (Accuracy: 0.917, Rank:\\n1.156) was developed by ﬁne-tuning Sentence-\\nBERT [Reimers et al., 2019] (with FinBERT at the\\nbackend) over an extended labelled set created us-\\ning the hierarchy of labels present in FIBO.\\n',\n",
       "  4,\n",
       "  0),\n",
       " (54.000003814697266,\n",
       "  547.3707885742188,\n",
       "  136.8123016357422,\n",
       "  562.9242553710938,\n",
       "  '1\\nIntroduction\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.000003814697266,\n",
       "  564.915771484375,\n",
       "  297.0077209472656,\n",
       "  675.6611938476562,\n",
       "  'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain, contains the\\nclearly deﬁned relationship, and organizes in a deﬁned struc-\\nture mostly as a hierarchy. These properties make ontologies\\na great source for getting a deeper understanding of the rela-\\ntionship and properties of resources from the domain in con-\\nsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       "  6,\n",
       "  0),\n",
       " (65.8219985961914,\n",
       "  682.5432739257812,\n",
       "  138.95306396484375,\n",
       "  704.5194702148438,\n",
       "  '∗Contact Author\\n†Equal Contribution\\n',\n",
       "  7,\n",
       "  0),\n",
       " (314.9998779296875,\n",
       "  215.8602294921875,\n",
       "  558.0118408203125,\n",
       "  582.5603637695312,\n",
       "  'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and concept search.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\\npetitions are being held to promote the development of effec-\\ntive similarity measures. In the third edition of the competi-\\ntion FinSim-32 (being held in conjunction with 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to assign hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym assignment. We ex-\\nperimented with basic featurization methods like TF-IDF and\\nadvanced methods like pre-trained embedding models. Our\\ntop 3 systems use pre-trained FinBERT [Araci, 2019] embed-\\nding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\\ncial domain . We also augmented the training data by utilizing\\nthe knowledge from DBpedia, Investopedia, FIBO and text\\ncorpus of prospectus shared with us. We describe the works\\nrelated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results from some of them. We draw our\\nconclusions in section 7 while giving a glimpse of things that\\nwe would like to try in the future.\\n',\n",
       "  8,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  596.344970703125,\n",
       "  408.739013671875,\n",
       "  611.8984375,\n",
       "  '2\\nRelated Works\\n',\n",
       "  9,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  616.6729736328125,\n",
       "  558.0078735351562,\n",
       "  661.5553588867188,\n",
       "  'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       "  10,\n",
       "  0),\n",
       " (315.0,\n",
       "  671.830322265625,\n",
       "  556.7860717773438,\n",
       "  704.5194702148438,\n",
       "  '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       "  11,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '46\\n',\n",
       "  12,\n",
       "  0),\n",
       " (160.9980010986328,\n",
       "  765.8161010742188,\n",
       "  472.083984375,\n",
       "  784.2754516601562,\n",
       "  'Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \\n(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \\n',\n",
       "  13,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  53.8602294921875,\n",
       "  297.0091247558594,\n",
       "  351.6933288574219,\n",
       "  'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  362.8589172363281,\n",
       "  170.32211303710938,\n",
       "  378.4123840332031,\n",
       "  '3\\nProblem Statement\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  381.221923828125,\n",
       "  297.0078125,\n",
       "  535.38427734375,\n",
       "  'Given a set F consisting of n tuples of ﬁnancial terms\\nand their hypernyms/top-level concepts/labels i.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\\nnym corresponding to the ith term ti and hiϵ set of labels men-\\ntioned in Table 1. For every unseen ﬁnancial term, our task is\\nto generate a ranked list ˆyi consisting of these 17 hypernyms\\nin order of decreasing semantic similarity.\\nEvaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.99999237060547,\n",
       "  517.7305908203125,\n",
       "  196.3375701904297,\n",
       "  554.9124755859375,\n",
       "  'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       "  3,\n",
       "  0),\n",
       " (53.999996185302734,\n",
       "  530.57958984375,\n",
       "  297.00555419921875,\n",
       "  581.4196166992188,\n",
       "  'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       "  4,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  592.5841674804688,\n",
       "  96.50004577636719,\n",
       "  608.1376342773438,\n",
       "  '4\\nData\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  610.6773681640625,\n",
       "  157.62457275390625,\n",
       "  624.8699951171875,\n",
       "  '4.1\\nData Description\\n',\n",
       "  6,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  627.0411987304688,\n",
       "  297.0077819824219,\n",
       "  704.7996215820312,\n",
       "  'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels. More than 91% of the terms have 6 words or\\nless and the longest term has 22 words. There were 10 du-\\nplicate entries, and 3 terms were assigned 2 different labels.\\nAlong with this, a corpus of prospectuses in English was pro-\\nvided that had 211 documents. Some of the terms mentioned\\n',\n",
       "  7,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  52.50552749633789,\n",
       "  514.7023315429688,\n",
       "  65.46739196777344,\n",
       "  'Label\\nCount\\n',\n",
       "  8,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  64.22119140625,\n",
       "  503.0755310058594,\n",
       "  76.22660827636719,\n",
       "  'Equity Index\\n280\\n',\n",
       "  9,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  75.57916259765625,\n",
       "  503.0755310058594,\n",
       "  87.58457946777344,\n",
       "  'Regulatory Agency\\n205\\n',\n",
       "  10,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  86.93621826171875,\n",
       "  503.0755310058594,\n",
       "  98.94163513183594,\n",
       "  'Credit Index\\n125\\n',\n",
       "  11,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  98.293212890625,\n",
       "  503.0755310058594,\n",
       "  110.29862976074219,\n",
       "  'Central Securities Depository\\n107\\n',\n",
       "  12,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  109.65118408203125,\n",
       "  498.0940246582031,\n",
       "  121.65660095214844,\n",
       "  'Debt pricing and yields\\n58\\n',\n",
       "  13,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  121.0081787109375,\n",
       "  498.0940246582031,\n",
       "  133.0135955810547,\n",
       "  'Bonds\\n55\\n',\n",
       "  14,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  132.3662109375,\n",
       "  498.0940246582031,\n",
       "  144.3716278076172,\n",
       "  'Swap\\n36\\n',\n",
       "  15,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  143.72320556640625,\n",
       "  498.0940246582031,\n",
       "  155.72862243652344,\n",
       "  'Stock Corporation\\n25\\n',\n",
       "  16,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  155.0802001953125,\n",
       "  498.0940246582031,\n",
       "  167.0856170654297,\n",
       "  'Option\\n24\\n',\n",
       "  17,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  166.43817138671875,\n",
       "  498.0940246582031,\n",
       "  178.44358825683594,\n",
       "  'Funds\\n22\\n',\n",
       "  18,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  177.795166015625,\n",
       "  498.0940246582031,\n",
       "  189.8005828857422,\n",
       "  'Future\\n19\\n',\n",
       "  19,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  189.1531982421875,\n",
       "  498.0940246582031,\n",
       "  201.1586151123047,\n",
       "  'Credit Events\\n18\\n',\n",
       "  20,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  200.51019287109375,\n",
       "  498.0940246582031,\n",
       "  212.51560974121094,\n",
       "  'MMIs\\n17\\n',\n",
       "  21,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  211.8671875,\n",
       "  498.0940246582031,\n",
       "  223.8726043701172,\n",
       "  'Stocks\\n17\\n',\n",
       "  22,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  223.2252197265625,\n",
       "  498.0940246582031,\n",
       "  235.2306365966797,\n",
       "  'Parametric schedules\\n15\\n',\n",
       "  23,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  234.58221435546875,\n",
       "  493.1125183105469,\n",
       "  246.58763122558594,\n",
       "  'Forward\\n9\\n',\n",
       "  24,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  245.940185546875,\n",
       "  493.1125183105469,\n",
       "  257.9455871582031,\n",
       "  'Securities restrictions\\n8\\n',\n",
       "  25,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  256.9385070800781,\n",
       "  508.0570373535156,\n",
       "  269.9003601074219,\n",
       "  'Total\\n1040\\n',\n",
       "  26,\n",
       "  0),\n",
       " (356.3590087890625,\n",
       "  278.8414306640625,\n",
       "  516.6351318359375,\n",
       "  289.64544677734375,\n",
       "  'Table 1: Label distribution in the training set\\n',\n",
       "  27,\n",
       "  0),\n",
       " (315.0,\n",
       "  307.34619140625,\n",
       "  558.0078735351562,\n",
       "  330.3096008300781,\n",
       "  'in the training data were present in the corpus. Table 1 shows\\nthe distribution of these labels in the training set.\\n',\n",
       "  28,\n",
       "  0),\n",
       " (315.0,\n",
       "  337.7643737792969,\n",
       "  558.0083618164062,\n",
       "  409.6215515136719,\n",
       "  '4.2\\nData Augmentation\\nSince the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       "  29,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  419.4424743652344,\n",
       "  558.0099487304688,\n",
       "  519.4776000976562,\n",
       "  'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       "  30,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  522.9501953125,\n",
       "  545.7528076171875,\n",
       "  534.9556274414062,\n",
       "  '• expansion had equal or less length than the acronym.\\n',\n",
       "  31,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  538.4281616210938,\n",
       "  439.3677978515625,\n",
       "  550.43359375,\n",
       "  '• expansion had parenthesis\\n',\n",
       "  32,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  553.9061279296875,\n",
       "  558.0073852539062,\n",
       "  576.87060546875,\n",
       "  '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       "  33,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  580.3431396484375,\n",
       "  541.7377319335938,\n",
       "  592.3485717773438,\n",
       "  '• the expansion had less than or equal to 5 characters.\\n',\n",
       "  34,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  595.8221435546875,\n",
       "  558.0078125,\n",
       "  640.70361328125,\n",
       "  'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       "  35,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  650.5245361328125,\n",
       "  558.0111083984375,\n",
       "  673.8475952148438,\n",
       "  'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the\\n',\n",
       "  36,\n",
       "  0),\n",
       " (327.65301513671875,\n",
       "  681.7933349609375,\n",
       "  464.4634094238281,\n",
       "  704.5194702148438,\n",
       "  '3https://spacy.io/\\n4https://lookup.dbpedia.org/api/search\\n',\n",
       "  37,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '47\\n',\n",
       "  38,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  54.7572021484375,\n",
       "  297.0076904296875,\n",
       "  253.0634002685547,\n",
       "  'train and test sets.\\nWe present such an example in Fig-\\nure 1. In addition to the description, the label was also re-\\ntained from the result payload to identify the right descrip-\\ntion for the input terms. We tried token overlap-based simi-\\nlarity of input terms with both matching labels and descrip-\\ntions. We decided to use the label to term match for descrip-\\ntion matching after going through a randomly drawn sam-\\nple. We cleaned both input terms and labels from DBpedia\\nresults by converting them to lower case, replacing punctua-\\ntions by space, removing repetitive spaces, and singularizing\\nthe text. We calculated the token overlap ratios for cleaned\\nterm and DBpedia labels using the formulas mentioned be-\\nlow: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\\nlength(s2/length(s1 where s1 and s2 represents sets of to-\\nkenized cleaned terms and tokenized and cleaned DBpedia\\nlabels respectively. We empirically decided to use all the in-\\nstances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\\na DBpedia label (and hence description) to the input term.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  263.1593322753906,\n",
       "  297.0082092285156,\n",
       "  473.3262023925781,\n",
       "  'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1801 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm ”callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  485.1318054199219,\n",
       "  170.54925537109375,\n",
       "  500.6852722167969,\n",
       "  '5\\nSystem Description\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  503.97479248046875,\n",
       "  297.0076904296875,\n",
       "  614.6102294921875,\n",
       "  'We tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 841 and 209 terms respectively.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  622.8909301757812,\n",
       "  147.92640686035156,\n",
       "  637.0835571289062,\n",
       "  '5.1\\nSystem - 1 (S1)\\n',\n",
       "  4,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  639.7337646484375,\n",
       "  297.0076904296875,\n",
       "  673.6572265625,\n",
       "  'This is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and stuck to the original set that\\nwas shared by organizers. We loaded FinBERT pre-trained\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.0,\n",
       "  682.704345703125,\n",
       "  273.7669372558594,\n",
       "  704.5194702148438,\n",
       "  '5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       "  6,\n",
       "  0),\n",
       " (315.0,\n",
       "  54.7572021484375,\n",
       "  558.0079345703125,\n",
       "  154.4335174560547,\n",
       "  'model and ﬁne-tuned it by trying to classify the representa-\\ntion of [CLS] token into one of the 17 labels mentioned pre-\\nviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       "  7,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  163.93324279785156,\n",
       "  408.92645263671875,\n",
       "  178.1258544921875,\n",
       "  '5.2\\nSystem - 2 (S2)\\n',\n",
       "  8,\n",
       "  0),\n",
       " (315.0,\n",
       "  181.99407958984375,\n",
       "  558.0079956054688,\n",
       "  270.7123718261719,\n",
       "  'This system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset has the descriptions of the terms,\\nthe input is considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       "  9,\n",
       "  0),\n",
       " (315.0,\n",
       "  280.212158203125,\n",
       "  406.19921875,\n",
       "  294.4047546386719,\n",
       "  '5.3\\nSystem -3 (S3)\\n',\n",
       "  10,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  298.2729797363281,\n",
       "  558.0079345703125,\n",
       "  704.7991333007812,\n",
       "  'We explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       "  11,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '48\\n',\n",
       "  12,\n",
       "  0),\n",
       " (54.0,\n",
       "  53.999427795410156,\n",
       "  558.0035400390625,\n",
       "  167.385986328125,\n",
       "  '<image: DeviceRGB, width: 853, height: 188, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (214.45899963378906,\n",
       "  176.03842163085938,\n",
       "  397.5357360839844,\n",
       "  186.84243774414062,\n",
       "  'Figure 1: Sample output from DBpedia search API\\n',\n",
       "  1,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  197.77151489257812,\n",
       "  501.2467956542969,\n",
       "  210.73338317871094,\n",
       "  'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       "  2,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  209.4881591796875,\n",
       "  549.5474853515625,\n",
       "  232.4525604248047,\n",
       "  'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       "  3,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  231.80419921875,\n",
       "  494.7907409667969,\n",
       "  254.7686004638672,\n",
       "  'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       "  4,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  254.12017822265625,\n",
       "  422.7373046875,\n",
       "  288.0435485839844,\n",
       "  'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       "  5,\n",
       "  0),\n",
       " (434.6759948730469,\n",
       "  265.378173828125,\n",
       "  507.5135192871094,\n",
       "  277.3835754394531,\n",
       "  'Bonds\\nDBpedia\\n',\n",
       "  6,\n",
       "  0),\n",
       " (186.55599975585938,\n",
       "  297.5824279785156,\n",
       "  425.4371032714844,\n",
       "  308.3864440917969,\n",
       "  'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       "  7,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  325.29351806640625,\n",
       "  242.35931396484375,\n",
       "  338.25537109375,\n",
       "  'Data Source\\nCount\\n',\n",
       "  8,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  337.0091857910156,\n",
       "  235.71401977539062,\n",
       "  349.01458740234375,\n",
       "  'Original modelling data\\n1040\\n',\n",
       "  9,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  348.3671875,\n",
       "  230.73251342773438,\n",
       "  360.3725891113281,\n",
       "  'DBpedia\\n257\\n',\n",
       "  10,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  359.72418212890625,\n",
       "  230.73251342773438,\n",
       "  371.7295837402344,\n",
       "  'FIBO\\n236\\n',\n",
       "  11,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  371.0821838378906,\n",
       "  225.75100708007812,\n",
       "  383.08758544921875,\n",
       "  'Investopedia\\n85\\n',\n",
       "  12,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  382.4391784667969,\n",
       "  230.73251342773438,\n",
       "  394.444580078125,\n",
       "  'Acronym expansion\\n218\\n',\n",
       "  13,\n",
       "  0),\n",
       " (104.73400115966797,\n",
       "  403.9834289550781,\n",
       "  246.2624053955078,\n",
       "  414.7874450683594,\n",
       "  'Table 3: Details of various data sources\\n',\n",
       "  14,\n",
       "  0),\n",
       " (54.0,\n",
       "  432.51519775390625,\n",
       "  297.0077209472656,\n",
       "  455.4795837402344,\n",
       "  'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (54.0,\n",
       "  466.2371826171875,\n",
       "  220.7125244140625,\n",
       "  481.7906494140625,\n",
       "  '6\\nExperimentation and Results\\n',\n",
       "  16,\n",
       "  0),\n",
       " (54.0,\n",
       "  484.294189453125,\n",
       "  297.0076904296875,\n",
       "  704.7996215820312,\n",
       "  'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1469 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only change between these runs was the data,\\n',\n",
       "  17,\n",
       "  0),\n",
       " (315.0,\n",
       "  327.545166015625,\n",
       "  558.0079956054688,\n",
       "  558.7284545898438,\n",
       "  'the improvement can be attributed to the expanded data.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models are already\\ngiven in the system description. After rigorous experimenta-\\ntion, these hyperparameters were selected empirically based\\non validation set performance. The results are presented in\\nTable 4. Since the number of submissions was restricted to\\n3 for each team, we do not have the performance numbers of\\nthe BERT models in the test set. Analysing the results we see\\nthat SentenceBERT trained with FinBERT at the backed as\\nmentioned in section-5.3 performed the best.\\n',\n",
       "  18,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  566.781005859375,\n",
       "  558.0079956054688,\n",
       "  704.7994384765625,\n",
       "  '7\\nConclusion and Future Works\\nIn this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\nApart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\n',\n",
       "  19,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '49\\n',\n",
       "  20,\n",
       "  0),\n",
       " (164.2689971923828,\n",
       "  53.997528076171875,\n",
       "  447.72991943359375,\n",
       "  214.14398193359375,\n",
       "  '<image: DeviceRGB, width: 2689, height: 1878, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (170.41200256347656,\n",
       "  222.79544067382812,\n",
       "  441.5796203613281,\n",
       "  233.59945678710938,\n",
       "  'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (161.54200744628906,\n",
       "  250.50650024414062,\n",
       "  262.9822082519531,\n",
       "  263.4683532714844,\n",
       "  'Validation set\\nTest set\\n',\n",
       "  2,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  261.8645324707031,\n",
       "  285.2682189941406,\n",
       "  274.8263854980469,\n",
       "  'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  273.5802001953125,\n",
       "  289.1537780761719,\n",
       "  285.5856018066406,\n",
       "  'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       "  4,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  284.9382019042969,\n",
       "  289.1537780761719,\n",
       "  296.943603515625,\n",
       "  'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       "  5,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  296.2951965332031,\n",
       "  270.0546875,\n",
       "  308.30059814453125,\n",
       "  'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       "  6,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  307.6521911621094,\n",
       "  270.0546875,\n",
       "  319.6575927734375,\n",
       "  'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       "  7,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  319.01019287109375,\n",
       "  289.1537780761719,\n",
       "  331.0155944824219,\n",
       "  'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       "  8,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  330.3671875,\n",
       "  289.1537780761719,\n",
       "  342.3725891113281,\n",
       "  'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       "  9,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  341.36651611328125,\n",
       "  289.1537780761719,\n",
       "  354.328369140625,\n",
       "  'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       "  10,\n",
       "  0),\n",
       " (54.0,\n",
       "  363.2684326171875,\n",
       "  296.9877014160156,\n",
       "  384.03546142578125,\n",
       "  'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       "  11,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  401.7261962890625,\n",
       "  297.0082702636719,\n",
       "  545.50048828125,\n",
       "  'that were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create pos-\\nitive and negative samples.\\n',\n",
       "  12,\n",
       "  0),\n",
       " (53.99996566772461,\n",
       "  556.18408203125,\n",
       "  297.0168151855469,\n",
       "  704.7994995117188,\n",
       "  'References\\n[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n',\n",
       "  13,\n",
       "  0),\n",
       " (314.9999694824219,\n",
       "  251.862060546875,\n",
       "  558.0069580078125,\n",
       "  319.5583801269531,\n",
       "  '[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n',\n",
       "  14,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  324.5259704589844,\n",
       "  558.0167846679688,\n",
       "  414.1402893066406,\n",
       "  '[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  419.10888671875,\n",
       "  558.0068359375,\n",
       "  475.8462219238281,\n",
       "  '[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n',\n",
       "  16,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  480.8138122558594,\n",
       "  558.0104370117188,\n",
       "  548.5111694335938,\n",
       "  '[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n',\n",
       "  17,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  553.478759765625,\n",
       "  558.0086059570312,\n",
       "  654.0521850585938,\n",
       "  '[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n',\n",
       "  18,\n",
       "  0),\n",
       " (314.9998779296875,\n",
       "  659.019775390625,\n",
       "  558.0068359375,\n",
       "  704.7991943359375,\n",
       "  '[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\n',\n",
       "  19,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '50\\n',\n",
       "  20,\n",
       "  0),\n",
       " (53.99995803833008,\n",
       "  54.7572021484375,\n",
       "  297.0168151855469,\n",
       "  704.799072265625,\n",
       "  'Association for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\n',\n",
       "  0,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  54.75665283203125,\n",
       "  558.00830078125,\n",
       "  508.2705993652344,\n",
       "  'cial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '51\\n',\n",
       "  2,\n",
       "  0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "463de6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57.92100143432617,\n",
       "  86.11181640625,\n",
       "  554.0775146484375,\n",
       "  120.71693420410156,\n",
       "  'Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       "  0,\n",
       "  0),\n",
       " (217.04800415039062,\n",
       "  131.30752563476562,\n",
       "  394.4539794921875,\n",
       "  150.76959228515625,\n",
       "  'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       "  1,\n",
       "  0),\n",
       " (192.7919921875,\n",
       "  153.19454956054688,\n",
       "  419.2077331542969,\n",
       "  191.8687744140625,\n",
       "  'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       "  2,\n",
       "  0),\n",
       " (153.25799560546875,\n",
       "  214.48614501953125,\n",
       "  197.7425537109375,\n",
       "  230.03961181640625,\n",
       "  'Abstract\\n',\n",
       "  3,\n",
       "  0),\n",
       " (73.92498779296875,\n",
       "  235.214111328125,\n",
       "  277.0807189941406,\n",
       "  532.1502075195312,\n",
       "  'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmenta-\\ntion in conjunction with semantic similarity is ben-\\neﬁcial for this task and could be beneﬁcial for\\nthe other tasks that deal with short phrases. Our\\nbest performing model (Accuracy: 0.917, Rank:\\n1.156) was developed by ﬁne-tuning Sentence-\\nBERT [Reimers et al., 2019] (with FinBERT at the\\nbackend) over an extended labelled set created us-\\ning the hierarchy of labels present in FIBO.\\n',\n",
       "  4,\n",
       "  0),\n",
       " (54.000003814697266,\n",
       "  547.3707885742188,\n",
       "  136.8123016357422,\n",
       "  562.9242553710938,\n",
       "  '1\\nIntroduction\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.000003814697266,\n",
       "  564.915771484375,\n",
       "  297.0077209472656,\n",
       "  675.6611938476562,\n",
       "  'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain, contains the\\nclearly deﬁned relationship, and organizes in a deﬁned struc-\\nture mostly as a hierarchy. These properties make ontologies\\na great source for getting a deeper understanding of the rela-\\ntionship and properties of resources from the domain in con-\\nsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       "  6,\n",
       "  0),\n",
       " (65.8219985961914,\n",
       "  682.5432739257812,\n",
       "  138.95306396484375,\n",
       "  704.5194702148438,\n",
       "  '∗Contact Author\\n†Equal Contribution\\n',\n",
       "  7,\n",
       "  0),\n",
       " (314.9998779296875,\n",
       "  215.8602294921875,\n",
       "  558.0118408203125,\n",
       "  582.5603637695312,\n",
       "  'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and concept search.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\\npetitions are being held to promote the development of effec-\\ntive similarity measures. In the third edition of the competi-\\ntion FinSim-32 (being held in conjunction with 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to assign hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym assignment. We ex-\\nperimented with basic featurization methods like TF-IDF and\\nadvanced methods like pre-trained embedding models. Our\\ntop 3 systems use pre-trained FinBERT [Araci, 2019] embed-\\nding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\\ncial domain . We also augmented the training data by utilizing\\nthe knowledge from DBpedia, Investopedia, FIBO and text\\ncorpus of prospectus shared with us. We describe the works\\nrelated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results from some of them. We draw our\\nconclusions in section 7 while giving a glimpse of things that\\nwe would like to try in the future.\\n',\n",
       "  8,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  596.344970703125,\n",
       "  408.739013671875,\n",
       "  611.8984375,\n",
       "  '2\\nRelated Works\\n',\n",
       "  9,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  616.6729736328125,\n",
       "  558.0078735351562,\n",
       "  661.5553588867188,\n",
       "  'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       "  10,\n",
       "  0),\n",
       " (315.0,\n",
       "  671.830322265625,\n",
       "  556.7860717773438,\n",
       "  704.5194702148438,\n",
       "  '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       "  11,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '46\\n',\n",
       "  12,\n",
       "  0),\n",
       " (160.9980010986328,\n",
       "  765.8161010742188,\n",
       "  472.083984375,\n",
       "  784.2754516601562,\n",
       "  'Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \\n(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \\n',\n",
       "  13,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  53.8602294921875,\n",
       "  297.0091247558594,\n",
       "  351.6933288574219,\n",
       "  'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  362.8589172363281,\n",
       "  170.32211303710938,\n",
       "  378.4123840332031,\n",
       "  '3\\nProblem Statement\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  381.221923828125,\n",
       "  297.0078125,\n",
       "  535.38427734375,\n",
       "  'Given a set F consisting of n tuples of ﬁnancial terms\\nand their hypernyms/top-level concepts/labels i.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\\nnym corresponding to the ith term ti and hiϵ set of labels men-\\ntioned in Table 1. For every unseen ﬁnancial term, our task is\\nto generate a ranked list ˆyi consisting of these 17 hypernyms\\nin order of decreasing semantic similarity.\\nEvaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.99999237060547,\n",
       "  517.7305908203125,\n",
       "  196.3375701904297,\n",
       "  554.9124755859375,\n",
       "  'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       "  3,\n",
       "  0),\n",
       " (53.999996185302734,\n",
       "  530.57958984375,\n",
       "  297.00555419921875,\n",
       "  581.4196166992188,\n",
       "  'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       "  4,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  592.5841674804688,\n",
       "  96.50004577636719,\n",
       "  608.1376342773438,\n",
       "  '4\\nData\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  610.6773681640625,\n",
       "  157.62457275390625,\n",
       "  624.8699951171875,\n",
       "  '4.1\\nData Description\\n',\n",
       "  6,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  627.0411987304688,\n",
       "  297.0077819824219,\n",
       "  704.7996215820312,\n",
       "  'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels. More than 91% of the terms have 6 words or\\nless and the longest term has 22 words. There were 10 du-\\nplicate entries, and 3 terms were assigned 2 different labels.\\nAlong with this, a corpus of prospectuses in English was pro-\\nvided that had 211 documents. Some of the terms mentioned\\n',\n",
       "  7,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  52.50552749633789,\n",
       "  514.7023315429688,\n",
       "  65.46739196777344,\n",
       "  'Label\\nCount\\n',\n",
       "  8,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  64.22119140625,\n",
       "  503.0755310058594,\n",
       "  76.22660827636719,\n",
       "  'Equity Index\\n280\\n',\n",
       "  9,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  75.57916259765625,\n",
       "  503.0755310058594,\n",
       "  87.58457946777344,\n",
       "  'Regulatory Agency\\n205\\n',\n",
       "  10,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  86.93621826171875,\n",
       "  503.0755310058594,\n",
       "  98.94163513183594,\n",
       "  'Credit Index\\n125\\n',\n",
       "  11,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  98.293212890625,\n",
       "  503.0755310058594,\n",
       "  110.29862976074219,\n",
       "  'Central Securities Depository\\n107\\n',\n",
       "  12,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  109.65118408203125,\n",
       "  498.0940246582031,\n",
       "  121.65660095214844,\n",
       "  'Debt pricing and yields\\n58\\n',\n",
       "  13,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  121.0081787109375,\n",
       "  498.0940246582031,\n",
       "  133.0135955810547,\n",
       "  'Bonds\\n55\\n',\n",
       "  14,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  132.3662109375,\n",
       "  498.0940246582031,\n",
       "  144.3716278076172,\n",
       "  'Swap\\n36\\n',\n",
       "  15,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  143.72320556640625,\n",
       "  498.0940246582031,\n",
       "  155.72862243652344,\n",
       "  'Stock Corporation\\n25\\n',\n",
       "  16,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  155.0802001953125,\n",
       "  498.0940246582031,\n",
       "  167.0856170654297,\n",
       "  'Option\\n24\\n',\n",
       "  17,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  166.43817138671875,\n",
       "  498.0940246582031,\n",
       "  178.44358825683594,\n",
       "  'Funds\\n22\\n',\n",
       "  18,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  177.795166015625,\n",
       "  498.0940246582031,\n",
       "  189.8005828857422,\n",
       "  'Future\\n19\\n',\n",
       "  19,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  189.1531982421875,\n",
       "  498.0940246582031,\n",
       "  201.1586151123047,\n",
       "  'Credit Events\\n18\\n',\n",
       "  20,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  200.51019287109375,\n",
       "  498.0940246582031,\n",
       "  212.51560974121094,\n",
       "  'MMIs\\n17\\n',\n",
       "  21,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  211.8671875,\n",
       "  498.0940246582031,\n",
       "  223.8726043701172,\n",
       "  'Stocks\\n17\\n',\n",
       "  22,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  223.2252197265625,\n",
       "  498.0940246582031,\n",
       "  235.2306365966797,\n",
       "  'Parametric schedules\\n15\\n',\n",
       "  23,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  234.58221435546875,\n",
       "  493.1125183105469,\n",
       "  246.58763122558594,\n",
       "  'Forward\\n9\\n',\n",
       "  24,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  245.940185546875,\n",
       "  493.1125183105469,\n",
       "  257.9455871582031,\n",
       "  'Securities restrictions\\n8\\n',\n",
       "  25,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  256.9385070800781,\n",
       "  508.0570373535156,\n",
       "  269.9003601074219,\n",
       "  'Total\\n1040\\n',\n",
       "  26,\n",
       "  0),\n",
       " (356.3590087890625,\n",
       "  278.8414306640625,\n",
       "  516.6351318359375,\n",
       "  289.64544677734375,\n",
       "  'Table 1: Label distribution in the training set\\n',\n",
       "  27,\n",
       "  0),\n",
       " (315.0,\n",
       "  307.34619140625,\n",
       "  558.0078735351562,\n",
       "  330.3096008300781,\n",
       "  'in the training data were present in the corpus. Table 1 shows\\nthe distribution of these labels in the training set.\\n',\n",
       "  28,\n",
       "  0),\n",
       " (315.0,\n",
       "  337.7643737792969,\n",
       "  558.0083618164062,\n",
       "  409.6215515136719,\n",
       "  '4.2\\nData Augmentation\\nSince the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       "  29,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  419.4424743652344,\n",
       "  558.0099487304688,\n",
       "  519.4776000976562,\n",
       "  'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       "  30,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  522.9501953125,\n",
       "  545.7528076171875,\n",
       "  534.9556274414062,\n",
       "  '• expansion had equal or less length than the acronym.\\n',\n",
       "  31,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  538.4281616210938,\n",
       "  439.3677978515625,\n",
       "  550.43359375,\n",
       "  '• expansion had parenthesis\\n',\n",
       "  32,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  553.9061279296875,\n",
       "  558.0073852539062,\n",
       "  576.87060546875,\n",
       "  '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       "  33,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  580.3431396484375,\n",
       "  541.7377319335938,\n",
       "  592.3485717773438,\n",
       "  '• the expansion had less than or equal to 5 characters.\\n',\n",
       "  34,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  595.8221435546875,\n",
       "  558.0078125,\n",
       "  640.70361328125,\n",
       "  'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       "  35,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  650.5245361328125,\n",
       "  558.0111083984375,\n",
       "  673.8475952148438,\n",
       "  'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the\\n',\n",
       "  36,\n",
       "  0),\n",
       " (327.65301513671875,\n",
       "  681.7933349609375,\n",
       "  464.4634094238281,\n",
       "  704.5194702148438,\n",
       "  '3https://spacy.io/\\n4https://lookup.dbpedia.org/api/search\\n',\n",
       "  37,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '47\\n',\n",
       "  38,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  54.7572021484375,\n",
       "  297.0076904296875,\n",
       "  253.0634002685547,\n",
       "  'train and test sets.\\nWe present such an example in Fig-\\nure 1. In addition to the description, the label was also re-\\ntained from the result payload to identify the right descrip-\\ntion for the input terms. We tried token overlap-based simi-\\nlarity of input terms with both matching labels and descrip-\\ntions. We decided to use the label to term match for descrip-\\ntion matching after going through a randomly drawn sam-\\nple. We cleaned both input terms and labels from DBpedia\\nresults by converting them to lower case, replacing punctua-\\ntions by space, removing repetitive spaces, and singularizing\\nthe text. We calculated the token overlap ratios for cleaned\\nterm and DBpedia labels using the formulas mentioned be-\\nlow: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\\nlength(s2/length(s1 where s1 and s2 represents sets of to-\\nkenized cleaned terms and tokenized and cleaned DBpedia\\nlabels respectively. We empirically decided to use all the in-\\nstances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\\na DBpedia label (and hence description) to the input term.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  263.1593322753906,\n",
       "  297.0082092285156,\n",
       "  473.3262023925781,\n",
       "  'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1801 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm ”callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  485.1318054199219,\n",
       "  170.54925537109375,\n",
       "  500.6852722167969,\n",
       "  '5\\nSystem Description\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  503.97479248046875,\n",
       "  297.0076904296875,\n",
       "  614.6102294921875,\n",
       "  'We tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 841 and 209 terms respectively.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  622.8909301757812,\n",
       "  147.92640686035156,\n",
       "  637.0835571289062,\n",
       "  '5.1\\nSystem - 1 (S1)\\n',\n",
       "  4,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  639.7337646484375,\n",
       "  297.0076904296875,\n",
       "  673.6572265625,\n",
       "  'This is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and stuck to the original set that\\nwas shared by organizers. We loaded FinBERT pre-trained\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.0,\n",
       "  682.704345703125,\n",
       "  273.7669372558594,\n",
       "  704.5194702148438,\n",
       "  '5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       "  6,\n",
       "  0),\n",
       " (315.0,\n",
       "  54.7572021484375,\n",
       "  558.0079345703125,\n",
       "  154.4335174560547,\n",
       "  'model and ﬁne-tuned it by trying to classify the representa-\\ntion of [CLS] token into one of the 17 labels mentioned pre-\\nviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       "  7,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  163.93324279785156,\n",
       "  408.92645263671875,\n",
       "  178.1258544921875,\n",
       "  '5.2\\nSystem - 2 (S2)\\n',\n",
       "  8,\n",
       "  0),\n",
       " (315.0,\n",
       "  181.99407958984375,\n",
       "  558.0079956054688,\n",
       "  270.7123718261719,\n",
       "  'This system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset has the descriptions of the terms,\\nthe input is considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       "  9,\n",
       "  0),\n",
       " (315.0,\n",
       "  280.212158203125,\n",
       "  406.19921875,\n",
       "  294.4047546386719,\n",
       "  '5.3\\nSystem -3 (S3)\\n',\n",
       "  10,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  298.2729797363281,\n",
       "  558.0079345703125,\n",
       "  704.7991333007812,\n",
       "  'We explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       "  11,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '48\\n',\n",
       "  12,\n",
       "  0),\n",
       " (54.0,\n",
       "  53.999427795410156,\n",
       "  558.0035400390625,\n",
       "  167.385986328125,\n",
       "  '<image: DeviceRGB, width: 853, height: 188, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (214.45899963378906,\n",
       "  176.03842163085938,\n",
       "  397.5357360839844,\n",
       "  186.84243774414062,\n",
       "  'Figure 1: Sample output from DBpedia search API\\n',\n",
       "  1,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  197.77151489257812,\n",
       "  501.2467956542969,\n",
       "  210.73338317871094,\n",
       "  'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       "  2,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  209.4881591796875,\n",
       "  549.5474853515625,\n",
       "  232.4525604248047,\n",
       "  'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       "  3,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  231.80419921875,\n",
       "  494.7907409667969,\n",
       "  254.7686004638672,\n",
       "  'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       "  4,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  254.12017822265625,\n",
       "  422.7373046875,\n",
       "  288.0435485839844,\n",
       "  'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       "  5,\n",
       "  0),\n",
       " (434.6759948730469,\n",
       "  265.378173828125,\n",
       "  507.5135192871094,\n",
       "  277.3835754394531,\n",
       "  'Bonds\\nDBpedia\\n',\n",
       "  6,\n",
       "  0),\n",
       " (186.55599975585938,\n",
       "  297.5824279785156,\n",
       "  425.4371032714844,\n",
       "  308.3864440917969,\n",
       "  'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       "  7,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  325.29351806640625,\n",
       "  242.35931396484375,\n",
       "  338.25537109375,\n",
       "  'Data Source\\nCount\\n',\n",
       "  8,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  337.0091857910156,\n",
       "  235.71401977539062,\n",
       "  349.01458740234375,\n",
       "  'Original modelling data\\n1040\\n',\n",
       "  9,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  348.3671875,\n",
       "  230.73251342773438,\n",
       "  360.3725891113281,\n",
       "  'DBpedia\\n257\\n',\n",
       "  10,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  359.72418212890625,\n",
       "  230.73251342773438,\n",
       "  371.7295837402344,\n",
       "  'FIBO\\n236\\n',\n",
       "  11,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  371.0821838378906,\n",
       "  225.75100708007812,\n",
       "  383.08758544921875,\n",
       "  'Investopedia\\n85\\n',\n",
       "  12,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  382.4391784667969,\n",
       "  230.73251342773438,\n",
       "  394.444580078125,\n",
       "  'Acronym expansion\\n218\\n',\n",
       "  13,\n",
       "  0),\n",
       " (104.73400115966797,\n",
       "  403.9834289550781,\n",
       "  246.2624053955078,\n",
       "  414.7874450683594,\n",
       "  'Table 3: Details of various data sources\\n',\n",
       "  14,\n",
       "  0),\n",
       " (54.0,\n",
       "  432.51519775390625,\n",
       "  297.0077209472656,\n",
       "  455.4795837402344,\n",
       "  'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (54.0,\n",
       "  466.2371826171875,\n",
       "  220.7125244140625,\n",
       "  481.7906494140625,\n",
       "  '6\\nExperimentation and Results\\n',\n",
       "  16,\n",
       "  0),\n",
       " (54.0,\n",
       "  484.294189453125,\n",
       "  297.0076904296875,\n",
       "  704.7996215820312,\n",
       "  'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1469 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only change between these runs was the data,\\n',\n",
       "  17,\n",
       "  0),\n",
       " (315.0,\n",
       "  327.545166015625,\n",
       "  558.0079956054688,\n",
       "  558.7284545898438,\n",
       "  'the improvement can be attributed to the expanded data.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models are already\\ngiven in the system description. After rigorous experimenta-\\ntion, these hyperparameters were selected empirically based\\non validation set performance. The results are presented in\\nTable 4. Since the number of submissions was restricted to\\n3 for each team, we do not have the performance numbers of\\nthe BERT models in the test set. Analysing the results we see\\nthat SentenceBERT trained with FinBERT at the backed as\\nmentioned in section-5.3 performed the best.\\n',\n",
       "  18,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  566.781005859375,\n",
       "  558.0079956054688,\n",
       "  704.7994384765625,\n",
       "  '7\\nConclusion and Future Works\\nIn this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\nApart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\n',\n",
       "  19,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '49\\n',\n",
       "  20,\n",
       "  0),\n",
       " (164.2689971923828,\n",
       "  53.997528076171875,\n",
       "  447.72991943359375,\n",
       "  214.14398193359375,\n",
       "  '<image: DeviceRGB, width: 2689, height: 1878, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (170.41200256347656,\n",
       "  222.79544067382812,\n",
       "  441.5796203613281,\n",
       "  233.59945678710938,\n",
       "  'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (161.54200744628906,\n",
       "  250.50650024414062,\n",
       "  262.9822082519531,\n",
       "  263.4683532714844,\n",
       "  'Validation set\\nTest set\\n',\n",
       "  2,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  261.8645324707031,\n",
       "  285.2682189941406,\n",
       "  274.8263854980469,\n",
       "  'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  273.5802001953125,\n",
       "  289.1537780761719,\n",
       "  285.5856018066406,\n",
       "  'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       "  4,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  284.9382019042969,\n",
       "  289.1537780761719,\n",
       "  296.943603515625,\n",
       "  'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       "  5,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  296.2951965332031,\n",
       "  270.0546875,\n",
       "  308.30059814453125,\n",
       "  'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       "  6,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  307.6521911621094,\n",
       "  270.0546875,\n",
       "  319.6575927734375,\n",
       "  'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       "  7,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  319.01019287109375,\n",
       "  289.1537780761719,\n",
       "  331.0155944824219,\n",
       "  'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       "  8,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  330.3671875,\n",
       "  289.1537780761719,\n",
       "  342.3725891113281,\n",
       "  'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       "  9,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  341.36651611328125,\n",
       "  289.1537780761719,\n",
       "  354.328369140625,\n",
       "  'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       "  10,\n",
       "  0),\n",
       " (54.0,\n",
       "  363.2684326171875,\n",
       "  296.9877014160156,\n",
       "  384.03546142578125,\n",
       "  'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       "  11,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  401.7261962890625,\n",
       "  297.0082702636719,\n",
       "  545.50048828125,\n",
       "  'that were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create pos-\\nitive and negative samples.\\n',\n",
       "  12,\n",
       "  0),\n",
       " (53.99996566772461,\n",
       "  556.18408203125,\n",
       "  297.0168151855469,\n",
       "  704.7994995117188,\n",
       "  'References\\n[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n',\n",
       "  13,\n",
       "  0),\n",
       " (314.9999694824219,\n",
       "  251.862060546875,\n",
       "  558.0069580078125,\n",
       "  319.5583801269531,\n",
       "  '[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n',\n",
       "  14,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  324.5259704589844,\n",
       "  558.0167846679688,\n",
       "  414.1402893066406,\n",
       "  '[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  419.10888671875,\n",
       "  558.0068359375,\n",
       "  475.8462219238281,\n",
       "  '[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n',\n",
       "  16,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  480.8138122558594,\n",
       "  558.0104370117188,\n",
       "  548.5111694335938,\n",
       "  '[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n',\n",
       "  17,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  553.478759765625,\n",
       "  558.0086059570312,\n",
       "  654.0521850585938,\n",
       "  '[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n',\n",
       "  18,\n",
       "  0),\n",
       " (314.9998779296875,\n",
       "  659.019775390625,\n",
       "  558.0068359375,\n",
       "  704.7991943359375,\n",
       "  '[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\n',\n",
       "  19,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '50\\n',\n",
       "  20,\n",
       "  0),\n",
       " (53.99995803833008,\n",
       "  54.7572021484375,\n",
       "  297.0168151855469,\n",
       "  704.799072265625,\n",
       "  'Association for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\n',\n",
       "  0,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  54.75665283203125,\n",
       "  558.00830078125,\n",
       "  508.2705993652344,\n",
       "  'cial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (302.3905029296875,\n",
       "  756.7999267578125,\n",
       "  309.6078796386719,\n",
       "  764.7999267578125,\n",
       "  '51\\n',\n",
       "  2,\n",
       "  0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42b69f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Arxiv=[]\n",
    "for i in doc_Arxiv.pages():\n",
    "    l_Arxiv = l_Arxiv + i.get_text('blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00911021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57.92100143432617,\n",
       "  86.11161804199219,\n",
       "  554.0845336914062,\n",
       "  120.71700286865234,\n",
       "  'Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       "  0,\n",
       "  0),\n",
       " (217.04800415039062,\n",
       "  131.30734252929688,\n",
       "  394.45404052734375,\n",
       "  150.7696533203125,\n",
       "  'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       "  1,\n",
       "  0),\n",
       " (192.7919921875,\n",
       "  153.19436645507812,\n",
       "  419.2115173339844,\n",
       "  191.86895751953125,\n",
       "  'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       "  2,\n",
       "  0),\n",
       " (153.25799560546875,\n",
       "  214.4859619140625,\n",
       "  197.7432861328125,\n",
       "  230.0396728515625,\n",
       "  'Abstract\\n',\n",
       "  3,\n",
       "  0),\n",
       " (73.92498779296875,\n",
       "  235.2144775390625,\n",
       "  277.0795593261719,\n",
       "  532.1500854492188,\n",
       "  'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmentation\\nin conjunction with semantic similarity is beneﬁcial\\nfor this task and could be useful for the other tasks\\nthat deal with short phrases. Our best performing\\nmodel (Accuracy: 0.917, Rank: 1.156) was devel-\\noped by ﬁne-tuning SentenceBERT [Reimers et al.,\\n2019] (with FinBERT at the backend) over an ex-\\ntended labelled set created using the hierarchy of\\nlabels present in FIBO.\\n',\n",
       "  4,\n",
       "  0),\n",
       " (53.9999885559082,\n",
       "  547.37060546875,\n",
       "  136.8136749267578,\n",
       "  562.92431640625,\n",
       "  '1\\nIntroduction\\n',\n",
       "  5,\n",
       "  0),\n",
       " (53.9999885559082,\n",
       "  564.9161376953125,\n",
       "  296.9981689453125,\n",
       "  675.6610717773438,\n",
       "  'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain. It contains the\\nclearly deﬁned relationships, and it is organized in a deﬁned\\nstructure mostly as a hierarchy. These properties make on-\\ntologies a great source for getting a deeper understanding of\\nthe relationship and properties of resources of the domain in\\nconsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       "  6,\n",
       "  0),\n",
       " (65.8219985961914,\n",
       "  682.5436401367188,\n",
       "  138.95611572265625,\n",
       "  704.51953125,\n",
       "  '∗Contact Author\\n†Equal Contribution\\n',\n",
       "  7,\n",
       "  0),\n",
       " (314.999755859375,\n",
       "  215.860595703125,\n",
       "  558.0033569335938,\n",
       "  582.5602416992188,\n",
       "  'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and searching of concepts.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] compe-\\ntitions are being held to promote the development of effective\\nsimilarity measures. In the third edition of the competition\\nFinSim-32 (being held in conjunction with the 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to rank hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym ranking. We experi-\\nmented with basic featurization methods like TF-IDF and ad-\\nvanced methods like pre-trained embedding models. Our top\\n3 systems use pre-trained FinBERT [Araci, 2019] embedding\\nmodel that was ﬁne-tuned on the data speciﬁc to ﬁnancial do-\\nmain . We also augmented the training data by utilizing the\\nknowledge from DBpedia, Investopedia, FIBO and text cor-\\npus of prospectus shared with us. We describe the works re-\\nlated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results obtained from some of them. We\\ndraw our conclusions in section 7 while giving a glimpse of\\nthings that we would like to try in the future.\\n',\n",
       "  8,\n",
       "  0),\n",
       " (314.999755859375,\n",
       "  596.3447875976562,\n",
       "  408.740478515625,\n",
       "  611.8984985351562,\n",
       "  '2\\nRelated Works\\n',\n",
       "  9,\n",
       "  0),\n",
       " (314.999755859375,\n",
       "  616.67333984375,\n",
       "  557.9976196289062,\n",
       "  661.5552368164062,\n",
       "  'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       "  10,\n",
       "  0),\n",
       " (315.0,\n",
       "  671.8306884765625,\n",
       "  556.79638671875,\n",
       "  704.51953125,\n",
       "  '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       "  11,\n",
       "  0),\n",
       " (10.940000534057617,\n",
       "  220.01995849609375,\n",
       "  37.619998931884766,\n",
       "  560.0,\n",
       "  'arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\\n',\n",
       "  12,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  53.860595703125,\n",
       "  297.0040588378906,\n",
       "  351.6932373046875,\n",
       "  'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  362.77374267578125,\n",
       "  170.3240509033203,\n",
       "  378.32745361328125,\n",
       "  '3\\nProblem Statement\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.999969482421875,\n",
       "  381.07427978515625,\n",
       "  297.0041198730469,\n",
       "  458.8321533203125,\n",
       "  'Given a set F consisting of n tuples of ﬁnancial terms\\nand\\ntheir\\nhypernyms/top-level\\nconcepts/labels\\ni.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents\\nthe hypernym corresponding to the ith term ti and hiϵ set of\\nlabels mentioned in Table 1. For every unseen ﬁnancial term,\\nour task is to generate a ranked list ˆyi consisting of these 17\\nhypernyms in order of decreasing semantic similarity.\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  468.74755859375,\n",
       "  296.9977722167969,\n",
       "  546.5562133789062,\n",
       "  'Evaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       "  3,\n",
       "  0),\n",
       " (53.99999237060547,\n",
       "  528.9028930664062,\n",
       "  196.3369903564453,\n",
       "  566.0833129882812,\n",
       "  'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       "  4,\n",
       "  0),\n",
       " (53.999996185302734,\n",
       "  541.7509155273438,\n",
       "  296.9986572265625,\n",
       "  592.5905151367188,\n",
       "  'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       "  5,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  603.6709594726562,\n",
       "  96.50074768066406,\n",
       "  619.2246704101562,\n",
       "  '4\\nData\\n',\n",
       "  6,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  621.7002563476562,\n",
       "  157.6255645751953,\n",
       "  635.8930053710938,\n",
       "  '4.1\\nData Description\\n',\n",
       "  7,\n",
       "  0),\n",
       " (54.00001525878906,\n",
       "  637.99951171875,\n",
       "  296.997802734375,\n",
       "  704.7994995117188,\n",
       "  'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels out of which 1040 term-label pairs are unique.\\nMore than 91% of the terms have 6 words or less and the\\nlongest term has 22 words. There were 10 duplicate entries,\\nand 3 terms were assigned 2 different labels. Along with this,\\n',\n",
       "  8,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  52.50591278076172,\n",
       "  514.7012939453125,\n",
       "  65.46725463867188,\n",
       "  'Label\\nCount\\n',\n",
       "  9,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  64.2215576171875,\n",
       "  503.07489013671875,\n",
       "  76.22649383544922,\n",
       "  'Equity Index\\n280\\n',\n",
       "  10,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  75.57952880859375,\n",
       "  503.07489013671875,\n",
       "  87.58446502685547,\n",
       "  'Regulatory Agency\\n205\\n',\n",
       "  11,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  86.93658447265625,\n",
       "  503.07489013671875,\n",
       "  98.94152069091797,\n",
       "  'Credit Index\\n125\\n',\n",
       "  12,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  98.2935791015625,\n",
       "  503.07489013671875,\n",
       "  110.29851531982422,\n",
       "  'Central Securities Depository\\n107\\n',\n",
       "  13,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  109.65155029296875,\n",
       "  498.0935974121094,\n",
       "  121.65648651123047,\n",
       "  'Debt pricing and yields\\n58\\n',\n",
       "  14,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  121.008544921875,\n",
       "  498.0935974121094,\n",
       "  133.0134735107422,\n",
       "  'Bonds\\n55\\n',\n",
       "  15,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  132.3665771484375,\n",
       "  498.0935974121094,\n",
       "  144.3715057373047,\n",
       "  'Swap\\n36\\n',\n",
       "  16,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  143.72357177734375,\n",
       "  498.0935974121094,\n",
       "  155.72850036621094,\n",
       "  'Stock Corporation\\n25\\n',\n",
       "  17,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  155.08056640625,\n",
       "  498.0935974121094,\n",
       "  167.0854949951172,\n",
       "  'Option\\n24\\n',\n",
       "  18,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  166.43853759765625,\n",
       "  498.0935974121094,\n",
       "  178.44346618652344,\n",
       "  'Funds\\n22\\n',\n",
       "  19,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  177.7955322265625,\n",
       "  498.0935974121094,\n",
       "  189.8004608154297,\n",
       "  'Future\\n19\\n',\n",
       "  20,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  189.153564453125,\n",
       "  498.0935974121094,\n",
       "  201.1584930419922,\n",
       "  'Credit Events\\n18\\n',\n",
       "  21,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  200.51055908203125,\n",
       "  498.0935974121094,\n",
       "  212.51548767089844,\n",
       "  'MMIs\\n17\\n',\n",
       "  22,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  211.8675537109375,\n",
       "  498.0935974121094,\n",
       "  223.8724822998047,\n",
       "  'Stocks\\n17\\n',\n",
       "  23,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  223.2255859375,\n",
       "  498.0935974121094,\n",
       "  235.2305145263672,\n",
       "  'Parametric schedules\\n15\\n',\n",
       "  24,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  234.58258056640625,\n",
       "  493.1123046875,\n",
       "  246.58750915527344,\n",
       "  'Forward\\n9\\n',\n",
       "  25,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  245.9405517578125,\n",
       "  493.1123046875,\n",
       "  257.94549560546875,\n",
       "  'Securities restrictions\\n8\\n',\n",
       "  26,\n",
       "  0),\n",
       " (358.29901123046875,\n",
       "  256.93890380859375,\n",
       "  508.0561828613281,\n",
       "  269.9002380371094,\n",
       "  'Total\\n1040\\n',\n",
       "  27,\n",
       "  0),\n",
       " (356.3590087890625,\n",
       "  278.841064453125,\n",
       "  516.6423950195312,\n",
       "  289.64556884765625,\n",
       "  'Table 1: Label distribution in the training set\\n',\n",
       "  28,\n",
       "  0),\n",
       " (315.0,\n",
       "  308.5025634765625,\n",
       "  557.997802734375,\n",
       "  353.38446044921875,\n",
       "  'a corpus of prospectuses in English that had 211 documents\\nwas provided. Some of the terms mentioned in the training\\ndata were present in the corpus. Table 1 shows the distribu-\\ntion of these labels in the training set.\\n',\n",
       "  29,\n",
       "  0),\n",
       " (315.0,\n",
       "  362.5732116699219,\n",
       "  430.8110046386719,\n",
       "  376.7659606933594,\n",
       "  '4.2\\nData Augmentation\\n',\n",
       "  30,\n",
       "  0),\n",
       " (315.0,\n",
       "  380.3235168457031,\n",
       "  558.004638671875,\n",
       "  436.1644287109375,\n",
       "  'Since the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       "  31,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  446.5638427734375,\n",
       "  558.0037231445312,\n",
       "  546.598388671875,\n",
       "  'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       "  32,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  551.2284545898438,\n",
       "  545.7438354492188,\n",
       "  563.2333984375,\n",
       "  '• expansion had equal or less length than the acronym.\\n',\n",
       "  33,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  567.8624267578125,\n",
       "  439.363037109375,\n",
       "  579.8673706054688,\n",
       "  '• expansion had parenthesis\\n',\n",
       "  34,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  584.4974365234375,\n",
       "  557.9976196289062,\n",
       "  607.4613647460938,\n",
       "  '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       "  35,\n",
       "  0),\n",
       " (326.4569396972656,\n",
       "  612.0914306640625,\n",
       "  541.7288208007812,\n",
       "  624.0963745117188,\n",
       "  '• the expansion had less than or equal to 5 characters.\\n',\n",
       "  36,\n",
       "  0),\n",
       " (314.99993896484375,\n",
       "  628.7254638671875,\n",
       "  557.9976806640625,\n",
       "  673.6073608398438,\n",
       "  'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       "  37,\n",
       "  0),\n",
       " (327.65301513671875,\n",
       "  692.6676635742188,\n",
       "  387.4657897949219,\n",
       "  704.51953125,\n",
       "  '3https://spacy.io/\\n',\n",
       "  38,\n",
       "  0),\n",
       " (53.99993896484375,\n",
       "  54.39891815185547,\n",
       "  297.00030517578125,\n",
       "  264.02227783203125,\n",
       "  'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the train\\nand test sets. We present such an example in Figure 1. In ad-\\ndition to the description, the label was also retained from the\\nresult payload to identify the right description for the input\\nterms. We tried token overlap-based similarity of input terms\\nwith both matching labels and descriptions. We decided to\\nuse the label to term match for description matching after go-\\ning through a randomly drawn sample. We cleaned both input\\nterms and labels from DBpedia results by converting them to\\nlower case, replacing punctuations by space, removing repet-\\nitive spaces, and singularizing the text. We calculated the\\ntoken overlap ratios for cleaned term and DBpedia labels us-\\ning these formulas: Ratio1 = length(s1 ∩ s2)/length(s1),\\nRatio2 = length(s2)/length(s1) where s1 and s2 represents\\nsets of tokenized cleaned terms and tokenized cleaned DBpe-\\ndia labels respectively. We empirically decided to use all the\\ninstances with Ratio1 = 1 and Ratio2 <= 1.25 for match-\\ning a DBpedia label (and hence description) to the input term.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (53.99993896484375,\n",
       "  273.5766906738281,\n",
       "  297.00048828125,\n",
       "  483.2000732421875,\n",
       "  'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1836 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm “callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (53.99993896484375,\n",
       "  492.6925964355469,\n",
       "  296.997802734375,\n",
       "  620.4730834960938,\n",
       "  '5\\nSystem Description\\nWe tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 832 and 208 terms respectively.\\n',\n",
       "  2,\n",
       "  0),\n",
       " (53.99993896484375,\n",
       "  626.98583984375,\n",
       "  296.9976806640625,\n",
       "  665.0940551757812,\n",
       "  '5.1\\nSystem - 1 (S1)\\nThis is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and used only the original set\\n',\n",
       "  3,\n",
       "  0),\n",
       " (54.0,\n",
       "  671.8306884765625,\n",
       "  273.7759094238281,\n",
       "  704.51953125,\n",
       "  '4https://lookup.dbpedia.org/api/search\\n5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       "  4,\n",
       "  0),\n",
       " (315.0,\n",
       "  54.757568359375,\n",
       "  557.9979248046875,\n",
       "  165.3923797607422,\n",
       "  'that was shared by organizers.\\nWe loaded FinBERT pre-\\ntrained model and ﬁne-tuned it by trying to classify the repre-\\nsentation of [CLS] token into one of the 17 labels mentioned\\npreviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       "  5,\n",
       "  0),\n",
       " (315.0,\n",
       "  172.1521453857422,\n",
       "  557.9978637695312,\n",
       "  276.1912841796875,\n",
       "  '5.2\\nSystem - 2 (S2)\\nThis system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset had the descriptions of the terms,\\nthe inputs were considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       "  6,\n",
       "  0),\n",
       " (314.9999084472656,\n",
       "  282.9520263671875,\n",
       "  558.0011596679688,\n",
       "  704.7990112304688,\n",
       "  '5.3\\nSystem -3 (S3)\\nWe explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of cosine similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       "  7,\n",
       "  0),\n",
       " (54.0,\n",
       "  53.999420166015625,\n",
       "  558.0036010742188,\n",
       "  167.385986328125,\n",
       "  '<image: DeviceRGB, width: 853, height: 188, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (214.45899963378906,\n",
       "  176.0380401611328,\n",
       "  397.5439758300781,\n",
       "  186.84255981445312,\n",
       "  'Figure 1: Sample output from DBpedia search API\\n',\n",
       "  1,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  197.7718963623047,\n",
       "  501.24560546875,\n",
       "  210.73324584960938,\n",
       "  'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       "  2,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  209.488525390625,\n",
       "  549.5443725585938,\n",
       "  232.4524383544922,\n",
       "  'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       "  3,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  231.8045654296875,\n",
       "  494.789794921875,\n",
       "  254.7684783935547,\n",
       "  'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       "  4,\n",
       "  0),\n",
       " (62.45500183105469,\n",
       "  254.12054443359375,\n",
       "  422.7225036621094,\n",
       "  288.04345703125,\n",
       "  'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       "  5,\n",
       "  0),\n",
       " (434.6759948730469,\n",
       "  265.3785400390625,\n",
       "  507.51202392578125,\n",
       "  277.38348388671875,\n",
       "  'Bonds\\nDBpedia\\n',\n",
       "  6,\n",
       "  0),\n",
       " (186.55599975585938,\n",
       "  297.5820617675781,\n",
       "  425.44775390625,\n",
       "  308.3865661621094,\n",
       "  'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       "  7,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  325.2939147949219,\n",
       "  242.3582305908203,\n",
       "  338.2552490234375,\n",
       "  'Data Source\\nCount\\n',\n",
       "  8,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  337.0095520019531,\n",
       "  235.71316528320312,\n",
       "  349.0144958496094,\n",
       "  'Original modelling data\\n1040\\n',\n",
       "  9,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  348.3675537109375,\n",
       "  230.73187255859375,\n",
       "  360.37249755859375,\n",
       "  'DBpedia\\n257\\n',\n",
       "  10,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  359.72454833984375,\n",
       "  230.73187255859375,\n",
       "  371.7294921875,\n",
       "  'FIBO\\n236\\n',\n",
       "  11,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  371.0825500488281,\n",
       "  225.75057983398438,\n",
       "  383.0874938964844,\n",
       "  'Investopedia\\n85\\n',\n",
       "  12,\n",
       "  0),\n",
       " (108.64099884033203,\n",
       "  382.4395446777344,\n",
       "  230.73187255859375,\n",
       "  394.4444885253906,\n",
       "  'Acronym expansion\\n218\\n',\n",
       "  13,\n",
       "  0),\n",
       " (104.73400115966797,\n",
       "  403.9830627441406,\n",
       "  246.2686309814453,\n",
       "  414.7875671386719,\n",
       "  'Table 3: Details of various data sources\\n',\n",
       "  14,\n",
       "  0),\n",
       " (54.0,\n",
       "  432.51556396484375,\n",
       "  296.9976501464844,\n",
       "  455.4794921875,\n",
       "  'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (54.0,\n",
       "  466.23699951171875,\n",
       "  220.71527099609375,\n",
       "  481.79071044921875,\n",
       "  '6\\nExperimentation and Results\\n',\n",
       "  16,\n",
       "  0),\n",
       " (54.0,\n",
       "  484.2945556640625,\n",
       "  296.9996643066406,\n",
       "  704.7994995117188,\n",
       "  'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1470 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only major change between these runs was\\n',\n",
       "  17,\n",
       "  0),\n",
       " (315.0,\n",
       "  327.5455322265625,\n",
       "  557.9999389648438,\n",
       "  571.5953979492188,\n",
       "  'the data, the improvement can be attributed to the expanded\\ndata.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models have been\\nalready mentioned in the system description. After rigorous\\nexperimentation, these hyperparameters were selected empir-\\nically based on validation set performance. The results are\\npresented in Table 4. Since the number of submissions was\\nrestricted to 3 for each team, we do not have the performance\\nnumbers of the BERT models in the test set. Analysing the\\nresults we see that SentenceBERT trained with FinBERT at\\nthe backed as mentioned in section-5.3 performed the best.\\n',\n",
       "  18,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  585.0458984375,\n",
       "  486.92779541015625,\n",
       "  600.599609375,\n",
       "  '7\\nConclusion and Future Works\\n',\n",
       "  19,\n",
       "  0),\n",
       " (315.0000305175781,\n",
       "  605.1234130859375,\n",
       "  557.9978637695312,\n",
       "  704.7993774414062,\n",
       "  'In this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\n',\n",
       "  20,\n",
       "  0),\n",
       " (164.2689971923828,\n",
       "  53.997528076171875,\n",
       "  447.72991943359375,\n",
       "  214.14398193359375,\n",
       "  '<image: DeviceRGB, width: 2689, height: 1878, bpc: 8>',\n",
       "  0,\n",
       "  1),\n",
       " (170.41200256347656,\n",
       "  222.79505920410156,\n",
       "  441.591796875,\n",
       "  233.59957885742188,\n",
       "  'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       "  1,\n",
       "  0),\n",
       " (161.54200744628906,\n",
       "  250.5068817138672,\n",
       "  262.9809265136719,\n",
       "  263.4682312011719,\n",
       "  'Validation set\\nTest set\\n',\n",
       "  2,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  261.86492919921875,\n",
       "  285.2674560546875,\n",
       "  274.8262634277344,\n",
       "  'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       "  3,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  273.58056640625,\n",
       "  289.15283203125,\n",
       "  285.58551025390625,\n",
       "  'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       "  4,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  284.9385681152344,\n",
       "  289.15283203125,\n",
       "  296.9435119628906,\n",
       "  'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       "  5,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  296.2955627441406,\n",
       "  270.0545349121094,\n",
       "  308.3005065917969,\n",
       "  'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       "  6,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  307.6525573730469,\n",
       "  270.0545349121094,\n",
       "  319.6575012207031,\n",
       "  'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       "  7,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  319.01055908203125,\n",
       "  289.15283203125,\n",
       "  331.0155029296875,\n",
       "  'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       "  8,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  330.3675537109375,\n",
       "  289.15283203125,\n",
       "  342.37249755859375,\n",
       "  'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       "  9,\n",
       "  0),\n",
       " (61.84700012207031,\n",
       "  341.3669128417969,\n",
       "  289.15283203125,\n",
       "  354.3282470703125,\n",
       "  'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       "  10,\n",
       "  0),\n",
       " (54.0,\n",
       "  363.26806640625,\n",
       "  296.9982604980469,\n",
       "  384.03558349609375,\n",
       "  'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       "  11,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  404.3045654296875,\n",
       "  297.0047607421875,\n",
       "  571.2853393554688,\n",
       "  'Apart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\nthat were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create\\nmore positive and negative samples.\\n',\n",
       "  12,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  587.1248779296875,\n",
       "  109.54383850097656,\n",
       "  602.6785888671875,\n",
       "  'References\\n',\n",
       "  13,\n",
       "  0),\n",
       " (53.99998474121094,\n",
       "  608.097412109375,\n",
       "  297.00445556640625,\n",
       "  675.7933959960938,\n",
       "  '[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n',\n",
       "  14,\n",
       "  0),\n",
       " (53.9999885559082,\n",
       "  680.9384155273438,\n",
       "  296.9979553222656,\n",
       "  704.7993774414062,\n",
       "  '[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n',\n",
       "  15,\n",
       "  0),\n",
       " (314.9999694824219,\n",
       "  251.8624267578125,\n",
       "  558.0032958984375,\n",
       "  704.7991333007812,\n",
       "  '[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n',\n",
       "  16,\n",
       "  0),\n",
       " (53.999996185302734,\n",
       "  54.757568359375,\n",
       "  297.0039978027344,\n",
       "  704.7991333007812,\n",
       "  '2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n',\n",
       "  0,\n",
       "  0),\n",
       " (314.9999694824219,\n",
       "  53.86016845703125,\n",
       "  558.004150390625,\n",
       "  530.1876831054688,\n",
       "  '[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\ncial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n',\n",
       "  1,\n",
       "  0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fb3648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       " 'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       " 'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       " 'Abstract\\n',\n",
       " 'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmentation\\nin conjunction with semantic similarity is beneﬁcial\\nfor this task and could be useful for the other tasks\\nthat deal with short phrases. Our best performing\\nmodel (Accuracy: 0.917, Rank: 1.156) was devel-\\noped by ﬁne-tuning SentenceBERT [Reimers et al.,\\n2019] (with FinBERT at the backend) over an ex-\\ntended labelled set created using the hierarchy of\\nlabels present in FIBO.\\n',\n",
       " '1\\nIntroduction\\n',\n",
       " 'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain. It contains the\\nclearly deﬁned relationships, and it is organized in a deﬁned\\nstructure mostly as a hierarchy. These properties make on-\\ntologies a great source for getting a deeper understanding of\\nthe relationship and properties of resources of the domain in\\nconsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       " '∗Contact Author\\n†Equal Contribution\\n',\n",
       " 'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and searching of concepts.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] compe-\\ntitions are being held to promote the development of effective\\nsimilarity measures. In the third edition of the competition\\nFinSim-32 (being held in conjunction with the 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to rank hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym ranking. We experi-\\nmented with basic featurization methods like TF-IDF and ad-\\nvanced methods like pre-trained embedding models. Our top\\n3 systems use pre-trained FinBERT [Araci, 2019] embedding\\nmodel that was ﬁne-tuned on the data speciﬁc to ﬁnancial do-\\nmain . We also augmented the training data by utilizing the\\nknowledge from DBpedia, Investopedia, FIBO and text cor-\\npus of prospectus shared with us. We describe the works re-\\nlated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results obtained from some of them. We\\ndraw our conclusions in section 7 while giving a glimpse of\\nthings that we would like to try in the future.\\n',\n",
       " '2\\nRelated Works\\n',\n",
       " 'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       " '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       " 'arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\\n',\n",
       " 'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       " '3\\nProblem Statement\\n',\n",
       " 'Given a set F consisting of n tuples of ﬁnancial terms\\nand\\ntheir\\nhypernyms/top-level\\nconcepts/labels\\ni.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents\\nthe hypernym corresponding to the ith term ti and hiϵ set of\\nlabels mentioned in Table 1. For every unseen ﬁnancial term,\\nour task is to generate a ranked list ˆyi consisting of these 17\\nhypernyms in order of decreasing semantic similarity.\\n',\n",
       " 'Evaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       " 'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       " 'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       " '4\\nData\\n',\n",
       " '4.1\\nData Description\\n',\n",
       " 'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels out of which 1040 term-label pairs are unique.\\nMore than 91% of the terms have 6 words or less and the\\nlongest term has 22 words. There were 10 duplicate entries,\\nand 3 terms were assigned 2 different labels. Along with this,\\n',\n",
       " 'Label\\nCount\\n',\n",
       " 'Equity Index\\n280\\n',\n",
       " 'Regulatory Agency\\n205\\n',\n",
       " 'Credit Index\\n125\\n',\n",
       " 'Central Securities Depository\\n107\\n',\n",
       " 'Debt pricing and yields\\n58\\n',\n",
       " 'Bonds\\n55\\n',\n",
       " 'Swap\\n36\\n',\n",
       " 'Stock Corporation\\n25\\n',\n",
       " 'Option\\n24\\n',\n",
       " 'Funds\\n22\\n',\n",
       " 'Future\\n19\\n',\n",
       " 'Credit Events\\n18\\n',\n",
       " 'MMIs\\n17\\n',\n",
       " 'Stocks\\n17\\n',\n",
       " 'Parametric schedules\\n15\\n',\n",
       " 'Forward\\n9\\n',\n",
       " 'Securities restrictions\\n8\\n',\n",
       " 'Total\\n1040\\n',\n",
       " 'Table 1: Label distribution in the training set\\n',\n",
       " 'a corpus of prospectuses in English that had 211 documents\\nwas provided. Some of the terms mentioned in the training\\ndata were present in the corpus. Table 1 shows the distribu-\\ntion of these labels in the training set.\\n',\n",
       " '4.2\\nData Augmentation\\n',\n",
       " 'Since the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       " 'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       " '• expansion had equal or less length than the acronym.\\n',\n",
       " '• expansion had parenthesis\\n',\n",
       " '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       " '• the expansion had less than or equal to 5 characters.\\n',\n",
       " 'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       " '3https://spacy.io/\\n',\n",
       " 'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the train\\nand test sets. We present such an example in Figure 1. In ad-\\ndition to the description, the label was also retained from the\\nresult payload to identify the right description for the input\\nterms. We tried token overlap-based similarity of input terms\\nwith both matching labels and descriptions. We decided to\\nuse the label to term match for description matching after go-\\ning through a randomly drawn sample. We cleaned both input\\nterms and labels from DBpedia results by converting them to\\nlower case, replacing punctuations by space, removing repet-\\nitive spaces, and singularizing the text. We calculated the\\ntoken overlap ratios for cleaned term and DBpedia labels us-\\ning these formulas: Ratio1 = length(s1 ∩ s2)/length(s1),\\nRatio2 = length(s2)/length(s1) where s1 and s2 represents\\nsets of tokenized cleaned terms and tokenized cleaned DBpe-\\ndia labels respectively. We empirically decided to use all the\\ninstances with Ratio1 = 1 and Ratio2 <= 1.25 for match-\\ning a DBpedia label (and hence description) to the input term.\\n',\n",
       " 'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1836 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm “callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       " '5\\nSystem Description\\nWe tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 832 and 208 terms respectively.\\n',\n",
       " '5.1\\nSystem - 1 (S1)\\nThis is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and used only the original set\\n',\n",
       " '4https://lookup.dbpedia.org/api/search\\n5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       " 'that was shared by organizers.\\nWe loaded FinBERT pre-\\ntrained model and ﬁne-tuned it by trying to classify the repre-\\nsentation of [CLS] token into one of the 17 labels mentioned\\npreviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       " '5.2\\nSystem - 2 (S2)\\nThis system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset had the descriptions of the terms,\\nthe inputs were considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       " '5.3\\nSystem -3 (S3)\\nWe explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of cosine similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       " 'Figure 1: Sample output from DBpedia search API\\n',\n",
       " 'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       " 'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       " 'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       " 'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       " 'Bonds\\nDBpedia\\n',\n",
       " 'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       " 'Data Source\\nCount\\n',\n",
       " 'Original modelling data\\n1040\\n',\n",
       " 'DBpedia\\n257\\n',\n",
       " 'FIBO\\n236\\n',\n",
       " 'Investopedia\\n85\\n',\n",
       " 'Acronym expansion\\n218\\n',\n",
       " 'Table 3: Details of various data sources\\n',\n",
       " 'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       " '6\\nExperimentation and Results\\n',\n",
       " 'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1470 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only major change between these runs was\\n',\n",
       " 'the data, the improvement can be attributed to the expanded\\ndata.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models have been\\nalready mentioned in the system description. After rigorous\\nexperimentation, these hyperparameters were selected empir-\\nically based on validation set performance. The results are\\npresented in Table 4. Since the number of submissions was\\nrestricted to 3 for each team, we do not have the performance\\nnumbers of the BERT models in the test set. Analysing the\\nresults we see that SentenceBERT trained with FinBERT at\\nthe backed as mentioned in section-5.3 performed the best.\\n',\n",
       " '7\\nConclusion and Future Works\\n',\n",
       " 'In this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\n',\n",
       " 'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       " 'Validation set\\nTest set\\n',\n",
       " 'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       " 'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       " 'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       " 'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       " 'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       " 'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       " 'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       " 'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       " 'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       " 'Apart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\nthat were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create\\nmore positive and negative samples.\\n',\n",
       " 'References\\n',\n",
       " '[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n',\n",
       " '[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n',\n",
       " '[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n',\n",
       " '2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n',\n",
       " '[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\ncial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_Arxiv = []\n",
    "for el in l_Arxiv:\n",
    "    if el[6]==0:\n",
    "        lst_Arxiv.append(el[4])\n",
    "lst_Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad9617fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       " 'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       " 'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       " 'Abstract\\n',\n",
       " 'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmenta-\\ntion in conjunction with semantic similarity is ben-\\neﬁcial for this task and could be beneﬁcial for\\nthe other tasks that deal with short phrases. Our\\nbest performing model (Accuracy: 0.917, Rank:\\n1.156) was developed by ﬁne-tuning Sentence-\\nBERT [Reimers et al., 2019] (with FinBERT at the\\nbackend) over an extended labelled set created us-\\ning the hierarchy of labels present in FIBO.\\n',\n",
       " '1\\nIntroduction\\n',\n",
       " 'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain, contains the\\nclearly deﬁned relationship, and organizes in a deﬁned struc-\\nture mostly as a hierarchy. These properties make ontologies\\na great source for getting a deeper understanding of the rela-\\ntionship and properties of resources from the domain in con-\\nsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       " '∗Contact Author\\n†Equal Contribution\\n',\n",
       " 'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and concept search.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\\npetitions are being held to promote the development of effec-\\ntive similarity measures. In the third edition of the competi-\\ntion FinSim-32 (being held in conjunction with 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to assign hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym assignment. We ex-\\nperimented with basic featurization methods like TF-IDF and\\nadvanced methods like pre-trained embedding models. Our\\ntop 3 systems use pre-trained FinBERT [Araci, 2019] embed-\\nding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\\ncial domain . We also augmented the training data by utilizing\\nthe knowledge from DBpedia, Investopedia, FIBO and text\\ncorpus of prospectus shared with us. We describe the works\\nrelated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results from some of them. We draw our\\nconclusions in section 7 while giving a glimpse of things that\\nwe would like to try in the future.\\n',\n",
       " '2\\nRelated Works\\n',\n",
       " 'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       " '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       " '46\\n',\n",
       " 'Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \\n(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \\n',\n",
       " 'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       " '3\\nProblem Statement\\n',\n",
       " 'Given a set F consisting of n tuples of ﬁnancial terms\\nand their hypernyms/top-level concepts/labels i.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\\nnym corresponding to the ith term ti and hiϵ set of labels men-\\ntioned in Table 1. For every unseen ﬁnancial term, our task is\\nto generate a ranked list ˆyi consisting of these 17 hypernyms\\nin order of decreasing semantic similarity.\\nEvaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       " 'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       " 'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       " '4\\nData\\n',\n",
       " '4.1\\nData Description\\n',\n",
       " 'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels. More than 91% of the terms have 6 words or\\nless and the longest term has 22 words. There were 10 du-\\nplicate entries, and 3 terms were assigned 2 different labels.\\nAlong with this, a corpus of prospectuses in English was pro-\\nvided that had 211 documents. Some of the terms mentioned\\n',\n",
       " 'Label\\nCount\\n',\n",
       " 'Equity Index\\n280\\n',\n",
       " 'Regulatory Agency\\n205\\n',\n",
       " 'Credit Index\\n125\\n',\n",
       " 'Central Securities Depository\\n107\\n',\n",
       " 'Debt pricing and yields\\n58\\n',\n",
       " 'Bonds\\n55\\n',\n",
       " 'Swap\\n36\\n',\n",
       " 'Stock Corporation\\n25\\n',\n",
       " 'Option\\n24\\n',\n",
       " 'Funds\\n22\\n',\n",
       " 'Future\\n19\\n',\n",
       " 'Credit Events\\n18\\n',\n",
       " 'MMIs\\n17\\n',\n",
       " 'Stocks\\n17\\n',\n",
       " 'Parametric schedules\\n15\\n',\n",
       " 'Forward\\n9\\n',\n",
       " 'Securities restrictions\\n8\\n',\n",
       " 'Total\\n1040\\n',\n",
       " 'Table 1: Label distribution in the training set\\n',\n",
       " 'in the training data were present in the corpus. Table 1 shows\\nthe distribution of these labels in the training set.\\n',\n",
       " '4.2\\nData Augmentation\\nSince the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       " 'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       " '• expansion had equal or less length than the acronym.\\n',\n",
       " '• expansion had parenthesis\\n',\n",
       " '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       " '• the expansion had less than or equal to 5 characters.\\n',\n",
       " 'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       " 'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the\\n',\n",
       " '3https://spacy.io/\\n4https://lookup.dbpedia.org/api/search\\n',\n",
       " '47\\n',\n",
       " 'train and test sets.\\nWe present such an example in Fig-\\nure 1. In addition to the description, the label was also re-\\ntained from the result payload to identify the right descrip-\\ntion for the input terms. We tried token overlap-based simi-\\nlarity of input terms with both matching labels and descrip-\\ntions. We decided to use the label to term match for descrip-\\ntion matching after going through a randomly drawn sam-\\nple. We cleaned both input terms and labels from DBpedia\\nresults by converting them to lower case, replacing punctua-\\ntions by space, removing repetitive spaces, and singularizing\\nthe text. We calculated the token overlap ratios for cleaned\\nterm and DBpedia labels using the formulas mentioned be-\\nlow: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\\nlength(s2/length(s1 where s1 and s2 represents sets of to-\\nkenized cleaned terms and tokenized and cleaned DBpedia\\nlabels respectively. We empirically decided to use all the in-\\nstances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\\na DBpedia label (and hence description) to the input term.\\n',\n",
       " 'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1801 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm ”callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       " '5\\nSystem Description\\n',\n",
       " 'We tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 841 and 209 terms respectively.\\n',\n",
       " '5.1\\nSystem - 1 (S1)\\n',\n",
       " 'This is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and stuck to the original set that\\nwas shared by organizers. We loaded FinBERT pre-trained\\n',\n",
       " '5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       " 'model and ﬁne-tuned it by trying to classify the representa-\\ntion of [CLS] token into one of the 17 labels mentioned pre-\\nviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       " '5.2\\nSystem - 2 (S2)\\n',\n",
       " 'This system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset has the descriptions of the terms,\\nthe input is considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       " '5.3\\nSystem -3 (S3)\\n',\n",
       " 'We explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       " '48\\n',\n",
       " 'Figure 1: Sample output from DBpedia search API\\n',\n",
       " 'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       " 'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       " 'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       " 'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       " 'Bonds\\nDBpedia\\n',\n",
       " 'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       " 'Data Source\\nCount\\n',\n",
       " 'Original modelling data\\n1040\\n',\n",
       " 'DBpedia\\n257\\n',\n",
       " 'FIBO\\n236\\n',\n",
       " 'Investopedia\\n85\\n',\n",
       " 'Acronym expansion\\n218\\n',\n",
       " 'Table 3: Details of various data sources\\n',\n",
       " 'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       " '6\\nExperimentation and Results\\n',\n",
       " 'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1469 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only change between these runs was the data,\\n',\n",
       " 'the improvement can be attributed to the expanded data.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models are already\\ngiven in the system description. After rigorous experimenta-\\ntion, these hyperparameters were selected empirically based\\non validation set performance. The results are presented in\\nTable 4. Since the number of submissions was restricted to\\n3 for each team, we do not have the performance numbers of\\nthe BERT models in the test set. Analysing the results we see\\nthat SentenceBERT trained with FinBERT at the backed as\\nmentioned in section-5.3 performed the best.\\n',\n",
       " '7\\nConclusion and Future Works\\nIn this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\nApart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\n',\n",
       " '49\\n',\n",
       " 'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       " 'Validation set\\nTest set\\n',\n",
       " 'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       " 'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       " 'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       " 'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       " 'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       " 'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       " 'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       " 'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       " 'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       " 'that were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create pos-\\nitive and negative samples.\\n',\n",
       " 'References\\n[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n',\n",
       " '[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n',\n",
       " '[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n',\n",
       " '[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n',\n",
       " '[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n',\n",
       " '[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n',\n",
       " '[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\n',\n",
       " '50\\n',\n",
       " 'Association for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\n',\n",
       " 'cial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n',\n",
       " '51\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_Anth = []\n",
    "for el in l_Anth:\n",
    "    if el[6]==0:\n",
    "        lst_Anth.append(el[4])\n",
    "lst_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bbf03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\n",
      "of Financial Terms\n",
      "\n",
      "Ankush Chopra∗† , Sohom Ghosh†\n",
      "\n",
      "Fidelity Investments, AI CoE, Bengaluru, India\n",
      "{ankush01729, sohom1ghosh}@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "1\n",
      "Introduction\n",
      "\n",
      "∗Contact Author\n",
      "†Equal Contribution\n",
      "\n",
      "2\n",
      "Related Works\n",
      "\n",
      "Hypernym-hyponym extraction and learning text similarity\n",
      "using semantic representations have been very challenging\n",
      "areas of research for the NLP community. SemEval-2018\n",
      "Task 9 [Camacho-Collados et al., 2018] was such an instance.\n",
      "\n",
      "1https://spec.edmcouncil.org/ﬁbo/\n",
      "2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\n",
      "task-ﬁnsim (accessed on 8th July 2021)\n",
      "\n",
      "Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\n",
      "formed the best in this shared task. They combined a super-\n",
      "vised word embedding based approach with an unsupervised\n",
      "pattern discovery based approach. The FinSim shared tasks\n",
      "[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\n",
      "ing these challenges speciﬁc to the Financial Domain. Team\n",
      "IIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\n",
      "nation of context-free static embedding Word2Vec [Mikolov\n",
      "et al., 2013] and contextualized dynamic embedding BERT\n",
      "[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\n",
      "the team FINSIM20 explored the use of cosine similarity be-\n",
      "tween terms and labels encoded using Universal Sentence En-\n",
      "coder [Cer et al., 2018]. They also tried to extract hypernyms\n",
      "automatically using graph based approaches. Team PolyU-\n",
      "CBS [Chersoni and Huang, 2021] won FinSim-2 shared\n",
      "task using Logistic Regression trained over word embedding\n",
      "and probabilities derived from BERT [Devlin et al., 2019]\n",
      "model. They also experimented with GPT-2 [Radford et al.,\n",
      "2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\n",
      "ing Nguyen et al. performed better than the baseline by us-\n",
      "ing Sentence BERT [Reimers et al., 2019] to calculate co-\n",
      "sine similarity between terms and hypernyms. [Saini, 2020;\n",
      "Pei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\n",
      "cussed various techniques to enrich the data which was avail-\n",
      "able for training. In this edition of FinSim, the number of\n",
      "training samples and labels (ﬁnancial concepts) were more\n",
      "than the previous two editions.\n",
      "\n",
      "3\n",
      "Problem Statement\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1 I(yi = ˆyi[1])\n",
      "MeanRank = 1\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1( ˆyi.index(yi))\n",
      "where ˆyi is the ranked list (with index starting from 1) of pre-\n",
      "dicted labels corresponding to the expected label yi. I is an\n",
      "identity matrix.\n",
      "\n",
      "4\n",
      "Data\n",
      "\n",
      "4.1\n",
      "Data Description\n",
      "\n",
      "Label\n",
      "Count\n",
      "\n",
      "Equity Index\n",
      "280\n",
      "\n",
      "Regulatory Agency\n",
      "205\n",
      "\n",
      "Credit Index\n",
      "125\n",
      "\n",
      "Central Securities Depository\n",
      "107\n",
      "\n",
      "Debt pricing and yields\n",
      "58\n",
      "\n",
      "Bonds\n",
      "55\n",
      "\n",
      "Swap\n",
      "36\n",
      "\n",
      "Stock Corporation\n",
      "25\n",
      "\n",
      "Option\n",
      "24\n",
      "\n",
      "Funds\n",
      "22\n",
      "\n",
      "Future\n",
      "19\n",
      "\n",
      "Credit Events\n",
      "18\n",
      "\n",
      "MMIs\n",
      "17\n",
      "\n",
      "Stocks\n",
      "17\n",
      "\n",
      "Parametric schedules\n",
      "15\n",
      "\n",
      "Forward\n",
      "9\n",
      "\n",
      "Securities restrictions\n",
      "8\n",
      "\n",
      "Total\n",
      "1040\n",
      "\n",
      "Table 1: Label distribution in the training set\n",
      "\n",
      "Acronym expansion: As mentioned by Keswani et al.\n",
      "[Keswani et al., 2020], the presence of acronyms created a\n",
      "major issue in maintaining consistency. We used the abbre-\n",
      "viation extractor available in spaCy3[Honnibal et al., 2020]\n",
      "package on the corpus of the prospectus to extract all the\n",
      "acronyms and their expansions. Upon manual inspection of\n",
      "a sample output, we identiﬁed that not all the extracted items\n",
      "were valid acronyms and their expansions. We cleaned the\n",
      "extracted list by dropping the records where:\n",
      "\n",
      "• expansion had equal or less length than the acronym.\n",
      "\n",
      "• expansion had parenthesis\n",
      "\n",
      "• extracted acronym was a valid English word such as\n",
      "”fund” or ”Germany”.\n",
      "\n",
      "• the expansion had less than or equal to 5 characters.\n",
      "\n",
      "We managed to extract 635 acronyms from the prospectus\n",
      "corpus after applying the above exclusions.\n",
      "We used this\n",
      "data to expand the matching terms in the given train set and\n",
      "test sets.\n",
      "\n",
      "Figure 1: Sample output from DBpedia search API\n",
      "\n",
      "Expanded Term/Term Deﬁnition\n",
      "Label\n",
      "Source\n",
      "\n",
      "Callable bond\n",
      "Bonds\n",
      "original and\n",
      "acronym expansion\n",
      "\n",
      "bond that includes a stipulation allowing the issuer\n",
      "the right to repurchase and retire the bond at the call price after the call protection period\n",
      "Bonds\n",
      "FIBO\n",
      "\n",
      "A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\n",
      "the issuer of the bond to retain the privilege of redeeming the bond at some point before\n",
      "the bond reaches its date of maturity.\n",
      "\n",
      "Bonds\n",
      "DBpedia\n",
      "\n",
      "Table 2: Result of Data Augmentation of the term ”Callable bond”\n",
      "\n",
      "Data Source\n",
      "Count\n",
      "\n",
      "Original modelling data\n",
      "1040\n",
      "\n",
      "DBpedia\n",
      "257\n",
      "\n",
      "FIBO\n",
      "236\n",
      "\n",
      "Investopedia\n",
      "85\n",
      "\n",
      "Acronym expansion\n",
      "218\n",
      "\n",
      "Table 3: Details of various data sources\n",
      "\n",
      "derive the ﬁnal prediction by averaging the output probabili-\n",
      "ties for all the 17 classes for all the occurrences of the term.\n",
      "\n",
      "6\n",
      "Experimentation and Results\n",
      "\n",
      "Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\n",
      "\n",
      "Validation set\n",
      "Test set\n",
      "\n",
      "Model\n",
      "Data\n",
      "Rank\n",
      "Acc.\n",
      "Rank\n",
      "Acc.\n",
      "\n",
      "Base-1\n",
      "Org.\n",
      "2.158\n",
      "0.498\n",
      "1.941\n",
      "0.564\n",
      "\n",
      "Base-2\n",
      "Org.\n",
      "1.201\n",
      "0.876\n",
      "1.75\n",
      "0.669\n",
      "\n",
      "BERT\n",
      "Org.\n",
      "1.177\n",
      "0.899\n",
      "-\n",
      "-\n",
      "\n",
      "BERT\n",
      "Ext.\n",
      "1.153\n",
      "0.928\n",
      "-\n",
      "-\n",
      "\n",
      "FinBERT(S1)\n",
      "Org.\n",
      "1.117\n",
      "0.928\n",
      "1.257\n",
      "0.886\n",
      "\n",
      "FinBERT(S2)\n",
      "Ext.\n",
      "1.110\n",
      "0.942\n",
      "1.220\n",
      "0.895\n",
      "\n",
      "SBERT(S3)\n",
      "Ext.\n",
      "1.086\n",
      "0.947\n",
      "1.156\n",
      "0.917\n",
      "\n",
      "Table 4: Results on validation and test set. Org. represents original\n",
      "and Ext. represents extended. Base refers to baseline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in lst_Arxiv:\n",
    "    if elem in lst_Anth:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "037e197b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1\\nIntroduction\\n',\n",
       " '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       " '2\\nRelated Works\\n',\n",
       " '3\\nProblem Statement\\n',\n",
       " '4\\nData\\n',\n",
       " '4.1\\nData Description\\n',\n",
       " '6\\nExperimentation and Results\\n',\n",
       " 'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       " 'Abstract\\n',\n",
       " 'Acronym expansion\\n218\\n',\n",
       " 'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       " 'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       " 'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       " 'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       " 'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       " 'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       " 'Bonds\\n55\\n',\n",
       " 'Bonds\\nDBpedia\\n',\n",
       " 'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       " 'Central Securities Depository\\n107\\n',\n",
       " 'Credit Events\\n18\\n',\n",
       " 'Credit Index\\n125\\n',\n",
       " 'DBpedia\\n257\\n',\n",
       " 'Data Source\\nCount\\n',\n",
       " 'Debt pricing and yields\\n58\\n',\n",
       " 'Equity Index\\n280\\n',\n",
       " 'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       " 'FIBO\\n236\\n',\n",
       " 'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       " 'Figure 1: Sample output from DBpedia search API\\n',\n",
       " 'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       " 'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       " 'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       " 'Forward\\n9\\n',\n",
       " 'Funds\\n22\\n',\n",
       " 'Future\\n19\\n',\n",
       " 'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       " 'Investopedia\\n85\\n',\n",
       " 'Label\\nCount\\n',\n",
       " 'MMIs\\n17\\n',\n",
       " 'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       " 'Option\\n24\\n',\n",
       " 'Original modelling data\\n1040\\n',\n",
       " 'Parametric schedules\\n15\\n',\n",
       " 'Regulatory Agency\\n205\\n',\n",
       " 'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       " 'Securities restrictions\\n8\\n',\n",
       " 'Stock Corporation\\n25\\n',\n",
       " 'Stocks\\n17\\n',\n",
       " 'Swap\\n36\\n',\n",
       " 'Table 1: Label distribution in the training set\\n',\n",
       " 'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       " 'Table 3: Details of various data sources\\n',\n",
       " 'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       " 'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       " 'Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       " 'Total\\n1040\\n',\n",
       " 'Validation set\\nTest set\\n',\n",
       " 'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       " 'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       " 'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       " 'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       " 'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       " '• expansion had equal or less length than the acronym.\\n',\n",
       " '• expansion had parenthesis\\n',\n",
       " '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       " '• the expansion had less than or equal to 5 characters.\\n',\n",
       " '∗Contact Author\\n†Equal Contribution\\n'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems_in_both_lists = set(lst_Anth) & set(lst_Arxiv)\n",
    "elems_in_both_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498db610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernym and synonym matching are one of the\n",
      "mainstream Natural Language Processing (NLP)\n",
      "tasks. In this paper, we present systems that at-\n",
      "tempt to solve this problem.\n",
      "We designed these\n",
      "systems to participate in the FinSim-3, a shared\n",
      "task of FinNLP workshop at IJCAI-2021.\n",
      "The\n",
      "shared task is focused on solving this problem for\n",
      "the ﬁnancial domain. We experimented with var-\n",
      "ious transformer based pre-trained embeddings by\n",
      "ﬁne-tuning these for either classiﬁcation or phrase\n",
      "similarity tasks. We also augmented the provided\n",
      "dataset with abbreviations derived from prospectus\n",
      "provided by the organizers and deﬁnitions of the\n",
      "ﬁnancial terms from DBpedia [Auer et al., 2007],\n",
      "Investopedia, and the Financial Industry Business\n",
      "Ontology (FIBO). Our best performing system uses\n",
      "both FinBERT [Araci, 2019] and data augmenta-\n",
      "tion from the afore-mentioned sources.\n",
      "We ob-\n",
      "served that term expansion using data augmenta-\n",
      "tion in conjunction with semantic similarity is ben-\n",
      "eﬁcial for this task and could be beneﬁcial for\n",
      "the other tasks that deal with short phrases. Our\n",
      "best performing model (Accuracy: 0.917, Rank:\n",
      "1.156) was developed by ﬁne-tuning Sentence-\n",
      "BERT [Reimers et al., 2019] (with FinBERT at the\n",
      "backend) over an extended labelled set created us-\n",
      "ing the hierarchy of labels present in FIBO.\n",
      "\n",
      "Ontologies are rich sources of information that provide deep\n",
      "information about the underlying concepts and entities. This\n",
      "information is described for a speciﬁc domain, contains the\n",
      "clearly deﬁned relationship, and organizes in a deﬁned struc-\n",
      "ture mostly as a hierarchy. These properties make ontologies\n",
      "a great source for getting a deeper understanding of the rela-\n",
      "tionship and properties of resources from the domain in con-\n",
      "sideration.\n",
      "Public knowledge graphs and ontologies like DBpedia and\n",
      "Yago have been shown to work on various applications like\n",
      "\n",
      "the ones described in [Kobilarov et al., 2009] and [Hahm et\n",
      "al., 2014]. This has motivated and paved ways for the creation\n",
      "of domain focused ontologies like FIBO1.\n",
      "Effective techniques that enable identifying lexical similar-\n",
      "ity between the terms or concepts increase the effectiveness\n",
      "of the ontologies. These methods not only help in building\n",
      "new ontologies faster or augment the existing ones, but also\n",
      "it helps in the effective querying and concept search.\n",
      "FinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\n",
      "petitions are being held to promote the development of effec-\n",
      "tive similarity measures. In the third edition of the competi-\n",
      "tion FinSim-32 (being held in conjunction with 30th Interna-\n",
      "tional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\n",
      "the participants are challenged to develop methods and sys-\n",
      "tems to assign hypernym and synonyms to ﬁnancial terms by\n",
      "mapping them to one of the 17 high-level ﬁnancial concepts\n",
      "present in FIBO.\n",
      "In this paper, we present the systems developed by our\n",
      "team Lipi for hypernym and synonym assignment. We ex-\n",
      "perimented with basic featurization methods like TF-IDF and\n",
      "advanced methods like pre-trained embedding models. Our\n",
      "top 3 systems use pre-trained FinBERT [Araci, 2019] embed-\n",
      "ding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\n",
      "cial domain . We also augmented the training data by utilizing\n",
      "the knowledge from DBpedia, Investopedia, FIBO and text\n",
      "corpus of prospectus shared with us. We describe the works\n",
      "related to our solution in the next section. Section 3 contains\n",
      "the formal problem statement, followed by data description\n",
      "in section 4. We describe our top three systems in section 5.\n",
      "Section 6 contains the details of the experimentation that we\n",
      "performed and the results from some of them. We draw our\n",
      "conclusions in section 7 while giving a glimpse of things that\n",
      "we would like to try in the future.\n",
      "\n",
      "46\n",
      "\n",
      "Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \n",
      "(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \n",
      "\n",
      "Given a set F consisting of n tuples of ﬁnancial terms\n",
      "and their hypernyms/top-level concepts/labels i.e.\n",
      "F\n",
      "=\n",
      "{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\n",
      "nym corresponding to the ith term ti and hiϵ set of labels men-\n",
      "tioned in Table 1. For every unseen ﬁnancial term, our task is\n",
      "to generate a ranked list ˆyi consisting of these 17 hypernyms\n",
      "in order of decreasing semantic similarity.\n",
      "Evaluation Metrics The expected output is a raked list of\n",
      "predicted labels for every scored instance. The proposed sys-\n",
      "tems are evaluated based on Accuracy and Mean Rank met-\n",
      "rices as per the shared task rules. Evaluation script was pro-\n",
      "vided by organizers, where accuracy and mean rank were de-\n",
      "ﬁned as:\n",
      "Accuracy = 1\n",
      "\n",
      "The training dataset shared for this task has a total of\n",
      "1050 single and multi-word terms tagged to 17 different\n",
      "classes/labels. More than 91% of the terms have 6 words or\n",
      "less and the longest term has 22 words. There were 10 du-\n",
      "plicate entries, and 3 terms were assigned 2 different labels.\n",
      "Along with this, a corpus of prospectuses in English was pro-\n",
      "vided that had 211 documents. Some of the terms mentioned\n",
      "\n",
      "in the training data were present in the corpus. Table 1 shows\n",
      "the distribution of these labels in the training set.\n",
      "\n",
      "4.2\n",
      "Data Augmentation\n",
      "Since the majority of the terms had only a few tokens, we\n",
      "decided to expand the terms wherever possible using various\n",
      "sources.\n",
      "This approach had also been adopted by [Saini,\n",
      "2020] and [Pei and Zhang, 2021] while participating in\n",
      "FinSim-1 and FinSim-2 respectively.\n",
      "\n",
      "Deﬁnitions from DBpedia: We used the DBpedia search\n",
      "API4 to extract the description of the terms present in the\n",
      "\n",
      "3https://spacy.io/\n",
      "4https://lookup.dbpedia.org/api/search\n",
      "\n",
      "47\n",
      "\n",
      "train and test sets.\n",
      "We present such an example in Fig-\n",
      "ure 1. In addition to the description, the label was also re-\n",
      "tained from the result payload to identify the right descrip-\n",
      "tion for the input terms. We tried token overlap-based simi-\n",
      "larity of input terms with both matching labels and descrip-\n",
      "tions. We decided to use the label to term match for descrip-\n",
      "tion matching after going through a randomly drawn sam-\n",
      "ple. We cleaned both input terms and labels from DBpedia\n",
      "results by converting them to lower case, replacing punctua-\n",
      "tions by space, removing repetitive spaces, and singularizing\n",
      "the text. We calculated the token overlap ratios for cleaned\n",
      "term and DBpedia labels using the formulas mentioned be-\n",
      "low: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\n",
      "length(s2/length(s1 where s1 and s2 represents sets of to-\n",
      "kenized cleaned terms and tokenized and cleaned DBpedia\n",
      "labels respectively. We empirically decided to use all the in-\n",
      "stances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\n",
      "a DBpedia label (and hence description) to the input term.\n",
      "\n",
      "Deﬁnitions from Investopedia and FIBO: Inspired by\n",
      "[Saini, 2020], we obtained deﬁnitions of the terms present\n",
      "in Investopedia’s data dictionary5 by crawling it. We down-\n",
      "loaded a glossary of ﬁnancial terms from the website of\n",
      "FIBO. We cleaned all the terms from the train and test set and\n",
      "also the terms present in Investopedia’s data dictionary using\n",
      "the steps described in the above DBpedia section. We then as-\n",
      "signed the Investopedia or FIBO deﬁnition to the terms from\n",
      "the train and test sets where cleaned terms from train and test\n",
      "data matched to cleaned Investopedia terms perfectly.\n",
      "The test set which was provided to us had 326 terms. We\n",
      "augmented the original train and test set with the records\n",
      "where we could either ﬁnd deﬁnition or expansion using the\n",
      "above sources. The train set size increased to 1801 records\n",
      "and the test set size increased to 607 after the data augmen-\n",
      "tation. We present an example of data augmentation for the\n",
      "term ”callable bond” in Table 2. Table 3 states the number\n",
      "of instances we used from each of the sources to augment the\n",
      "data we had.\n",
      "\n",
      "5\n",
      "System Description\n",
      "\n",
      "We tried to solve this problem as the term classiﬁcation and\n",
      "term similarity problems. Two of our 3 submissions are mod-\n",
      "elled as the term classiﬁcation problem, whereas the third sys-\n",
      "tem is designed to be a phrase/sentence similarity problem be-\n",
      "tween terms (or expanded terms from the augmented dataset)\n",
      "and the deﬁnitions of 17 class labels that were extracted from\n",
      "FIBO / Internet. All the systems rely on semantic similarity\n",
      "and use FinBERT model to generate the term or token embed-\n",
      "ding representations. We divided the given data into training\n",
      "and validation sets having 841 and 209 terms respectively.\n",
      "\n",
      "5.1\n",
      "System - 1 (S1)\n",
      "\n",
      "This is the simplest of our proposed systems, where we did\n",
      "not use the augmented dataset and stuck to the original set that\n",
      "was shared by organizers. We loaded FinBERT pre-trained\n",
      "\n",
      "5https://www.investopedia.com/ﬁnancial-term-dictionary-\n",
      "4769738\n",
      "\n",
      "model and ﬁne-tuned it by trying to classify the representa-\n",
      "tion of [CLS] token into one of the 17 labels mentioned pre-\n",
      "viously. Since the original data did not have longer terms,\n",
      "we kept the maximum length to 32, and train and validation\n",
      "batch sizes of 64. We used Adam optimizer with a learning\n",
      "rate of 0.00002. We ran the model for 40 epochs and picked\n",
      "the model saved after 18th epoch based on the performance on\n",
      "the validation set. Finally, we ranked the predictions based on\n",
      "the predicted probability of each class.\n",
      "\n",
      "5.2\n",
      "System - 2 (S2)\n",
      "\n",
      "This system is similar to System-1 with the only difference\n",
      "that data being the augmented set and not the original dataset.\n",
      "Since the augmented dataset has the descriptions of the terms,\n",
      "the input is considerably longer. Hence, we increased the\n",
      "maximum length to 256 while keeping all the other hyper-\n",
      "parameters the same. After, training the model for 40 epochs\n",
      "we selected the model saved after the 17th epoch as the best\n",
      "model based on validation set performance.\n",
      "\n",
      "5.3\n",
      "System -3 (S3)\n",
      "\n",
      "We explored the FIBO ontology to understand the hierarchy\n",
      "[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\n",
      "in Figure 2. We used the augmented data described in sec-\n",
      "tion 4.2 to create a labelled dataset having similarity scores.\n",
      "For every term deﬁnition (T) to label deﬁnition (L) mapping\n",
      "which existed in the extended training set, we assigned a sim-\n",
      "ilarity score of 1.0 to the (T,L) pair and picked up 10 train-\n",
      "ing instances randomly ensuring none of their label deﬁnition\n",
      "was same as L. For each of the label deﬁnitions (LL) present\n",
      "in this sample, we extracted its root node and ﬁrst child node.\n",
      "We did the same for the original label deﬁnition (L). Then, we\n",
      "compared these nodes. If the root node and ﬁrst child node of\n",
      "L were different from that of LL then we assigned a similar-\n",
      "ity score of 0 to the (T, LL) pair. If the root nodes were the\n",
      "same, we assigned a similarity score of ’k’ when the ﬁrst child\n",
      "nodes differed and a similarity score of ’2k’ when they were\n",
      "the same (where 0 < k < 1). We empirically ﬁgured out that\n",
      "k=0.4 works the best. As expected, the number of instances\n",
      "with a similarity score equal to 0 increased substantially. We\n",
      "under-sampled such instances and the new training set had\n",
      "30% instances with similarity score 1.0, 12% instances with\n",
      "similarity score ’k’, 28% instances with similarity score ’2k’\n",
      "and 30% instances with similarity score 0. After that, we ﬁne-\n",
      "tuned a FinBERT [Araci, 2019] model using Sentence BERT\n",
      "[Reimers et al., 2019] framework with this newly generated\n",
      "labelled data for 25 epochs with a batch size of 20. Our ob-\n",
      "jective was to minimize the multiple negatives ranking loss\n",
      "and online contrastive loss. We used a margin of 0.5 and co-\n",
      "sine distance as a distance metric while training this model.\n",
      "Finally, we converted all of the 17 labels’ deﬁnitions and term\n",
      "deﬁnitions from the validation set to vectors using this ﬁne-\n",
      "tuned model. For every such term deﬁnition, we performed\n",
      "a semantic search over the label vectors and ranked them in\n",
      "decreasing order of similarity.\n",
      "System 2 and 3 take advantage of term expansion during both\n",
      "model training and scoring phases, which causes certain ob-\n",
      "servations to appear more than once (reference: Table 3). We\n",
      "\n",
      "48\n",
      "\n",
      "We had 1040 observations after removing the duplicates. We\n",
      "did an 80:20 split to create a training and validation set from\n",
      "this. We augmented the given modelling set by incorporating\n",
      "deﬁnitions from DBpedia, FIBO and Investopedia. We used\n",
      "the list of acronyms extracted from the prospectus corpus to\n",
      "create a copy with acronym expansion. This helped us to in-\n",
      "crease the original data to 1836 records (mentioned in Table\n",
      "1). It should be noted that we could not ﬁnd the expansions\n",
      "for all the terms given in the modelling set. Train and valida-\n",
      "tion set sizes for the original modelling set and expanded data\n",
      "were (832 & 208) and (1469 & 366) respectively.\n",
      "We established a baseline by running the scripts provided\n",
      "by the organizers. Then, we considered original modelling\n",
      "data and ﬁne-tuned base BERT-cased model [Devlin et al.,\n",
      "2019] to predict the class label by taking the representa-\n",
      "tion of [CLS] token while passing it through few layers of\n",
      "a feed-forward network. This performed better than base-\n",
      "line. We then tried the same BERT-base model on the ex-\n",
      "panded dataset, which gave us further performance improve-\n",
      "ment. Since the only change between these runs was the data,\n",
      "\n",
      "the improvement can be attributed to the expanded data.\n",
      "We experimented with a few of the other pre-trained mod-\n",
      "els that are available on the Huggingface model repository\n",
      "[Wolf et al., 2020]. We observed clear improvement when\n",
      "we used the FinBERT model which was trained on data spe-\n",
      "ciﬁc to the ﬁnancial domain. The model performance succes-\n",
      "sively increased when we used a combination of data expan-\n",
      "sion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\n",
      "BERT using Sentence Transformers [Reimers et al., 2019] to\n",
      "capture semantic textual similarity. For this, we used several\n",
      "combinations of term and term deﬁnitions with label and la-\n",
      "bel deﬁnitions.\n",
      "All the hyperparameters for the ﬁnal 3 models are already\n",
      "given in the system description. After rigorous experimenta-\n",
      "tion, these hyperparameters were selected empirically based\n",
      "on validation set performance. The results are presented in\n",
      "Table 4. Since the number of submissions was restricted to\n",
      "3 for each team, we do not have the performance numbers of\n",
      "the BERT models in the test set. Analysing the results we see\n",
      "that SentenceBERT trained with FinBERT at the backed as\n",
      "mentioned in section-5.3 performed the best.\n",
      "\n",
      "7\n",
      "Conclusion and Future Works\n",
      "In this work, we attempted to solve the hypernym and syn-\n",
      "onym discovery hosted at FinSim-3. This challenge aimed\n",
      "to enable the better use of ontologies like FIBO using hy-\n",
      "pernyms and synonyms, and we used these ontologies them-\n",
      "selves to develop our systems which perform signiﬁcantly\n",
      "better than the provided baseline systems. This proves the\n",
      "present use of these ontologies. The presented solution is\n",
      "recursive in a sense as it uses knowledge from ontologies\n",
      "to further increase the effectiveness and use of the same.\n",
      "Apart from data augmentation, our solution relies upon se-\n",
      "mantic similarity learnt from pre-trained embedding models\n",
      "\n",
      "49\n",
      "\n",
      "that were learnt on the relevant domain. We observed the\n",
      "clear beneﬁts of domain speciﬁc pretraining during the ex-\n",
      "perimentation.\n",
      "In future, we would like to explore Knowledge Graphs (as\n",
      "described in [Portisch et al., 2021]) to further improve the\n",
      "improve performance of the models. We also want to ex-\n",
      "plore other variants of FinBERT [Araci, 2019] and ﬁne-tune\n",
      "them using the Masked Language Modeling technique (as\n",
      "mentioned by the winner of FinSim-2 [Chersoni and Huang,\n",
      "2021]) and Next Sentence Prediction objective. Moreover,\n",
      "this research can be extended by extracting sentences present\n",
      "in the prospectus (similar to [Goel et al., 2021]) to create pos-\n",
      "itive and negative samples.\n",
      "\n",
      "References\n",
      "[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\n",
      "and Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\n",
      "ing sense of text in ﬁnancial domain. In Proceedings of\n",
      "the Second Workshop on Financial Technology and Natu-\n",
      "ral Language Processing, pages 104–107, Kyoto, Japan, 5\n",
      "January 2020.\n",
      "[Araci, 2019] Dogu Araci.\n",
      "Finbert:\n",
      "Financial sentiment\n",
      "analysis with pre-trained language models, 2019.\n",
      "[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\n",
      "bilarov, Jens Lehmann, Richard Cyganiak, and Zachary\n",
      "Ives. Dbpedia: A nucleus for a web of open data, 2007.\n",
      "\n",
      "[Bernier-Colborne and Barri`ere, 2018] Gabriel\n",
      "Bernier-\n",
      "Colborne and Caroline Barri`ere. CRIM at SemEval-2018\n",
      "task 9: A hybrid approach to hypernym discovery. In Pro-\n",
      "ceedings of The 12th International Workshop on Semantic\n",
      "Evaluation, pages 725–731, New Orleans, Louisiana,\n",
      "June 2018. Association for Computational Linguistics.\n",
      "\n",
      "[Camacho-Collados et al., 2018] Jose\n",
      "Camacho-Collados,\n",
      "Claudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\n",
      "Tommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\n",
      "Navigli, and Horacio Saggion.\n",
      "SemEval-2018 task 9:\n",
      "Hypernym discovery. In Proceedings of The 12th Interna-\n",
      "tional Workshop on Semantic Evaluation, pages 712–724,\n",
      "New Orleans, Louisiana, June 2018. Association for\n",
      "Computational Linguistics.\n",
      "\n",
      "[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\n",
      "Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\n",
      "stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\n",
      "Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\n",
      "versal sentence encoder, 2018.\n",
      "\n",
      "[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\n",
      "Ren Huang. PolyU-CBS at the FinSim-2 Task: Combin-\n",
      "ing Distributional, String-Based and Transformers-Based\n",
      "Features for Hypernymy Detection in the Financial Do-\n",
      "main, page 316–319. Association for Computing Machin-\n",
      "ery, New York, NY, USA, 2021.\n",
      "\n",
      "[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\n",
      "ton Lee, and Kristina Toutanova. BERT: Pre-training of\n",
      "deep bidirectional transformers for language understand-\n",
      "ing. In Proceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume 1\n",
      "(Long and Short Papers), pages 4171–4186, Minneapo-\n",
      "lis, Minnesota, June 2019. Association for Computational\n",
      "Linguistics.\n",
      "\n",
      "[Goel et al., 2021] Tushar Goel,\n",
      "Vipul Chauhan,\n",
      "Ishan\n",
      "Verma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\n",
      "2021 @FinSim-2: Transformer Based Models for Auto-\n",
      "matic Classiﬁcation of Financial Terms, page 311–315.\n",
      "\n",
      "50\n",
      "\n",
      "Association for Computing Machinery, New York, NY,\n",
      "USA, 2021.\n",
      "[Hahm et al., 2014] Younggyun\n",
      "Hahm,\n",
      "Jungyeul\n",
      "Park,\n",
      "Kyungtae Lim, Youngsik Kim, Dosam Hwang, and\n",
      "Key-Sun Choi.\n",
      "Named entity corpus construction us-\n",
      "ing wikipedia and dbpedia ontology.\n",
      "In LREC, pages\n",
      "2565–2569, 2014.\n",
      "[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\n",
      "Soﬁe Van Landeghem, and Adriane Boyd.\n",
      "spaCy:\n",
      "Industrial-strength\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "in\n",
      "Python, 2020.\n",
      "[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\n",
      "mad Taher Pilehvar.\n",
      "SemEval-2016 task 14: Semantic\n",
      "taxonomy enrichment. In Proceedings of the 10th Interna-\n",
      "tional Workshop on Semantic Evaluation (SemEval-2016),\n",
      "pages 1092–1102, San Diego, California, June 2016. As-\n",
      "sociation for Computational Linguistics.\n",
      "[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\n",
      "Ashutosh Modi. IITK at the FinSim task: Hypernym de-\n",
      "tection in ﬁnancial domain via context-free and contextu-\n",
      "alized word embeddings. In Proceedings of the Second\n",
      "Workshop on Financial Technology and Natural Language\n",
      "Processing, pages 87–92, Kyoto, Japan, 5 January 2020.\n",
      "[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\n",
      "Raimond,\n",
      "Silver\n",
      "Oliver,\n",
      "Chris\n",
      "Sizemore,\n",
      "Michael\n",
      "Smethurst, Christian Bizer, and Robert Lee.\n",
      "Media\n",
      "meets semantic web – how the bbc uses dbpedia and\n",
      "linked data to make connections. In Lora Aroyo, Paolo\n",
      "Traverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\n",
      "Eero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\n",
      "Sabou, and Elena Simperl, editors, The Semantic Web: Re-\n",
      "search and Applications, pages 723–737, Berlin, Heidel-\n",
      "berg, 2009. Springer Berlin Heidelberg.\n",
      "[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\n",
      "Virginie Mouilleron, and Dialekti Valsamou-Stanislawski.\n",
      "The FinSim 2020 shared task: Learning semantic repre-\n",
      "sentations for the ﬁnancial domain. In Proceedings of the\n",
      "Second Workshop on Financial Technology and Natural\n",
      "Language Processing, pages 81–86, Kyoto, Japan, 5 Jan-\n",
      "uary 2020.\n",
      "[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\n",
      "mail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\n",
      "ing Semantic Similarities for the Financial Domain, page\n",
      "288–292.\n",
      "Association for Computing Machinery, New\n",
      "York, NY, USA, 2021.\n",
      "[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\n",
      "rado, and Jeffrey Dean. Efﬁcient estimation of word rep-\n",
      "resentations in vector space, 2013.\n",
      "[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\n",
      "Gael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\n",
      "LBPAM at the FinSim-2 Task: Learning Financial Seman-\n",
      "tic Similarities with Siamese Transformers, page 302–306.\n",
      "Association for Computing Machinery, New York, NY,\n",
      "USA, 2021.\n",
      "[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\n",
      "the ﬁnsim-2 task: Learning word representations of ﬁnan-\n",
      "\n",
      "cial data with customized corpus.\n",
      "In Companion Pro-\n",
      "ceedings of the Web Conference 2021, WWW ’21, page\n",
      "307–310, New York, NY, USA, 2021. Association for\n",
      "Computing Machinery.\n",
      "[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\n",
      "Heiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\n",
      "tection in the Financial Services Domain Using Knowl-\n",
      "edge Graphs, page 293–297. Association for Computing\n",
      "Machinery, New York, NY, USA, 2021.\n",
      "[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\n",
      "David Luan, Dario Amodei, and Ilya Sutskever. Language\n",
      "models are unsupervised multitask learners, 2019.\n",
      "[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\n",
      "Reimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\n",
      "Johannes Daxenberger, and Iryna Gurevych.\n",
      "Sentence-\n",
      "bert: Sentence embeddings using siamese bert-networks.\n",
      "In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing. Association for\n",
      "Computational Linguistics, 2019.\n",
      "[Saini, 2020] Anuj Saini.\n",
      "Anuj at the FinSim task:\n",
      "Anuj@FINSIM¡VLearning semantic representation of ﬁ-\n",
      "nancial domain with investopedia. In Proceedings of the\n",
      "Second Workshop on Financial Technology and Natural\n",
      "Language Processing, pages 93–97, Kyoto, Japan, 5 Jan-\n",
      "uary 2020.\n",
      "[Stepiˇsnik Perdih et al., 2021] Timen\n",
      "Stepiˇsnik\n",
      "Perdih,\n",
      "Senja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\n",
      "Ontology-Augmented Financial Concept Classiﬁcation,\n",
      "page 298–301.\n",
      "Association for Computing Machinery,\n",
      "New York, NY, USA, 2021.\n",
      "[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\n",
      "Sanh, Julien Chaumond, Clement Delangue, Anthony\n",
      "Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\n",
      "Funtowicz, Joe Davison, Sam Shleifer, Patrick von\n",
      "Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen\n",
      "Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\n",
      "Quentin Lhoest, and Alexander M. Rush. Huggingface’s\n",
      "transformers: State-of-the-art natural language processing,\n",
      "2020.\n",
      "\n",
      "51\n",
      "\n",
      "Hypernym and synonym matching are one of the\n",
      "mainstream Natural Language Processing (NLP)\n",
      "tasks. In this paper, we present systems that at-\n",
      "tempt to solve this problem.\n",
      "We designed these\n",
      "systems to participate in the FinSim-3, a shared\n",
      "task of FinNLP workshop at IJCAI-2021.\n",
      "The\n",
      "shared task is focused on solving this problem for\n",
      "the ﬁnancial domain. We experimented with var-\n",
      "ious transformer based pre-trained embeddings by\n",
      "ﬁne-tuning these for either classiﬁcation or phrase\n",
      "similarity tasks. We also augmented the provided\n",
      "dataset with abbreviations derived from prospectus\n",
      "provided by the organizers and deﬁnitions of the\n",
      "ﬁnancial terms from DBpedia [Auer et al., 2007],\n",
      "Investopedia, and the Financial Industry Business\n",
      "Ontology (FIBO). Our best performing system uses\n",
      "both FinBERT [Araci, 2019] and data augmenta-\n",
      "tion from the afore-mentioned sources.\n",
      "We ob-\n",
      "served that term expansion using data augmentation\n",
      "in conjunction with semantic similarity is beneﬁcial\n",
      "for this task and could be useful for the other tasks\n",
      "that deal with short phrases. Our best performing\n",
      "model (Accuracy: 0.917, Rank: 1.156) was devel-\n",
      "oped by ﬁne-tuning SentenceBERT [Reimers et al.,\n",
      "2019] (with FinBERT at the backend) over an ex-\n",
      "tended labelled set created using the hierarchy of\n",
      "labels present in FIBO.\n",
      "\n",
      "Ontologies are rich sources of information that provide deep\n",
      "information about the underlying concepts and entities. This\n",
      "information is described for a speciﬁc domain. It contains the\n",
      "clearly deﬁned relationships, and it is organized in a deﬁned\n",
      "structure mostly as a hierarchy. These properties make on-\n",
      "tologies a great source for getting a deeper understanding of\n",
      "the relationship and properties of resources of the domain in\n",
      "consideration.\n",
      "Public knowledge graphs and ontologies like DBpedia and\n",
      "Yago have been shown to work on various applications like\n",
      "\n",
      "the ones described in [Kobilarov et al., 2009] and [Hahm et\n",
      "al., 2014]. This has motivated and paved ways for the creation\n",
      "of domain focused ontologies like FIBO1.\n",
      "Effective techniques that enable identifying lexical similar-\n",
      "ity between the terms or concepts increase the effectiveness\n",
      "of the ontologies. These methods not only help in building\n",
      "new ontologies faster or augment the existing ones, but also\n",
      "it helps in the effective querying and searching of concepts.\n",
      "FinSim [Maarouf et al., 2020; Mansar et al., 2021] compe-\n",
      "titions are being held to promote the development of effective\n",
      "similarity measures. In the third edition of the competition\n",
      "FinSim-32 (being held in conjunction with the 30th Interna-\n",
      "tional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\n",
      "the participants are challenged to develop methods and sys-\n",
      "tems to rank hypernym and synonyms to ﬁnancial terms by\n",
      "mapping them to one of the 17 high-level ﬁnancial concepts\n",
      "present in FIBO.\n",
      "In this paper, we present the systems developed by our\n",
      "team Lipi for hypernym and synonym ranking. We experi-\n",
      "mented with basic featurization methods like TF-IDF and ad-\n",
      "vanced methods like pre-trained embedding models. Our top\n",
      "3 systems use pre-trained FinBERT [Araci, 2019] embedding\n",
      "model that was ﬁne-tuned on the data speciﬁc to ﬁnancial do-\n",
      "main . We also augmented the training data by utilizing the\n",
      "knowledge from DBpedia, Investopedia, FIBO and text cor-\n",
      "pus of prospectus shared with us. We describe the works re-\n",
      "lated to our solution in the next section. Section 3 contains\n",
      "the formal problem statement, followed by data description\n",
      "in section 4. We describe our top three systems in section 5.\n",
      "Section 6 contains the details of the experimentation that we\n",
      "performed and the results obtained from some of them. We\n",
      "draw our conclusions in section 7 while giving a glimpse of\n",
      "things that we would like to try in the future.\n",
      "\n",
      "arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\n",
      "\n",
      "Given a set F consisting of n tuples of ﬁnancial terms\n",
      "and\n",
      "their\n",
      "hypernyms/top-level\n",
      "concepts/labels\n",
      "i.e.\n",
      "F\n",
      "=\n",
      "{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents\n",
      "the hypernym corresponding to the ith term ti and hiϵ set of\n",
      "labels mentioned in Table 1. For every unseen ﬁnancial term,\n",
      "our task is to generate a ranked list ˆyi consisting of these 17\n",
      "hypernyms in order of decreasing semantic similarity.\n",
      "\n",
      "Evaluation Metrics The expected output is a raked list of\n",
      "predicted labels for every scored instance. The proposed sys-\n",
      "tems are evaluated based on Accuracy and Mean Rank met-\n",
      "rices as per the shared task rules. Evaluation script was pro-\n",
      "vided by organizers, where accuracy and mean rank were de-\n",
      "ﬁned as:\n",
      "Accuracy = 1\n",
      "\n",
      "The training dataset shared for this task has a total of\n",
      "1050 single and multi-word terms tagged to 17 different\n",
      "classes/labels out of which 1040 term-label pairs are unique.\n",
      "More than 91% of the terms have 6 words or less and the\n",
      "longest term has 22 words. There were 10 duplicate entries,\n",
      "and 3 terms were assigned 2 different labels. Along with this,\n",
      "\n",
      "a corpus of prospectuses in English that had 211 documents\n",
      "was provided. Some of the terms mentioned in the training\n",
      "data were present in the corpus. Table 1 shows the distribu-\n",
      "tion of these labels in the training set.\n",
      "\n",
      "4.2\n",
      "Data Augmentation\n",
      "\n",
      "Since the majority of the terms had only a few tokens, we\n",
      "decided to expand the terms wherever possible using various\n",
      "sources.\n",
      "This approach had also been adopted by [Saini,\n",
      "2020] and [Pei and Zhang, 2021] while participating in\n",
      "FinSim-1 and FinSim-2 respectively.\n",
      "\n",
      "3https://spacy.io/\n",
      "\n",
      "Deﬁnitions from DBpedia: We used the DBpedia search\n",
      "API4 to extract the description of the terms present in the train\n",
      "and test sets. We present such an example in Figure 1. In ad-\n",
      "dition to the description, the label was also retained from the\n",
      "result payload to identify the right description for the input\n",
      "terms. We tried token overlap-based similarity of input terms\n",
      "with both matching labels and descriptions. We decided to\n",
      "use the label to term match for description matching after go-\n",
      "ing through a randomly drawn sample. We cleaned both input\n",
      "terms and labels from DBpedia results by converting them to\n",
      "lower case, replacing punctuations by space, removing repet-\n",
      "itive spaces, and singularizing the text. We calculated the\n",
      "token overlap ratios for cleaned term and DBpedia labels us-\n",
      "ing these formulas: Ratio1 = length(s1 ∩ s2)/length(s1),\n",
      "Ratio2 = length(s2)/length(s1) where s1 and s2 represents\n",
      "sets of tokenized cleaned terms and tokenized cleaned DBpe-\n",
      "dia labels respectively. We empirically decided to use all the\n",
      "instances with Ratio1 = 1 and Ratio2 <= 1.25 for match-\n",
      "ing a DBpedia label (and hence description) to the input term.\n",
      "\n",
      "Deﬁnitions from Investopedia and FIBO: Inspired by\n",
      "[Saini, 2020], we obtained deﬁnitions of the terms present\n",
      "in Investopedia’s data dictionary5 by crawling it. We down-\n",
      "loaded a glossary of ﬁnancial terms from the website of\n",
      "FIBO. We cleaned all the terms from the train and test set and\n",
      "also the terms present in Investopedia’s data dictionary using\n",
      "the steps described in the above DBpedia section. We then as-\n",
      "signed the Investopedia or FIBO deﬁnition to the terms from\n",
      "the train and test sets where cleaned terms from train and test\n",
      "data matched to cleaned Investopedia terms perfectly.\n",
      "The test set which was provided to us had 326 terms. We\n",
      "augmented the original train and test set with the records\n",
      "where we could either ﬁnd deﬁnition or expansion using the\n",
      "above sources. The train set size increased to 1836 records\n",
      "and the test set size increased to 607 after the data augmen-\n",
      "tation. We present an example of data augmentation for the\n",
      "term “callable bond” in Table 2. Table 3 states the number\n",
      "of instances we used from each of the sources to augment the\n",
      "data we had.\n",
      "\n",
      "5\n",
      "System Description\n",
      "We tried to solve this problem as the term classiﬁcation and\n",
      "term similarity problems. Two of our 3 submissions are mod-\n",
      "elled as the term classiﬁcation problem, whereas the third sys-\n",
      "tem is designed to be a phrase/sentence similarity problem be-\n",
      "tween terms (or expanded terms from the augmented dataset)\n",
      "and the deﬁnitions of 17 class labels that were extracted from\n",
      "FIBO / Internet. All the systems rely on semantic similarity\n",
      "and use FinBERT model to generate the term or token embed-\n",
      "ding representations. We divided the given data into training\n",
      "and validation sets having 832 and 208 terms respectively.\n",
      "\n",
      "5.1\n",
      "System - 1 (S1)\n",
      "This is the simplest of our proposed systems, where we did\n",
      "not use the augmented dataset and used only the original set\n",
      "\n",
      "4https://lookup.dbpedia.org/api/search\n",
      "5https://www.investopedia.com/ﬁnancial-term-dictionary-\n",
      "4769738\n",
      "\n",
      "that was shared by organizers.\n",
      "We loaded FinBERT pre-\n",
      "trained model and ﬁne-tuned it by trying to classify the repre-\n",
      "sentation of [CLS] token into one of the 17 labels mentioned\n",
      "previously. Since the original data did not have longer terms,\n",
      "we kept the maximum length to 32, and train and validation\n",
      "batch sizes of 64. We used Adam optimizer with a learning\n",
      "rate of 0.00002. We ran the model for 40 epochs and picked\n",
      "the model saved after 18th epoch based on the performance on\n",
      "the validation set. Finally, we ranked the predictions based on\n",
      "the predicted probability of each class.\n",
      "\n",
      "5.2\n",
      "System - 2 (S2)\n",
      "This system is similar to System-1 with the only difference\n",
      "that data being the augmented set and not the original dataset.\n",
      "Since the augmented dataset had the descriptions of the terms,\n",
      "the inputs were considerably longer. Hence, we increased the\n",
      "maximum length to 256 while keeping all the other hyper-\n",
      "parameters the same. After, training the model for 40 epochs\n",
      "we selected the model saved after the 17th epoch as the best\n",
      "model based on validation set performance.\n",
      "\n",
      "5.3\n",
      "System -3 (S3)\n",
      "We explored the FIBO ontology to understand the hierarchy\n",
      "[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\n",
      "in Figure 2. We used the augmented data described in sec-\n",
      "tion 4.2 to create a labelled dataset having similarity scores.\n",
      "For every term deﬁnition (T) to label deﬁnition (L) mapping\n",
      "which existed in the extended training set, we assigned a sim-\n",
      "ilarity score of 1.0 to the (T,L) pair and picked up 10 train-\n",
      "ing instances randomly ensuring none of their label deﬁnition\n",
      "was same as L. For each of the label deﬁnitions (LL) present\n",
      "in this sample, we extracted its root node and ﬁrst child node.\n",
      "We did the same for the original label deﬁnition (L). Then, we\n",
      "compared these nodes. If the root node and ﬁrst child node of\n",
      "L were different from that of LL then we assigned a similar-\n",
      "ity score of 0 to the (T, LL) pair. If the root nodes were the\n",
      "same, we assigned a similarity score of ’k’ when the ﬁrst child\n",
      "nodes differed and a similarity score of ’2k’ when they were\n",
      "the same (where 0 < k < 1). We empirically ﬁgured out that\n",
      "k=0.4 works the best. As expected, the number of instances\n",
      "with a similarity score equal to 0 increased substantially. We\n",
      "under-sampled such instances and the new training set had\n",
      "30% instances with similarity score 1.0, 12% instances with\n",
      "similarity score ’k’, 28% instances with similarity score ’2k’\n",
      "and 30% instances with similarity score 0. After that, we ﬁne-\n",
      "tuned a FinBERT [Araci, 2019] model using Sentence BERT\n",
      "[Reimers et al., 2019] framework with this newly generated\n",
      "labelled data for 25 epochs with a batch size of 20. Our ob-\n",
      "jective was to minimize the multiple negatives ranking loss\n",
      "and online contrastive loss. We used a margin of 0.5 and co-\n",
      "sine distance as a distance metric while training this model.\n",
      "Finally, we converted all of the 17 labels’ deﬁnitions and term\n",
      "deﬁnitions from the validation set to vectors using this ﬁne-\n",
      "tuned model. For every such term deﬁnition, we performed\n",
      "a semantic search over the label vectors and ranked them in\n",
      "decreasing order of cosine similarity.\n",
      "System 2 and 3 take advantage of term expansion during both\n",
      "model training and scoring phases, which causes certain ob-\n",
      "servations to appear more than once (reference: Table 3). We\n",
      "\n",
      "We had 1040 observations after removing the duplicates. We\n",
      "did an 80:20 split to create a training and validation set from\n",
      "this. We augmented the given modelling set by incorporating\n",
      "deﬁnitions from DBpedia, FIBO and Investopedia. We used\n",
      "the list of acronyms extracted from the prospectus corpus to\n",
      "create a copy with acronym expansion. This helped us to in-\n",
      "crease the original data to 1836 records (mentioned in Table\n",
      "1). It should be noted that we could not ﬁnd the expansions\n",
      "for all the terms given in the modelling set. Train and valida-\n",
      "tion set sizes for the original modelling set and expanded data\n",
      "were (832 & 208) and (1470 & 366) respectively.\n",
      "We established a baseline by running the scripts provided\n",
      "by the organizers. Then, we considered original modelling\n",
      "data and ﬁne-tuned base BERT-cased model [Devlin et al.,\n",
      "2019] to predict the class label by taking the representa-\n",
      "tion of [CLS] token while passing it through few layers of\n",
      "a feed-forward network. This performed better than base-\n",
      "line. We then tried the same BERT-base model on the ex-\n",
      "panded dataset, which gave us further performance improve-\n",
      "ment. Since the only major change between these runs was\n",
      "\n",
      "the data, the improvement can be attributed to the expanded\n",
      "data.\n",
      "We experimented with a few of the other pre-trained mod-\n",
      "els that are available on the Huggingface model repository\n",
      "[Wolf et al., 2020]. We observed clear improvement when\n",
      "we used the FinBERT model which was trained on data spe-\n",
      "ciﬁc to the ﬁnancial domain. The model performance succes-\n",
      "sively increased when we used a combination of data expan-\n",
      "sion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\n",
      "BERT using Sentence Transformers [Reimers et al., 2019] to\n",
      "capture semantic textual similarity. For this, we used several\n",
      "combinations of term and term deﬁnitions with label and la-\n",
      "bel deﬁnitions.\n",
      "All the hyperparameters for the ﬁnal 3 models have been\n",
      "already mentioned in the system description. After rigorous\n",
      "experimentation, these hyperparameters were selected empir-\n",
      "ically based on validation set performance. The results are\n",
      "presented in Table 4. Since the number of submissions was\n",
      "restricted to 3 for each team, we do not have the performance\n",
      "numbers of the BERT models in the test set. Analysing the\n",
      "results we see that SentenceBERT trained with FinBERT at\n",
      "the backed as mentioned in section-5.3 performed the best.\n",
      "\n",
      "7\n",
      "Conclusion and Future Works\n",
      "\n",
      "In this work, we attempted to solve the hypernym and syn-\n",
      "onym discovery hosted at FinSim-3. This challenge aimed\n",
      "to enable the better use of ontologies like FIBO using hy-\n",
      "pernyms and synonyms, and we used these ontologies them-\n",
      "selves to develop our systems which perform signiﬁcantly\n",
      "better than the provided baseline systems. This proves the\n",
      "present use of these ontologies. The presented solution is\n",
      "recursive in a sense as it uses knowledge from ontologies\n",
      "to further increase the effectiveness and use of the same.\n",
      "\n",
      "Apart from data augmentation, our solution relies upon se-\n",
      "mantic similarity learnt from pre-trained embedding models\n",
      "that were learnt on the relevant domain. We observed the\n",
      "clear beneﬁts of domain speciﬁc pretraining during the ex-\n",
      "perimentation.\n",
      "In future, we would like to explore Knowledge Graphs (as\n",
      "described in [Portisch et al., 2021]) to further improve the\n",
      "improve performance of the models. We also want to ex-\n",
      "plore other variants of FinBERT [Araci, 2019] and ﬁne-tune\n",
      "them using the Masked Language Modeling technique (as\n",
      "mentioned by the winner of FinSim-2 [Chersoni and Huang,\n",
      "2021]) and Next Sentence Prediction objective. Moreover,\n",
      "this research can be extended by extracting sentences present\n",
      "in the prospectus (similar to [Goel et al., 2021]) to create\n",
      "more positive and negative samples.\n",
      "\n",
      "References\n",
      "\n",
      "[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\n",
      "and Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\n",
      "ing sense of text in ﬁnancial domain. In Proceedings of\n",
      "the Second Workshop on Financial Technology and Natu-\n",
      "ral Language Processing, pages 104–107, Kyoto, Japan, 5\n",
      "January 2020.\n",
      "\n",
      "[Araci, 2019] Dogu Araci.\n",
      "Finbert:\n",
      "Financial sentiment\n",
      "analysis with pre-trained language models, 2019.\n",
      "\n",
      "[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\n",
      "bilarov, Jens Lehmann, Richard Cyganiak, and Zachary\n",
      "Ives. Dbpedia: A nucleus for a web of open data, 2007.\n",
      "[Bernier-Colborne and Barri`ere, 2018] Gabriel\n",
      "Bernier-\n",
      "Colborne and Caroline Barri`ere. CRIM at SemEval-2018\n",
      "task 9: A hybrid approach to hypernym discovery. In Pro-\n",
      "ceedings of The 12th International Workshop on Semantic\n",
      "Evaluation, pages 725–731, New Orleans, Louisiana,\n",
      "June 2018. Association for Computational Linguistics.\n",
      "[Camacho-Collados et al., 2018] Jose\n",
      "Camacho-Collados,\n",
      "Claudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\n",
      "Tommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\n",
      "Navigli, and Horacio Saggion.\n",
      "SemEval-2018 task 9:\n",
      "Hypernym discovery. In Proceedings of The 12th Interna-\n",
      "tional Workshop on Semantic Evaluation, pages 712–724,\n",
      "New Orleans, Louisiana, June 2018. Association for\n",
      "Computational Linguistics.\n",
      "[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\n",
      "Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\n",
      "stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\n",
      "Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\n",
      "versal sentence encoder, 2018.\n",
      "[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\n",
      "Ren Huang. PolyU-CBS at the FinSim-2 Task: Combin-\n",
      "ing Distributional, String-Based and Transformers-Based\n",
      "Features for Hypernymy Detection in the Financial Do-\n",
      "main, page 316–319. Association for Computing Machin-\n",
      "ery, New York, NY, USA, 2021.\n",
      "[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\n",
      "ton Lee, and Kristina Toutanova. BERT: Pre-training of\n",
      "deep bidirectional transformers for language understand-\n",
      "ing. In Proceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume 1\n",
      "(Long and Short Papers), pages 4171–4186, Minneapo-\n",
      "lis, Minnesota, June 2019. Association for Computational\n",
      "Linguistics.\n",
      "[Goel et al., 2021] Tushar Goel,\n",
      "Vipul Chauhan,\n",
      "Ishan\n",
      "Verma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\n",
      "\n",
      "2021 @FinSim-2: Transformer Based Models for Auto-\n",
      "matic Classiﬁcation of Financial Terms, page 311–315.\n",
      "Association for Computing Machinery, New York, NY,\n",
      "USA, 2021.\n",
      "[Hahm et al., 2014] Younggyun\n",
      "Hahm,\n",
      "Jungyeul\n",
      "Park,\n",
      "Kyungtae Lim, Youngsik Kim, Dosam Hwang, and\n",
      "Key-Sun Choi.\n",
      "Named entity corpus construction us-\n",
      "ing wikipedia and dbpedia ontology.\n",
      "In LREC, pages\n",
      "2565–2569, 2014.\n",
      "[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\n",
      "Soﬁe Van Landeghem, and Adriane Boyd.\n",
      "spaCy:\n",
      "Industrial-strength\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "in\n",
      "Python, 2020.\n",
      "[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\n",
      "mad Taher Pilehvar.\n",
      "SemEval-2016 task 14: Semantic\n",
      "taxonomy enrichment. In Proceedings of the 10th Interna-\n",
      "tional Workshop on Semantic Evaluation (SemEval-2016),\n",
      "pages 1092–1102, San Diego, California, June 2016. As-\n",
      "sociation for Computational Linguistics.\n",
      "[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\n",
      "Ashutosh Modi. IITK at the FinSim task: Hypernym de-\n",
      "tection in ﬁnancial domain via context-free and contextu-\n",
      "alized word embeddings. In Proceedings of the Second\n",
      "Workshop on Financial Technology and Natural Language\n",
      "Processing, pages 87–92, Kyoto, Japan, 5 January 2020.\n",
      "[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\n",
      "Raimond,\n",
      "Silver\n",
      "Oliver,\n",
      "Chris\n",
      "Sizemore,\n",
      "Michael\n",
      "Smethurst, Christian Bizer, and Robert Lee.\n",
      "Media\n",
      "meets semantic web – how the bbc uses dbpedia and\n",
      "linked data to make connections. In Lora Aroyo, Paolo\n",
      "Traverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\n",
      "Eero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\n",
      "Sabou, and Elena Simperl, editors, The Semantic Web: Re-\n",
      "search and Applications, pages 723–737, Berlin, Heidel-\n",
      "berg, 2009. Springer Berlin Heidelberg.\n",
      "[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\n",
      "Virginie Mouilleron, and Dialekti Valsamou-Stanislawski.\n",
      "The FinSim 2020 shared task: Learning semantic repre-\n",
      "sentations for the ﬁnancial domain. In Proceedings of the\n",
      "Second Workshop on Financial Technology and Natural\n",
      "Language Processing, pages 81–86, Kyoto, Japan, 5 Jan-\n",
      "uary 2020.\n",
      "[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\n",
      "mail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\n",
      "ing Semantic Similarities for the Financial Domain, page\n",
      "288–292.\n",
      "Association for Computing Machinery, New\n",
      "York, NY, USA, 2021.\n",
      "[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\n",
      "rado, and Jeffrey Dean. Efﬁcient estimation of word rep-\n",
      "resentations in vector space, 2013.\n",
      "[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\n",
      "Gael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\n",
      "LBPAM at the FinSim-2 Task: Learning Financial Seman-\n",
      "tic Similarities with Siamese Transformers, page 302–306.\n",
      "Association for Computing Machinery, New York, NY,\n",
      "USA, 2021.\n",
      "\n",
      "[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\n",
      "the ﬁnsim-2 task: Learning word representations of ﬁnan-\n",
      "cial data with customized corpus.\n",
      "In Companion Pro-\n",
      "ceedings of the Web Conference 2021, WWW ’21, page\n",
      "307–310, New York, NY, USA, 2021. Association for\n",
      "Computing Machinery.\n",
      "[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\n",
      "Heiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\n",
      "tection in the Financial Services Domain Using Knowl-\n",
      "edge Graphs, page 293–297. Association for Computing\n",
      "Machinery, New York, NY, USA, 2021.\n",
      "[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\n",
      "David Luan, Dario Amodei, and Ilya Sutskever. Language\n",
      "models are unsupervised multitask learners, 2019.\n",
      "[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\n",
      "Reimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\n",
      "Johannes Daxenberger, and Iryna Gurevych.\n",
      "Sentence-\n",
      "bert: Sentence embeddings using siamese bert-networks.\n",
      "In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing. Association for\n",
      "Computational Linguistics, 2019.\n",
      "[Saini, 2020] Anuj Saini.\n",
      "Anuj at the FinSim task:\n",
      "Anuj@FINSIM¡VLearning semantic representation of ﬁ-\n",
      "nancial domain with investopedia. In Proceedings of the\n",
      "Second Workshop on Financial Technology and Natural\n",
      "Language Processing, pages 93–97, Kyoto, Japan, 5 Jan-\n",
      "uary 2020.\n",
      "[Stepiˇsnik Perdih et al., 2021] Timen\n",
      "Stepiˇsnik\n",
      "Perdih,\n",
      "Senja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\n",
      "Ontology-Augmented Financial Concept Classiﬁcation,\n",
      "page 298–301.\n",
      "Association for Computing Machinery,\n",
      "New York, NY, USA, 2021.\n",
      "[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\n",
      "Sanh, Julien Chaumond, Clement Delangue, Anthony\n",
      "Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\n",
      "Funtowicz, Joe Davison, Sam Shleifer, Patrick von\n",
      "Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen\n",
      "Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\n",
      "Quentin Lhoest, and Alexander M. Rush. Huggingface’s\n",
      "transformers: State-of-the-art natural language processing,\n",
      "2020.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in lst_Anth:\n",
    "    if elem not in elems_in_both_lists:\n",
    "        print(elem)\n",
    "for elem in lst_Arxiv:\n",
    "    if elem not in elems_in_both_lists:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ece670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84fe2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_Anth = []\n",
    "join = ''\n",
    "for y in lst_Anth:\n",
    "    if y in elems_in_both_lists:\n",
    "        if join != '':\n",
    "            n_Anth.append(join)\n",
    "            join = ''\n",
    "        n_Anth.append(y)\n",
    "\n",
    "    if y not in elems_in_both_lists:\n",
    "        #store, join with next one and joined to n_list\n",
    "        join += ' ' # add space\n",
    "        join+=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57815d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       " 'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       " 'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       " 'Abstract\\n',\n",
       " ' Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmenta-\\ntion in conjunction with semantic similarity is ben-\\neﬁcial for this task and could be beneﬁcial for\\nthe other tasks that deal with short phrases. Our\\nbest performing model (Accuracy: 0.917, Rank:\\n1.156) was developed by ﬁne-tuning Sentence-\\nBERT [Reimers et al., 2019] (with FinBERT at the\\nbackend) over an extended labelled set created us-\\ning the hierarchy of labels present in FIBO.\\n',\n",
       " '1\\nIntroduction\\n',\n",
       " ' Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain, contains the\\nclearly deﬁned relationship, and organizes in a deﬁned struc-\\nture mostly as a hierarchy. These properties make ontologies\\na great source for getting a deeper understanding of the rela-\\ntionship and properties of resources from the domain in con-\\nsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       " '∗Contact Author\\n†Equal Contribution\\n',\n",
       " ' the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and concept search.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\\npetitions are being held to promote the development of effec-\\ntive similarity measures. In the third edition of the competi-\\ntion FinSim-32 (being held in conjunction with 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to assign hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym assignment. We ex-\\nperimented with basic featurization methods like TF-IDF and\\nadvanced methods like pre-trained embedding models. Our\\ntop 3 systems use pre-trained FinBERT [Araci, 2019] embed-\\nding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\\ncial domain . We also augmented the training data by utilizing\\nthe knowledge from DBpedia, Investopedia, FIBO and text\\ncorpus of prospectus shared with us. We describe the works\\nrelated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results from some of them. We draw our\\nconclusions in section 7 while giving a glimpse of things that\\nwe would like to try in the future.\\n',\n",
       " '2\\nRelated Works\\n',\n",
       " 'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       " '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       " ' 46\\n Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \\n(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \\n',\n",
       " 'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       " '3\\nProblem Statement\\n',\n",
       " ' Given a set F consisting of n tuples of ﬁnancial terms\\nand their hypernyms/top-level concepts/labels i.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\\nnym corresponding to the ith term ti and hiϵ set of labels men-\\ntioned in Table 1. For every unseen ﬁnancial term, our task is\\nto generate a ranked list ˆyi consisting of these 17 hypernyms\\nin order of decreasing semantic similarity.\\nEvaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       " 'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       " 'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       " '4\\nData\\n',\n",
       " '4.1\\nData Description\\n',\n",
       " ' The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels. More than 91% of the terms have 6 words or\\nless and the longest term has 22 words. There were 10 du-\\nplicate entries, and 3 terms were assigned 2 different labels.\\nAlong with this, a corpus of prospectuses in English was pro-\\nvided that had 211 documents. Some of the terms mentioned\\n',\n",
       " 'Label\\nCount\\n',\n",
       " 'Equity Index\\n280\\n',\n",
       " 'Regulatory Agency\\n205\\n',\n",
       " 'Credit Index\\n125\\n',\n",
       " 'Central Securities Depository\\n107\\n',\n",
       " 'Debt pricing and yields\\n58\\n',\n",
       " 'Bonds\\n55\\n',\n",
       " 'Swap\\n36\\n',\n",
       " 'Stock Corporation\\n25\\n',\n",
       " 'Option\\n24\\n',\n",
       " 'Funds\\n22\\n',\n",
       " 'Future\\n19\\n',\n",
       " 'Credit Events\\n18\\n',\n",
       " 'MMIs\\n17\\n',\n",
       " 'Stocks\\n17\\n',\n",
       " 'Parametric schedules\\n15\\n',\n",
       " 'Forward\\n9\\n',\n",
       " 'Securities restrictions\\n8\\n',\n",
       " 'Total\\n1040\\n',\n",
       " 'Table 1: Label distribution in the training set\\n',\n",
       " ' in the training data were present in the corpus. Table 1 shows\\nthe distribution of these labels in the training set.\\n 4.2\\nData Augmentation\\nSince the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       " 'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       " '• expansion had equal or less length than the acronym.\\n',\n",
       " '• expansion had parenthesis\\n',\n",
       " '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       " '• the expansion had less than or equal to 5 characters.\\n',\n",
       " 'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       " ' Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the\\n 3https://spacy.io/\\n4https://lookup.dbpedia.org/api/search\\n 47\\n train and test sets.\\nWe present such an example in Fig-\\nure 1. In addition to the description, the label was also re-\\ntained from the result payload to identify the right descrip-\\ntion for the input terms. We tried token overlap-based simi-\\nlarity of input terms with both matching labels and descrip-\\ntions. We decided to use the label to term match for descrip-\\ntion matching after going through a randomly drawn sam-\\nple. We cleaned both input terms and labels from DBpedia\\nresults by converting them to lower case, replacing punctua-\\ntions by space, removing repetitive spaces, and singularizing\\nthe text. We calculated the token overlap ratios for cleaned\\nterm and DBpedia labels using the formulas mentioned be-\\nlow: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\\nlength(s2/length(s1 where s1 and s2 represents sets of to-\\nkenized cleaned terms and tokenized and cleaned DBpedia\\nlabels respectively. We empirically decided to use all the in-\\nstances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\\na DBpedia label (and hence description) to the input term.\\n Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1801 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm ”callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n 5\\nSystem Description\\n We tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 841 and 209 terms respectively.\\n 5.1\\nSystem - 1 (S1)\\n This is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and stuck to the original set that\\nwas shared by organizers. We loaded FinBERT pre-trained\\n 5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n model and ﬁne-tuned it by trying to classify the representa-\\ntion of [CLS] token into one of the 17 labels mentioned pre-\\nviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n 5.2\\nSystem - 2 (S2)\\n This system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset has the descriptions of the terms,\\nthe input is considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n 5.3\\nSystem -3 (S3)\\n We explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n 48\\n',\n",
       " 'Figure 1: Sample output from DBpedia search API\\n',\n",
       " 'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       " 'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       " 'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       " 'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       " 'Bonds\\nDBpedia\\n',\n",
       " 'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       " 'Data Source\\nCount\\n',\n",
       " 'Original modelling data\\n1040\\n',\n",
       " 'DBpedia\\n257\\n',\n",
       " 'FIBO\\n236\\n',\n",
       " 'Investopedia\\n85\\n',\n",
       " 'Acronym expansion\\n218\\n',\n",
       " 'Table 3: Details of various data sources\\n',\n",
       " 'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       " '6\\nExperimentation and Results\\n',\n",
       " ' We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1469 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only change between these runs was the data,\\n the improvement can be attributed to the expanded data.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models are already\\ngiven in the system description. After rigorous experimenta-\\ntion, these hyperparameters were selected empirically based\\non validation set performance. The results are presented in\\nTable 4. Since the number of submissions was restricted to\\n3 for each team, we do not have the performance numbers of\\nthe BERT models in the test set. Analysing the results we see\\nthat SentenceBERT trained with FinBERT at the backed as\\nmentioned in section-5.3 performed the best.\\n 7\\nConclusion and Future Works\\nIn this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\nApart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\n 49\\n',\n",
       " 'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       " 'Validation set\\nTest set\\n',\n",
       " 'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       " 'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       " 'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       " 'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       " 'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       " 'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       " 'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       " 'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       " 'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca972db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\\nof Financial Terms\\n',\n",
       " 'Ankush Chopra∗† , Sohom Ghosh†\\n',\n",
       " 'Fidelity Investments, AI CoE, Bengaluru, India\\n{ankush01729, sohom1ghosh}@gmail.com\\n',\n",
       " 'Abstract\\n',\n",
       " 'Hypernym and synonym matching are one of the\\nmainstream Natural Language Processing (NLP)\\ntasks. In this paper, we present systems that at-\\ntempt to solve this problem.\\nWe designed these\\nsystems to participate in the FinSim-3, a shared\\ntask of FinNLP workshop at IJCAI-2021.\\nThe\\nshared task is focused on solving this problem for\\nthe ﬁnancial domain. We experimented with var-\\nious transformer based pre-trained embeddings by\\nﬁne-tuning these for either classiﬁcation or phrase\\nsimilarity tasks. We also augmented the provided\\ndataset with abbreviations derived from prospectus\\nprovided by the organizers and deﬁnitions of the\\nﬁnancial terms from DBpedia [Auer et al., 2007],\\nInvestopedia, and the Financial Industry Business\\nOntology (FIBO). Our best performing system uses\\nboth FinBERT [Araci, 2019] and data augmenta-\\ntion from the afore-mentioned sources.\\nWe ob-\\nserved that term expansion using data augmentation\\nin conjunction with semantic similarity is beneﬁcial\\nfor this task and could be useful for the other tasks\\nthat deal with short phrases. Our best performing\\nmodel (Accuracy: 0.917, Rank: 1.156) was devel-\\noped by ﬁne-tuning SentenceBERT [Reimers et al.,\\n2019] (with FinBERT at the backend) over an ex-\\ntended labelled set created using the hierarchy of\\nlabels present in FIBO.\\n',\n",
       " '1\\nIntroduction\\n',\n",
       " 'Ontologies are rich sources of information that provide deep\\ninformation about the underlying concepts and entities. This\\ninformation is described for a speciﬁc domain. It contains the\\nclearly deﬁned relationships, and it is organized in a deﬁned\\nstructure mostly as a hierarchy. These properties make on-\\ntologies a great source for getting a deeper understanding of\\nthe relationship and properties of resources of the domain in\\nconsideration.\\nPublic knowledge graphs and ontologies like DBpedia and\\nYago have been shown to work on various applications like\\n',\n",
       " '∗Contact Author\\n†Equal Contribution\\n',\n",
       " 'the ones described in [Kobilarov et al., 2009] and [Hahm et\\nal., 2014]. This has motivated and paved ways for the creation\\nof domain focused ontologies like FIBO1.\\nEffective techniques that enable identifying lexical similar-\\nity between the terms or concepts increase the effectiveness\\nof the ontologies. These methods not only help in building\\nnew ontologies faster or augment the existing ones, but also\\nit helps in the effective querying and searching of concepts.\\nFinSim [Maarouf et al., 2020; Mansar et al., 2021] compe-\\ntitions are being held to promote the development of effective\\nsimilarity measures. In the third edition of the competition\\nFinSim-32 (being held in conjunction with the 30th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\\nthe participants are challenged to develop methods and sys-\\ntems to rank hypernym and synonyms to ﬁnancial terms by\\nmapping them to one of the 17 high-level ﬁnancial concepts\\npresent in FIBO.\\nIn this paper, we present the systems developed by our\\nteam Lipi for hypernym and synonym ranking. We experi-\\nmented with basic featurization methods like TF-IDF and ad-\\nvanced methods like pre-trained embedding models. Our top\\n3 systems use pre-trained FinBERT [Araci, 2019] embedding\\nmodel that was ﬁne-tuned on the data speciﬁc to ﬁnancial do-\\nmain . We also augmented the training data by utilizing the\\nknowledge from DBpedia, Investopedia, FIBO and text cor-\\npus of prospectus shared with us. We describe the works re-\\nlated to our solution in the next section. Section 3 contains\\nthe formal problem statement, followed by data description\\nin section 4. We describe our top three systems in section 5.\\nSection 6 contains the details of the experimentation that we\\nperformed and the results obtained from some of them. We\\ndraw our conclusions in section 7 while giving a glimpse of\\nthings that we would like to try in the future.\\n',\n",
       " '2\\nRelated Works\\n',\n",
       " 'Hypernym-hyponym extraction and learning text similarity\\nusing semantic representations have been very challenging\\nareas of research for the NLP community. SemEval-2018\\nTask 9 [Camacho-Collados et al., 2018] was such an instance.\\n',\n",
       " '1https://spec.edmcouncil.org/ﬁbo/\\n2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\\ntask-ﬁnsim (accessed on 8th July 2021)\\n',\n",
       " 'arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\\n',\n",
       " 'Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\\nformed the best in this shared task. They combined a super-\\nvised word embedding based approach with an unsupervised\\npattern discovery based approach. The FinSim shared tasks\\n[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\\ning these challenges speciﬁc to the Financial Domain. Team\\nIIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\\nnation of context-free static embedding Word2Vec [Mikolov\\net al., 2013] and contextualized dynamic embedding BERT\\n[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\\nthe team FINSIM20 explored the use of cosine similarity be-\\ntween terms and labels encoded using Universal Sentence En-\\ncoder [Cer et al., 2018]. They also tried to extract hypernyms\\nautomatically using graph based approaches. Team PolyU-\\nCBS [Chersoni and Huang, 2021] won FinSim-2 shared\\ntask using Logistic Regression trained over word embedding\\nand probabilities derived from BERT [Devlin et al., 2019]\\nmodel. They also experimented with GPT-2 [Radford et al.,\\n2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\\ning Nguyen et al. performed better than the baseline by us-\\ning Sentence BERT [Reimers et al., 2019] to calculate co-\\nsine similarity between terms and hypernyms. [Saini, 2020;\\nPei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\\ncussed various techniques to enrich the data which was avail-\\nable for training. In this edition of FinSim, the number of\\ntraining samples and labels (ﬁnancial concepts) were more\\nthan the previous two editions.\\n',\n",
       " '3\\nProblem Statement\\n',\n",
       " 'Given a set F consisting of n tuples of ﬁnancial terms\\nand\\ntheir\\nhypernyms/top-level\\nconcepts/labels\\ni.e.\\nF\\n=\\n{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents\\nthe hypernym corresponding to the ith term ti and hiϵ set of\\nlabels mentioned in Table 1. For every unseen ﬁnancial term,\\nour task is to generate a ranked list ˆyi consisting of these 17\\nhypernyms in order of decreasing semantic similarity.\\n',\n",
       " 'Evaluation Metrics The expected output is a raked list of\\npredicted labels for every scored instance. The proposed sys-\\ntems are evaluated based on Accuracy and Mean Rank met-\\nrices as per the shared task rules. Evaluation script was pro-\\nvided by organizers, where accuracy and mean rank were de-\\nﬁned as:\\nAccuracy = 1\\n',\n",
       " 'n\\n�n\\ni=1 I(yi = ˆyi[1])\\nMeanRank = 1\\n',\n",
       " 'n\\n�n\\ni=1( ˆyi.index(yi))\\nwhere ˆyi is the ranked list (with index starting from 1) of pre-\\ndicted labels corresponding to the expected label yi. I is an\\nidentity matrix.\\n',\n",
       " '4\\nData\\n',\n",
       " '4.1\\nData Description\\n',\n",
       " 'The training dataset shared for this task has a total of\\n1050 single and multi-word terms tagged to 17 different\\nclasses/labels out of which 1040 term-label pairs are unique.\\nMore than 91% of the terms have 6 words or less and the\\nlongest term has 22 words. There were 10 duplicate entries,\\nand 3 terms were assigned 2 different labels. Along with this,\\n',\n",
       " 'Label\\nCount\\n',\n",
       " 'Equity Index\\n280\\n',\n",
       " 'Regulatory Agency\\n205\\n',\n",
       " 'Credit Index\\n125\\n',\n",
       " 'Central Securities Depository\\n107\\n',\n",
       " 'Debt pricing and yields\\n58\\n',\n",
       " 'Bonds\\n55\\n',\n",
       " 'Swap\\n36\\n',\n",
       " 'Stock Corporation\\n25\\n',\n",
       " 'Option\\n24\\n',\n",
       " 'Funds\\n22\\n',\n",
       " 'Future\\n19\\n',\n",
       " 'Credit Events\\n18\\n',\n",
       " 'MMIs\\n17\\n',\n",
       " 'Stocks\\n17\\n',\n",
       " 'Parametric schedules\\n15\\n',\n",
       " 'Forward\\n9\\n',\n",
       " 'Securities restrictions\\n8\\n',\n",
       " 'Total\\n1040\\n',\n",
       " 'Table 1: Label distribution in the training set\\n',\n",
       " 'a corpus of prospectuses in English that had 211 documents\\nwas provided. Some of the terms mentioned in the training\\ndata were present in the corpus. Table 1 shows the distribu-\\ntion of these labels in the training set.\\n',\n",
       " '4.2\\nData Augmentation\\n',\n",
       " 'Since the majority of the terms had only a few tokens, we\\ndecided to expand the terms wherever possible using various\\nsources.\\nThis approach had also been adopted by [Saini,\\n2020] and [Pei and Zhang, 2021] while participating in\\nFinSim-1 and FinSim-2 respectively.\\n',\n",
       " 'Acronym expansion: As mentioned by Keswani et al.\\n[Keswani et al., 2020], the presence of acronyms created a\\nmajor issue in maintaining consistency. We used the abbre-\\nviation extractor available in spaCy3[Honnibal et al., 2020]\\npackage on the corpus of the prospectus to extract all the\\nacronyms and their expansions. Upon manual inspection of\\na sample output, we identiﬁed that not all the extracted items\\nwere valid acronyms and their expansions. We cleaned the\\nextracted list by dropping the records where:\\n',\n",
       " '• expansion had equal or less length than the acronym.\\n',\n",
       " '• expansion had parenthesis\\n',\n",
       " '• extracted acronym was a valid English word such as\\n”fund” or ”Germany”.\\n',\n",
       " '• the expansion had less than or equal to 5 characters.\\n',\n",
       " 'We managed to extract 635 acronyms from the prospectus\\ncorpus after applying the above exclusions.\\nWe used this\\ndata to expand the matching terms in the given train set and\\ntest sets.\\n',\n",
       " '3https://spacy.io/\\n',\n",
       " 'Deﬁnitions from DBpedia: We used the DBpedia search\\nAPI4 to extract the description of the terms present in the train\\nand test sets. We present such an example in Figure 1. In ad-\\ndition to the description, the label was also retained from the\\nresult payload to identify the right description for the input\\nterms. We tried token overlap-based similarity of input terms\\nwith both matching labels and descriptions. We decided to\\nuse the label to term match for description matching after go-\\ning through a randomly drawn sample. We cleaned both input\\nterms and labels from DBpedia results by converting them to\\nlower case, replacing punctuations by space, removing repet-\\nitive spaces, and singularizing the text. We calculated the\\ntoken overlap ratios for cleaned term and DBpedia labels us-\\ning these formulas: Ratio1 = length(s1 ∩ s2)/length(s1),\\nRatio2 = length(s2)/length(s1) where s1 and s2 represents\\nsets of tokenized cleaned terms and tokenized cleaned DBpe-\\ndia labels respectively. We empirically decided to use all the\\ninstances with Ratio1 = 1 and Ratio2 <= 1.25 for match-\\ning a DBpedia label (and hence description) to the input term.\\n',\n",
       " 'Deﬁnitions from Investopedia and FIBO: Inspired by\\n[Saini, 2020], we obtained deﬁnitions of the terms present\\nin Investopedia’s data dictionary5 by crawling it. We down-\\nloaded a glossary of ﬁnancial terms from the website of\\nFIBO. We cleaned all the terms from the train and test set and\\nalso the terms present in Investopedia’s data dictionary using\\nthe steps described in the above DBpedia section. We then as-\\nsigned the Investopedia or FIBO deﬁnition to the terms from\\nthe train and test sets where cleaned terms from train and test\\ndata matched to cleaned Investopedia terms perfectly.\\nThe test set which was provided to us had 326 terms. We\\naugmented the original train and test set with the records\\nwhere we could either ﬁnd deﬁnition or expansion using the\\nabove sources. The train set size increased to 1836 records\\nand the test set size increased to 607 after the data augmen-\\ntation. We present an example of data augmentation for the\\nterm “callable bond” in Table 2. Table 3 states the number\\nof instances we used from each of the sources to augment the\\ndata we had.\\n',\n",
       " '5\\nSystem Description\\nWe tried to solve this problem as the term classiﬁcation and\\nterm similarity problems. Two of our 3 submissions are mod-\\nelled as the term classiﬁcation problem, whereas the third sys-\\ntem is designed to be a phrase/sentence similarity problem be-\\ntween terms (or expanded terms from the augmented dataset)\\nand the deﬁnitions of 17 class labels that were extracted from\\nFIBO / Internet. All the systems rely on semantic similarity\\nand use FinBERT model to generate the term or token embed-\\nding representations. We divided the given data into training\\nand validation sets having 832 and 208 terms respectively.\\n',\n",
       " '5.1\\nSystem - 1 (S1)\\nThis is the simplest of our proposed systems, where we did\\nnot use the augmented dataset and used only the original set\\n',\n",
       " '4https://lookup.dbpedia.org/api/search\\n5https://www.investopedia.com/ﬁnancial-term-dictionary-\\n4769738\\n',\n",
       " 'that was shared by organizers.\\nWe loaded FinBERT pre-\\ntrained model and ﬁne-tuned it by trying to classify the repre-\\nsentation of [CLS] token into one of the 17 labels mentioned\\npreviously. Since the original data did not have longer terms,\\nwe kept the maximum length to 32, and train and validation\\nbatch sizes of 64. We used Adam optimizer with a learning\\nrate of 0.00002. We ran the model for 40 epochs and picked\\nthe model saved after 18th epoch based on the performance on\\nthe validation set. Finally, we ranked the predictions based on\\nthe predicted probability of each class.\\n',\n",
       " '5.2\\nSystem - 2 (S2)\\nThis system is similar to System-1 with the only difference\\nthat data being the augmented set and not the original dataset.\\nSince the augmented dataset had the descriptions of the terms,\\nthe inputs were considerably longer. Hence, we increased the\\nmaximum length to 256 while keeping all the other hyper-\\nparameters the same. After, training the model for 40 epochs\\nwe selected the model saved after the 17th epoch as the best\\nmodel based on validation set performance.\\n',\n",
       " '5.3\\nSystem -3 (S3)\\nWe explored the FIBO ontology to understand the hierarchy\\n[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\\nin Figure 2. We used the augmented data described in sec-\\ntion 4.2 to create a labelled dataset having similarity scores.\\nFor every term deﬁnition (T) to label deﬁnition (L) mapping\\nwhich existed in the extended training set, we assigned a sim-\\nilarity score of 1.0 to the (T,L) pair and picked up 10 train-\\ning instances randomly ensuring none of their label deﬁnition\\nwas same as L. For each of the label deﬁnitions (LL) present\\nin this sample, we extracted its root node and ﬁrst child node.\\nWe did the same for the original label deﬁnition (L). Then, we\\ncompared these nodes. If the root node and ﬁrst child node of\\nL were different from that of LL then we assigned a similar-\\nity score of 0 to the (T, LL) pair. If the root nodes were the\\nsame, we assigned a similarity score of ’k’ when the ﬁrst child\\nnodes differed and a similarity score of ’2k’ when they were\\nthe same (where 0 < k < 1). We empirically ﬁgured out that\\nk=0.4 works the best. As expected, the number of instances\\nwith a similarity score equal to 0 increased substantially. We\\nunder-sampled such instances and the new training set had\\n30% instances with similarity score 1.0, 12% instances with\\nsimilarity score ’k’, 28% instances with similarity score ’2k’\\nand 30% instances with similarity score 0. After that, we ﬁne-\\ntuned a FinBERT [Araci, 2019] model using Sentence BERT\\n[Reimers et al., 2019] framework with this newly generated\\nlabelled data for 25 epochs with a batch size of 20. Our ob-\\njective was to minimize the multiple negatives ranking loss\\nand online contrastive loss. We used a margin of 0.5 and co-\\nsine distance as a distance metric while training this model.\\nFinally, we converted all of the 17 labels’ deﬁnitions and term\\ndeﬁnitions from the validation set to vectors using this ﬁne-\\ntuned model. For every such term deﬁnition, we performed\\na semantic search over the label vectors and ranked them in\\ndecreasing order of cosine similarity.\\nSystem 2 and 3 take advantage of term expansion during both\\nmodel training and scoring phases, which causes certain ob-\\nservations to appear more than once (reference: Table 3). We\\n',\n",
       " 'Figure 1: Sample output from DBpedia search API\\n',\n",
       " 'Expanded Term/Term Deﬁnition\\nLabel\\nSource\\n',\n",
       " 'Callable bond\\nBonds\\noriginal and\\nacronym expansion\\n',\n",
       " 'bond that includes a stipulation allowing the issuer\\nthe right to repurchase and retire the bond at the call price after the call protection period\\nBonds\\nFIBO\\n',\n",
       " 'A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\\nthe issuer of the bond to retain the privilege of redeeming the bond at some point before\\nthe bond reaches its date of maturity.\\n',\n",
       " 'Bonds\\nDBpedia\\n',\n",
       " 'Table 2: Result of Data Augmentation of the term ”Callable bond”\\n',\n",
       " 'Data Source\\nCount\\n',\n",
       " 'Original modelling data\\n1040\\n',\n",
       " 'DBpedia\\n257\\n',\n",
       " 'FIBO\\n236\\n',\n",
       " 'Investopedia\\n85\\n',\n",
       " 'Acronym expansion\\n218\\n',\n",
       " 'Table 3: Details of various data sources\\n',\n",
       " 'derive the ﬁnal prediction by averaging the output probabili-\\nties for all the 17 classes for all the occurrences of the term.\\n',\n",
       " '6\\nExperimentation and Results\\n',\n",
       " 'We had 1040 observations after removing the duplicates. We\\ndid an 80:20 split to create a training and validation set from\\nthis. We augmented the given modelling set by incorporating\\ndeﬁnitions from DBpedia, FIBO and Investopedia. We used\\nthe list of acronyms extracted from the prospectus corpus to\\ncreate a copy with acronym expansion. This helped us to in-\\ncrease the original data to 1836 records (mentioned in Table\\n1). It should be noted that we could not ﬁnd the expansions\\nfor all the terms given in the modelling set. Train and valida-\\ntion set sizes for the original modelling set and expanded data\\nwere (832 & 208) and (1470 & 366) respectively.\\nWe established a baseline by running the scripts provided\\nby the organizers. Then, we considered original modelling\\ndata and ﬁne-tuned base BERT-cased model [Devlin et al.,\\n2019] to predict the class label by taking the representa-\\ntion of [CLS] token while passing it through few layers of\\na feed-forward network. This performed better than base-\\nline. We then tried the same BERT-base model on the ex-\\npanded dataset, which gave us further performance improve-\\nment. Since the only major change between these runs was\\n',\n",
       " 'the data, the improvement can be attributed to the expanded\\ndata.\\nWe experimented with a few of the other pre-trained mod-\\nels that are available on the Huggingface model repository\\n[Wolf et al., 2020]. We observed clear improvement when\\nwe used the FinBERT model which was trained on data spe-\\nciﬁc to the ﬁnancial domain. The model performance succes-\\nsively increased when we used a combination of data expan-\\nsion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\\nBERT using Sentence Transformers [Reimers et al., 2019] to\\ncapture semantic textual similarity. For this, we used several\\ncombinations of term and term deﬁnitions with label and la-\\nbel deﬁnitions.\\nAll the hyperparameters for the ﬁnal 3 models have been\\nalready mentioned in the system description. After rigorous\\nexperimentation, these hyperparameters were selected empir-\\nically based on validation set performance. The results are\\npresented in Table 4. Since the number of submissions was\\nrestricted to 3 for each team, we do not have the performance\\nnumbers of the BERT models in the test set. Analysing the\\nresults we see that SentenceBERT trained with FinBERT at\\nthe backed as mentioned in section-5.3 performed the best.\\n',\n",
       " '7\\nConclusion and Future Works\\n',\n",
       " 'In this work, we attempted to solve the hypernym and syn-\\nonym discovery hosted at FinSim-3. This challenge aimed\\nto enable the better use of ontologies like FIBO using hy-\\npernyms and synonyms, and we used these ontologies them-\\nselves to develop our systems which perform signiﬁcantly\\nbetter than the provided baseline systems. This proves the\\npresent use of these ontologies. The presented solution is\\nrecursive in a sense as it uses knowledge from ontologies\\nto further increase the effectiveness and use of the same.\\n',\n",
       " 'Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\\n',\n",
       " 'Validation set\\nTest set\\n',\n",
       " 'Model\\nData\\nRank\\nAcc.\\nRank\\nAcc.\\n',\n",
       " 'Base-1\\nOrg.\\n2.158\\n0.498\\n1.941\\n0.564\\n',\n",
       " 'Base-2\\nOrg.\\n1.201\\n0.876\\n1.75\\n0.669\\n',\n",
       " 'BERT\\nOrg.\\n1.177\\n0.899\\n-\\n-\\n',\n",
       " 'BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n',\n",
       " 'FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n',\n",
       " 'FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n',\n",
       " 'SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n',\n",
       " 'Table 4: Results on validation and test set. Org. represents original\\nand Ext. represents extended. Base refers to baseline.\\n',\n",
       " 'Apart from data augmentation, our solution relies upon se-\\nmantic similarity learnt from pre-trained embedding models\\nthat were learnt on the relevant domain. We observed the\\nclear beneﬁts of domain speciﬁc pretraining during the ex-\\nperimentation.\\nIn future, we would like to explore Knowledge Graphs (as\\ndescribed in [Portisch et al., 2021]) to further improve the\\nimprove performance of the models. We also want to ex-\\nplore other variants of FinBERT [Araci, 2019] and ﬁne-tune\\nthem using the Masked Language Modeling technique (as\\nmentioned by the winner of FinSim-2 [Chersoni and Huang,\\n2021]) and Next Sentence Prediction objective. Moreover,\\nthis research can be extended by extracting sentences present\\nin the prospectus (similar to [Goel et al., 2021]) to create\\nmore positive and negative samples.\\n',\n",
       " 'References\\n',\n",
       " '[Anand et al., 2020] Vivek Anand, Yash Agrawal, Aarti Pol,\\nand Vasudeva Varma. FINSIM20 at the FinSim task: Mak-\\ning sense of text in ﬁnancial domain. In Proceedings of\\nthe Second Workshop on Financial Technology and Natu-\\nral Language Processing, pages 104–107, Kyoto, Japan, 5\\nJanuary 2020.\\n',\n",
       " '[Araci, 2019] Dogu Araci.\\nFinbert:\\nFinancial sentiment\\nanalysis with pre-trained language models, 2019.\\n',\n",
       " '[Auer et al., 2007] S¨oren Auer, Christian Bizer, Georgi Ko-\\nbilarov, Jens Lehmann, Richard Cyganiak, and Zachary\\nIves. Dbpedia: A nucleus for a web of open data, 2007.\\n[Bernier-Colborne and Barri`ere, 2018] Gabriel\\nBernier-\\nColborne and Caroline Barri`ere. CRIM at SemEval-2018\\ntask 9: A hybrid approach to hypernym discovery. In Pro-\\nceedings of The 12th International Workshop on Semantic\\nEvaluation, pages 725–731, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics.\\n[Camacho-Collados et al., 2018] Jose\\nCamacho-Collados,\\nClaudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas,\\nTommaso Pasini, Enrico Santus, Vered Shwartz, Roberto\\nNavigli, and Horacio Saggion.\\nSemEval-2018 task 9:\\nHypernym discovery. In Proceedings of The 12th Interna-\\ntional Workshop on Semantic Evaluation, pages 712–724,\\nNew Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[Cer et al., 2018] Daniel Cer, Yinfei Yang, Sheng yi Kong,\\nNan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-\\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Uni-\\nversal sentence encoder, 2018.\\n[Chersoni and Huang, 2021] Emmanuele Chersoni and Chu-\\nRen Huang. PolyU-CBS at the FinSim-2 Task: Combin-\\ning Distributional, String-Based and Transformers-Based\\nFeatures for Hypernymy Detection in the Financial Do-\\nmain, page 316–319. Association for Computing Machin-\\nery, New York, NY, USA, 2021.\\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n[Goel et al., 2021] Tushar Goel,\\nVipul Chauhan,\\nIshan\\nVerma, Tirthankar Dasgupta, and Lipika Dey. TCS WITM\\n',\n",
       " '2021 @FinSim-2: Transformer Based Models for Auto-\\nmatic Classiﬁcation of Financial Terms, page 311–315.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n[Hahm et al., 2014] Younggyun\\nHahm,\\nJungyeul\\nPark,\\nKyungtae Lim, Youngsik Kim, Dosam Hwang, and\\nKey-Sun Choi.\\nNamed entity corpus construction us-\\ning wikipedia and dbpedia ontology.\\nIn LREC, pages\\n2565–2569, 2014.\\n[Honnibal et al., 2020] Matthew Honnibal, Ines Montani,\\nSoﬁe Van Landeghem, and Adriane Boyd.\\nspaCy:\\nIndustrial-strength\\nNatural\\nLanguage\\nProcessing\\nin\\nPython, 2020.\\n[Jurgens and Pilehvar, 2016] David Jurgens and Moham-\\nmad Taher Pilehvar.\\nSemEval-2016 task 14: Semantic\\ntaxonomy enrichment. In Proceedings of the 10th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-2016),\\npages 1092–1102, San Diego, California, June 2016. As-\\nsociation for Computational Linguistics.\\n[Keswani et al., 2020] Vishal Keswani, Sakshi Singh, and\\nAshutosh Modi. IITK at the FinSim task: Hypernym de-\\ntection in ﬁnancial domain via context-free and contextu-\\nalized word embeddings. In Proceedings of the Second\\nWorkshop on Financial Technology and Natural Language\\nProcessing, pages 87–92, Kyoto, Japan, 5 January 2020.\\n[Kobilarov et al., 2009] Georgi Kobilarov, Tom Scott, Yves\\nRaimond,\\nSilver\\nOliver,\\nChris\\nSizemore,\\nMichael\\nSmethurst, Christian Bizer, and Robert Lee.\\nMedia\\nmeets semantic web – how the bbc uses dbpedia and\\nlinked data to make connections. In Lora Aroyo, Paolo\\nTraverso, Fabio Ciravegna, Philipp Cimiano, Tom Heath,\\nEero Hyv¨onen, Riichiro Mizoguchi, Eyal Oren, Marta\\nSabou, and Elena Simperl, editors, The Semantic Web: Re-\\nsearch and Applications, pages 723–737, Berlin, Heidel-\\nberg, 2009. Springer Berlin Heidelberg.\\n[Maarouf et al., 2020] Ismail El Maarouf, Youness Mansar,\\nVirginie Mouilleron, and Dialekti Valsamou-Stanislawski.\\nThe FinSim 2020 shared task: Learning semantic repre-\\nsentations for the ﬁnancial domain. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 81–86, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Mansar et al., 2021] Youness Mansar, Juyeon Kang, and Is-\\nmail El Maarouf. The FinSim-2 2021 Shared Task: Learn-\\ning Semantic Similarities for the Financial Domain, page\\n288–292.\\nAssociation for Computing Machinery, New\\nYork, NY, USA, 2021.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\\nresentations in vector space, 2013.\\n[Nguyen et al., 2021] Nhu Khoa Nguyen, Emanuela Boros,\\nGael Lejeune, Antoine Doucet, and Thierry Delahaut. L3i\\nLBPAM at the FinSim-2 Task: Learning Financial Seman-\\ntic Similarities with Siamese Transformers, page 302–306.\\nAssociation for Computing Machinery, New York, NY,\\nUSA, 2021.\\n',\n",
       " '[Pei and Zhang, 2021] Yulong Pei and Qian Zhang. Goat at\\nthe ﬁnsim-2 task: Learning word representations of ﬁnan-\\ncial data with customized corpus.\\nIn Companion Pro-\\nceedings of the Web Conference 2021, WWW ’21, page\\n307–310, New York, NY, USA, 2021. Association for\\nComputing Machinery.\\n[Portisch et al., 2021] Jan Portisch, Michael Hladik, and\\nHeiko Paulheim. FinMatcher at FinSim-2: Hypernym De-\\ntection in the Financial Services Domain Using Knowl-\\nedge Graphs, page 293–297. Association for Computing\\nMachinery, New York, NY, USA, 2021.\\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners, 2019.\\n[Reimers et al., 2019] Nils Reimers, Iryna Gurevych, Nils\\nReimers, Iryna Gurevych, Nandan Thakur, Nils Reimers,\\nJohannes Daxenberger, and Iryna Gurevych.\\nSentence-\\nbert: Sentence embeddings using siamese bert-networks.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing. Association for\\nComputational Linguistics, 2019.\\n[Saini, 2020] Anuj Saini.\\nAnuj at the FinSim task:\\nAnuj@FINSIM¡VLearning semantic representation of ﬁ-\\nnancial domain with investopedia. In Proceedings of the\\nSecond Workshop on Financial Technology and Natural\\nLanguage Processing, pages 93–97, Kyoto, Japan, 5 Jan-\\nuary 2020.\\n[Stepiˇsnik Perdih et al., 2021] Timen\\nStepiˇsnik\\nPerdih,\\nSenja Pollak, and Blaˇz ˇSkrlj. JSI at the FinSim-2 Task:\\nOntology-Augmented Financial Concept Classiﬁcation,\\npage 298–301.\\nAssociation for Computing Machinery,\\nNew York, NY, USA, 2021.\\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\\nSanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\\ntransformers: State-of-the-art natural language processing,\\n2020.\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadcbedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 109)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_Anth), len(lst_Anth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0fdd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_Arxiv = []\n",
    "join = ''\n",
    "for y in lst_Arxiv:\n",
    "    if y in elems_in_both_lists:\n",
    "        if join != '':\n",
    "            n_Arxiv.append(join)\n",
    "            join = ''\n",
    "        n_Arxiv.append(y)\n",
    "\n",
    "    if y not in elems_in_both_lists:\n",
    "        #store, join with next one and joined to n_list\n",
    "        join += ' ' # add space\n",
    "        join+=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983249a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 77)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_Arxiv), len(n_Anth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41876163",
   "metadata": {},
   "source": [
    "Quite good method, gets most headings. Not all e.g. 7 conclusion and future works. Might perform better or worse for other papers. \n",
    "* Now needs cleaning: /n and -/n. \n",
    "* Numbers -> keep them i think. \n",
    "* Keep all parts that are the same? yeah. \n",
    "* vectorize parts \n",
    "* remove all parts that contain the wordt arxiv (first check no part of anthology contains arxiv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cadad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dd35991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(n_Anth, n_Arxiv).reset_index()\n",
    "df.columns =['Anth', 'Arxiv' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59d6ffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Anth', 'Arxiv'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8b6d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4: Results on validation and test set. Org. represents original\n",
      "and Ext. represents extended. Base refers to baseline.\n",
      "\n",
      "Table 4: Results on validation and test set. Org. represents original\n",
      "and Ext. represents extended. Base refers to baseline.\n",
      "\n",
      "Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\n",
      "of Financial Terms\n",
      "\n",
      "Term Expansion and FinBERT ﬁne-tuning for Hypernym and Synonym Ranking\n",
      "of Financial Terms\n",
      "\n",
      "Ankush Chopra∗† , Sohom Ghosh†\n",
      "\n",
      "Ankush Chopra∗† , Sohom Ghosh†\n",
      "\n",
      "Fidelity Investments, AI CoE, Bengaluru, India\n",
      "{ankush01729, sohom1ghosh}@gmail.com\n",
      "\n",
      "Fidelity Investments, AI CoE, Bengaluru, India\n",
      "{ankush01729, sohom1ghosh}@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "Abstract\n",
      "\n",
      " Hypernym and synonym matching are one of the\n",
      "mainstream Natural Language Processing (NLP)\n",
      "tasks. In this paper, we present systems that at-\n",
      "tempt to solve this problem.\n",
      "We designed these\n",
      "systems to participate in the FinSim-3, a shared\n",
      "task of FinNLP workshop at IJCAI-2021.\n",
      "The\n",
      "shared task is focused on solving this problem for\n",
      "the ﬁnancial domain. We experimented with var-\n",
      "ious transformer based pre-trained embeddings by\n",
      "ﬁne-tuning these for either classiﬁcation or phrase\n",
      "similarity tasks. We also augmented the provided\n",
      "dataset with abbreviations derived from prospectus\n",
      "provided by the organizers and deﬁnitions of the\n",
      "ﬁnancial terms from DBpedia [Auer et al., 2007],\n",
      "Investopedia, and the Financial Industry Business\n",
      "Ontology (FIBO). Our best performing system uses\n",
      "both FinBERT [Araci, 2019] and data augmenta-\n",
      "tion from the afore-mentioned sources.\n",
      "We ob-\n",
      "served that term expansion using data augmentation\n",
      "in conjunction with semantic similarity is beneﬁcial\n",
      "for this task and could be useful for the other tasks\n",
      "that deal with short phrases. Our best performing\n",
      "model (Accuracy: 0.917, Rank: 1.156) was devel-\n",
      "oped by ﬁne-tuning SentenceBERT [Reimers et al.,\n",
      "2019] (with FinBERT at the backend) over an ex-\n",
      "tended labelled set created using the hierarchy of\n",
      "labels present in FIBO.\n",
      "\n",
      " Hypernym and synonym matching are one of the\n",
      "mainstream Natural Language Processing (NLP)\n",
      "tasks. In this paper, we present systems that at-\n",
      "tempt to solve this problem.\n",
      "We designed these\n",
      "systems to participate in the FinSim-3, a shared\n",
      "task of FinNLP workshop at IJCAI-2021.\n",
      "The\n",
      "shared task is focused on solving this problem for\n",
      "the ﬁnancial domain. We experimented with var-\n",
      "ious transformer based pre-trained embeddings by\n",
      "ﬁne-tuning these for either classiﬁcation or phrase\n",
      "similarity tasks. We also augmented the provided\n",
      "dataset with abbreviations derived from prospectus\n",
      "provided by the organizers and deﬁnitions of the\n",
      "ﬁnancial terms from DBpedia [Auer et al., 2007],\n",
      "Investopedia, and the Financial Industry Business\n",
      "Ontology (FIBO). Our best performing system uses\n",
      "both FinBERT [Araci, 2019] and data augmenta-\n",
      "tion from the afore-mentioned sources.\n",
      "We ob-\n",
      "served that term expansion using data augmenta-\n",
      "tion in conjunction with semantic similarity is ben-\n",
      "eﬁcial for this task and could be beneﬁcial for\n",
      "the other tasks that deal with short phrases. Our\n",
      "best performing model (Accuracy: 0.917, Rank:\n",
      "1.156) was developed by ﬁne-tuning Sentence-\n",
      "BERT [Reimers et al., 2019] (with FinBERT at the\n",
      "backend) over an extended labelled set created us-\n",
      "ing the hierarchy of labels present in FIBO.\n",
      "\n",
      "1\n",
      "Introduction\n",
      "\n",
      "1\n",
      "Introduction\n",
      "\n",
      " Ontologies are rich sources of information that provide deep\n",
      "information about the underlying concepts and entities. This\n",
      "information is described for a speciﬁc domain. It contains the\n",
      "clearly deﬁned relationships, and it is organized in a deﬁned\n",
      "structure mostly as a hierarchy. These properties make on-\n",
      "tologies a great source for getting a deeper understanding of\n",
      "the relationship and properties of resources of the domain in\n",
      "consideration.\n",
      "Public knowledge graphs and ontologies like DBpedia and\n",
      "Yago have been shown to work on various applications like\n",
      "\n",
      " Ontologies are rich sources of information that provide deep\n",
      "information about the underlying concepts and entities. This\n",
      "information is described for a speciﬁc domain, contains the\n",
      "clearly deﬁned relationship, and organizes in a deﬁned struc-\n",
      "ture mostly as a hierarchy. These properties make ontologies\n",
      "a great source for getting a deeper understanding of the rela-\n",
      "tionship and properties of resources from the domain in con-\n",
      "sideration.\n",
      "Public knowledge graphs and ontologies like DBpedia and\n",
      "Yago have been shown to work on various applications like\n",
      "\n",
      "∗Contact Author\n",
      "†Equal Contribution\n",
      "\n",
      "∗Contact Author\n",
      "†Equal Contribution\n",
      "\n",
      " the ones described in [Kobilarov et al., 2009] and [Hahm et\n",
      "al., 2014]. This has motivated and paved ways for the creation\n",
      "of domain focused ontologies like FIBO1.\n",
      "Effective techniques that enable identifying lexical similar-\n",
      "ity between the terms or concepts increase the effectiveness\n",
      "of the ontologies. These methods not only help in building\n",
      "new ontologies faster or augment the existing ones, but also\n",
      "it helps in the effective querying and searching of concepts.\n",
      "FinSim [Maarouf et al., 2020; Mansar et al., 2021] compe-\n",
      "titions are being held to promote the development of effective\n",
      "similarity measures. In the third edition of the competition\n",
      "FinSim-32 (being held in conjunction with the 30th Interna-\n",
      "tional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\n",
      "the participants are challenged to develop methods and sys-\n",
      "tems to rank hypernym and synonyms to ﬁnancial terms by\n",
      "mapping them to one of the 17 high-level ﬁnancial concepts\n",
      "present in FIBO.\n",
      "In this paper, we present the systems developed by our\n",
      "team Lipi for hypernym and synonym ranking. We experi-\n",
      "mented with basic featurization methods like TF-IDF and ad-\n",
      "vanced methods like pre-trained embedding models. Our top\n",
      "3 systems use pre-trained FinBERT [Araci, 2019] embedding\n",
      "model that was ﬁne-tuned on the data speciﬁc to ﬁnancial do-\n",
      "main . We also augmented the training data by utilizing the\n",
      "knowledge from DBpedia, Investopedia, FIBO and text cor-\n",
      "pus of prospectus shared with us. We describe the works re-\n",
      "lated to our solution in the next section. Section 3 contains\n",
      "the formal problem statement, followed by data description\n",
      "in section 4. We describe our top three systems in section 5.\n",
      "Section 6 contains the details of the experimentation that we\n",
      "performed and the results obtained from some of them. We\n",
      "draw our conclusions in section 7 while giving a glimpse of\n",
      "things that we would like to try in the future.\n",
      "\n",
      " the ones described in [Kobilarov et al., 2009] and [Hahm et\n",
      "al., 2014]. This has motivated and paved ways for the creation\n",
      "of domain focused ontologies like FIBO1.\n",
      "Effective techniques that enable identifying lexical similar-\n",
      "ity between the terms or concepts increase the effectiveness\n",
      "of the ontologies. These methods not only help in building\n",
      "new ontologies faster or augment the existing ones, but also\n",
      "it helps in the effective querying and concept search.\n",
      "FinSim [Maarouf et al., 2020; Mansar et al., 2021] com-\n",
      "petitions are being held to promote the development of effec-\n",
      "tive similarity measures. In the third edition of the competi-\n",
      "tion FinSim-32 (being held in conjunction with 30th Interna-\n",
      "tional Joint Conference on Artiﬁcial Intelligence (IJCAI-21)),\n",
      "the participants are challenged to develop methods and sys-\n",
      "tems to assign hypernym and synonyms to ﬁnancial terms by\n",
      "mapping them to one of the 17 high-level ﬁnancial concepts\n",
      "present in FIBO.\n",
      "In this paper, we present the systems developed by our\n",
      "team Lipi for hypernym and synonym assignment. We ex-\n",
      "perimented with basic featurization methods like TF-IDF and\n",
      "advanced methods like pre-trained embedding models. Our\n",
      "top 3 systems use pre-trained FinBERT [Araci, 2019] embed-\n",
      "ding model that was ﬁne-tuned on the data speciﬁc to ﬁnan-\n",
      "cial domain . We also augmented the training data by utilizing\n",
      "the knowledge from DBpedia, Investopedia, FIBO and text\n",
      "corpus of prospectus shared with us. We describe the works\n",
      "related to our solution in the next section. Section 3 contains\n",
      "the formal problem statement, followed by data description\n",
      "in section 4. We describe our top three systems in section 5.\n",
      "Section 6 contains the details of the experimentation that we\n",
      "performed and the results from some of them. We draw our\n",
      "conclusions in section 7 while giving a glimpse of things that\n",
      "we would like to try in the future.\n",
      "\n",
      "2\n",
      "Related Works\n",
      "\n",
      "2\n",
      "Related Works\n",
      "\n",
      "Hypernym-hyponym extraction and learning text similarity\n",
      "using semantic representations have been very challenging\n",
      "areas of research for the NLP community. SemEval-2018\n",
      "Task 9 [Camacho-Collados et al., 2018] was such an instance.\n",
      "\n",
      "Hypernym-hyponym extraction and learning text similarity\n",
      "using semantic representations have been very challenging\n",
      "areas of research for the NLP community. SemEval-2018\n",
      "Task 9 [Camacho-Collados et al., 2018] was such an instance.\n",
      "\n",
      "1https://spec.edmcouncil.org/ﬁbo/\n",
      "2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\n",
      "task-ﬁnsim (accessed on 8th July 2021)\n",
      "\n",
      "1https://spec.edmcouncil.org/ﬁbo/\n",
      "2https://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp2021/shared-\n",
      "task-ﬁnsim (accessed on 8th July 2021)\n",
      "\n",
      " arXiv:2107.13764v1  [cs.CL]  29 Jul 2021\n",
      "\n",
      " 46\n",
      " Proceedings of the Third Workshop on Financial Technology and Natural Language Processing \n",
      "(FinNLP@IJCAI 2021), pages 46-51, Online, August 19, 2021.     \n",
      "\n",
      "Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\n",
      "formed the best in this shared task. They combined a super-\n",
      "vised word embedding based approach with an unsupervised\n",
      "pattern discovery based approach. The FinSim shared tasks\n",
      "[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\n",
      "ing these challenges speciﬁc to the Financial Domain. Team\n",
      "IIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\n",
      "nation of context-free static embedding Word2Vec [Mikolov\n",
      "et al., 2013] and contextualized dynamic embedding BERT\n",
      "[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\n",
      "the team FINSIM20 explored the use of cosine similarity be-\n",
      "tween terms and labels encoded using Universal Sentence En-\n",
      "coder [Cer et al., 2018]. They also tried to extract hypernyms\n",
      "automatically using graph based approaches. Team PolyU-\n",
      "CBS [Chersoni and Huang, 2021] won FinSim-2 shared\n",
      "task using Logistic Regression trained over word embedding\n",
      "and probabilities derived from BERT [Devlin et al., 2019]\n",
      "model. They also experimented with GPT-2 [Radford et al.,\n",
      "2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\n",
      "ing Nguyen et al. performed better than the baseline by us-\n",
      "ing Sentence BERT [Reimers et al., 2019] to calculate co-\n",
      "sine similarity between terms and hypernyms. [Saini, 2020;\n",
      "Pei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\n",
      "cussed various techniques to enrich the data which was avail-\n",
      "able for training. In this edition of FinSim, the number of\n",
      "training samples and labels (ﬁnancial concepts) were more\n",
      "than the previous two editions.\n",
      "\n",
      "Team CRIM [Bernier-Colborne and Barri`ere, 2018] per-\n",
      "formed the best in this shared task. They combined a super-\n",
      "vised word embedding based approach with an unsupervised\n",
      "pattern discovery based approach. The FinSim shared tasks\n",
      "[Maarouf et al., 2020; Mansar et al., 2021] deal with adopt-\n",
      "ing these challenges speciﬁc to the Financial Domain. Team\n",
      "IIT-K [Keswani et al., 2020] won FinSim-1 using a combi-\n",
      "nation of context-free static embedding Word2Vec [Mikolov\n",
      "et al., 2013] and contextualized dynamic embedding BERT\n",
      "[Devlin et al., 2019]. Anand et al. [Anand et al., 2020] from\n",
      "the team FINSIM20 explored the use of cosine similarity be-\n",
      "tween terms and labels encoded using Universal Sentence En-\n",
      "coder [Cer et al., 2018]. They also tried to extract hypernyms\n",
      "automatically using graph based approaches. Team PolyU-\n",
      "CBS [Chersoni and Huang, 2021] won FinSim-2 shared\n",
      "task using Logistic Regression trained over word embedding\n",
      "and probabilities derived from BERT [Devlin et al., 2019]\n",
      "model. They also experimented with GPT-2 [Radford et al.,\n",
      "2019]. Team L3i-LBPAM [Nguyen et al., 2021] compris-\n",
      "ing Nguyen et al. performed better than the baseline by us-\n",
      "ing Sentence BERT [Reimers et al., 2019] to calculate co-\n",
      "sine similarity between terms and hypernyms. [Saini, 2020;\n",
      "Pei and Zhang, 2021] and [Jurgens and Pilehvar, 2016] dis-\n",
      "cussed various techniques to enrich the data which was avail-\n",
      "able for training. In this edition of FinSim, the number of\n",
      "training samples and labels (ﬁnancial concepts) were more\n",
      "than the previous two editions.\n",
      "\n",
      "3\n",
      "Problem Statement\n",
      "\n",
      "3\n",
      "Problem Statement\n",
      "\n",
      " Given a set F consisting of n tuples of ﬁnancial terms\n",
      "and\n",
      "their\n",
      "hypernyms/top-level\n",
      "concepts/labels\n",
      "i.e.\n",
      "F\n",
      "=\n",
      "{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents\n",
      "the hypernym corresponding to the ith term ti and hiϵ set of\n",
      "labels mentioned in Table 1. For every unseen ﬁnancial term,\n",
      "our task is to generate a ranked list ˆyi consisting of these 17\n",
      "hypernyms in order of decreasing semantic similarity.\n",
      " Evaluation Metrics The expected output is a raked list of\n",
      "predicted labels for every scored instance. The proposed sys-\n",
      "tems are evaluated based on Accuracy and Mean Rank met-\n",
      "rices as per the shared task rules. Evaluation script was pro-\n",
      "vided by organizers, where accuracy and mean rank were de-\n",
      "ﬁned as:\n",
      "Accuracy = 1\n",
      "\n",
      " Given a set F consisting of n tuples of ﬁnancial terms\n",
      "and their hypernyms/top-level concepts/labels i.e.\n",
      "F\n",
      "=\n",
      "{(t1, h1), (t2, h2), ...(tn, hn)} where hi represents the hyper-\n",
      "nym corresponding to the ith term ti and hiϵ set of labels men-\n",
      "tioned in Table 1. For every unseen ﬁnancial term, our task is\n",
      "to generate a ranked list ˆyi consisting of these 17 hypernyms\n",
      "in order of decreasing semantic similarity.\n",
      "Evaluation Metrics The expected output is a raked list of\n",
      "predicted labels for every scored instance. The proposed sys-\n",
      "tems are evaluated based on Accuracy and Mean Rank met-\n",
      "rices as per the shared task rules. Evaluation script was pro-\n",
      "vided by organizers, where accuracy and mean rank were de-\n",
      "ﬁned as:\n",
      "Accuracy = 1\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1 I(yi = ˆyi[1])\n",
      "MeanRank = 1\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1 I(yi = ˆyi[1])\n",
      "MeanRank = 1\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1( ˆyi.index(yi))\n",
      "where ˆyi is the ranked list (with index starting from 1) of pre-\n",
      "dicted labels corresponding to the expected label yi. I is an\n",
      "identity matrix.\n",
      "\n",
      "n\n",
      "�n\n",
      "i=1( ˆyi.index(yi))\n",
      "where ˆyi is the ranked list (with index starting from 1) of pre-\n",
      "dicted labels corresponding to the expected label yi. I is an\n",
      "identity matrix.\n",
      "\n",
      "4\n",
      "Data\n",
      "\n",
      "4\n",
      "Data\n",
      "\n",
      "4.1\n",
      "Data Description\n",
      "\n",
      "4.1\n",
      "Data Description\n",
      "\n",
      " The training dataset shared for this task has a total of\n",
      "1050 single and multi-word terms tagged to 17 different\n",
      "classes/labels out of which 1040 term-label pairs are unique.\n",
      "More than 91% of the terms have 6 words or less and the\n",
      "longest term has 22 words. There were 10 duplicate entries,\n",
      "and 3 terms were assigned 2 different labels. Along with this,\n",
      "\n",
      " The training dataset shared for this task has a total of\n",
      "1050 single and multi-word terms tagged to 17 different\n",
      "classes/labels. More than 91% of the terms have 6 words or\n",
      "less and the longest term has 22 words. There were 10 du-\n",
      "plicate entries, and 3 terms were assigned 2 different labels.\n",
      "Along with this, a corpus of prospectuses in English was pro-\n",
      "vided that had 211 documents. Some of the terms mentioned\n",
      "\n",
      "Label\n",
      "Count\n",
      "\n",
      "Label\n",
      "Count\n",
      "\n",
      "Equity Index\n",
      "280\n",
      "\n",
      "Equity Index\n",
      "280\n",
      "\n",
      "Regulatory Agency\n",
      "205\n",
      "\n",
      "Regulatory Agency\n",
      "205\n",
      "\n",
      "Credit Index\n",
      "125\n",
      "\n",
      "Credit Index\n",
      "125\n",
      "\n",
      "Central Securities Depository\n",
      "107\n",
      "\n",
      "Central Securities Depository\n",
      "107\n",
      "\n",
      "Debt pricing and yields\n",
      "58\n",
      "\n",
      "Debt pricing and yields\n",
      "58\n",
      "\n",
      "Bonds\n",
      "55\n",
      "\n",
      "Bonds\n",
      "55\n",
      "\n",
      "Swap\n",
      "36\n",
      "\n",
      "Swap\n",
      "36\n",
      "\n",
      "Stock Corporation\n",
      "25\n",
      "\n",
      "Stock Corporation\n",
      "25\n",
      "\n",
      "Option\n",
      "24\n",
      "\n",
      "Option\n",
      "24\n",
      "\n",
      "Funds\n",
      "22\n",
      "\n",
      "Funds\n",
      "22\n",
      "\n",
      "Future\n",
      "19\n",
      "\n",
      "Future\n",
      "19\n",
      "\n",
      "Credit Events\n",
      "18\n",
      "\n",
      "Credit Events\n",
      "18\n",
      "\n",
      "MMIs\n",
      "17\n",
      "\n",
      "MMIs\n",
      "17\n",
      "\n",
      "Stocks\n",
      "17\n",
      "\n",
      "Stocks\n",
      "17\n",
      "\n",
      "Parametric schedules\n",
      "15\n",
      "\n",
      "Parametric schedules\n",
      "15\n",
      "\n",
      "Forward\n",
      "9\n",
      "\n",
      "Forward\n",
      "9\n",
      "\n",
      "Securities restrictions\n",
      "8\n",
      "\n",
      "Securities restrictions\n",
      "8\n",
      "\n",
      "Total\n",
      "1040\n",
      "\n",
      "Total\n",
      "1040\n",
      "\n",
      "Table 1: Label distribution in the training set\n",
      "\n",
      "Table 1: Label distribution in the training set\n",
      "\n",
      " a corpus of prospectuses in English that had 211 documents\n",
      "was provided. Some of the terms mentioned in the training\n",
      "data were present in the corpus. Table 1 shows the distribu-\n",
      "tion of these labels in the training set.\n",
      " 4.2\n",
      "Data Augmentation\n",
      " Since the majority of the terms had only a few tokens, we\n",
      "decided to expand the terms wherever possible using various\n",
      "sources.\n",
      "This approach had also been adopted by [Saini,\n",
      "2020] and [Pei and Zhang, 2021] while participating in\n",
      "FinSim-1 and FinSim-2 respectively.\n",
      "\n",
      " in the training data were present in the corpus. Table 1 shows\n",
      "the distribution of these labels in the training set.\n",
      " 4.2\n",
      "Data Augmentation\n",
      "Since the majority of the terms had only a few tokens, we\n",
      "decided to expand the terms wherever possible using various\n",
      "sources.\n",
      "This approach had also been adopted by [Saini,\n",
      "2020] and [Pei and Zhang, 2021] while participating in\n",
      "FinSim-1 and FinSim-2 respectively.\n",
      "\n",
      "Acronym expansion: As mentioned by Keswani et al.\n",
      "[Keswani et al., 2020], the presence of acronyms created a\n",
      "major issue in maintaining consistency. We used the abbre-\n",
      "viation extractor available in spaCy3[Honnibal et al., 2020]\n",
      "package on the corpus of the prospectus to extract all the\n",
      "acronyms and their expansions. Upon manual inspection of\n",
      "a sample output, we identiﬁed that not all the extracted items\n",
      "were valid acronyms and their expansions. We cleaned the\n",
      "extracted list by dropping the records where:\n",
      "\n",
      "Acronym expansion: As mentioned by Keswani et al.\n",
      "[Keswani et al., 2020], the presence of acronyms created a\n",
      "major issue in maintaining consistency. We used the abbre-\n",
      "viation extractor available in spaCy3[Honnibal et al., 2020]\n",
      "package on the corpus of the prospectus to extract all the\n",
      "acronyms and their expansions. Upon manual inspection of\n",
      "a sample output, we identiﬁed that not all the extracted items\n",
      "were valid acronyms and their expansions. We cleaned the\n",
      "extracted list by dropping the records where:\n",
      "\n",
      "• expansion had equal or less length than the acronym.\n",
      "\n",
      "• expansion had equal or less length than the acronym.\n",
      "\n",
      "• expansion had parenthesis\n",
      "\n",
      "• expansion had parenthesis\n",
      "\n",
      "• extracted acronym was a valid English word such as\n",
      "”fund” or ”Germany”.\n",
      "\n",
      "• extracted acronym was a valid English word such as\n",
      "”fund” or ”Germany”.\n",
      "\n",
      "• the expansion had less than or equal to 5 characters.\n",
      "\n",
      "• the expansion had less than or equal to 5 characters.\n",
      "\n",
      "We managed to extract 635 acronyms from the prospectus\n",
      "corpus after applying the above exclusions.\n",
      "We used this\n",
      "data to expand the matching terms in the given train set and\n",
      "test sets.\n",
      "\n",
      "We managed to extract 635 acronyms from the prospectus\n",
      "corpus after applying the above exclusions.\n",
      "We used this\n",
      "data to expand the matching terms in the given train set and\n",
      "test sets.\n",
      "\n",
      " 3https://spacy.io/\n",
      " Deﬁnitions from DBpedia: We used the DBpedia search\n",
      "API4 to extract the description of the terms present in the train\n",
      "and test sets. We present such an example in Figure 1. In ad-\n",
      "dition to the description, the label was also retained from the\n",
      "result payload to identify the right description for the input\n",
      "terms. We tried token overlap-based similarity of input terms\n",
      "with both matching labels and descriptions. We decided to\n",
      "use the label to term match for description matching after go-\n",
      "ing through a randomly drawn sample. We cleaned both input\n",
      "terms and labels from DBpedia results by converting them to\n",
      "lower case, replacing punctuations by space, removing repet-\n",
      "itive spaces, and singularizing the text. We calculated the\n",
      "token overlap ratios for cleaned term and DBpedia labels us-\n",
      "ing these formulas: Ratio1 = length(s1 ∩ s2)/length(s1),\n",
      "Ratio2 = length(s2)/length(s1) where s1 and s2 represents\n",
      "sets of tokenized cleaned terms and tokenized cleaned DBpe-\n",
      "dia labels respectively. We empirically decided to use all the\n",
      "instances with Ratio1 = 1 and Ratio2 <= 1.25 for match-\n",
      "ing a DBpedia label (and hence description) to the input term.\n",
      " Deﬁnitions from Investopedia and FIBO: Inspired by\n",
      "[Saini, 2020], we obtained deﬁnitions of the terms present\n",
      "in Investopedia’s data dictionary5 by crawling it. We down-\n",
      "loaded a glossary of ﬁnancial terms from the website of\n",
      "FIBO. We cleaned all the terms from the train and test set and\n",
      "also the terms present in Investopedia’s data dictionary using\n",
      "the steps described in the above DBpedia section. We then as-\n",
      "signed the Investopedia or FIBO deﬁnition to the terms from\n",
      "the train and test sets where cleaned terms from train and test\n",
      "data matched to cleaned Investopedia terms perfectly.\n",
      "The test set which was provided to us had 326 terms. We\n",
      "augmented the original train and test set with the records\n",
      "where we could either ﬁnd deﬁnition or expansion using the\n",
      "above sources. The train set size increased to 1836 records\n",
      "and the test set size increased to 607 after the data augmen-\n",
      "tation. We present an example of data augmentation for the\n",
      "term “callable bond” in Table 2. Table 3 states the number\n",
      "of instances we used from each of the sources to augment the\n",
      "data we had.\n",
      " 5\n",
      "System Description\n",
      "We tried to solve this problem as the term classiﬁcation and\n",
      "term similarity problems. Two of our 3 submissions are mod-\n",
      "elled as the term classiﬁcation problem, whereas the third sys-\n",
      "tem is designed to be a phrase/sentence similarity problem be-\n",
      "tween terms (or expanded terms from the augmented dataset)\n",
      "and the deﬁnitions of 17 class labels that were extracted from\n",
      "FIBO / Internet. All the systems rely on semantic similarity\n",
      "and use FinBERT model to generate the term or token embed-\n",
      "ding representations. We divided the given data into training\n",
      "and validation sets having 832 and 208 terms respectively.\n",
      " 5.1\n",
      "System - 1 (S1)\n",
      "This is the simplest of our proposed systems, where we did\n",
      "not use the augmented dataset and used only the original set\n",
      " 4https://lookup.dbpedia.org/api/search\n",
      "5https://www.investopedia.com/ﬁnancial-term-dictionary-\n",
      "4769738\n",
      " that was shared by organizers.\n",
      "We loaded FinBERT pre-\n",
      "trained model and ﬁne-tuned it by trying to classify the repre-\n",
      "sentation of [CLS] token into one of the 17 labels mentioned\n",
      "previously. Since the original data did not have longer terms,\n",
      "we kept the maximum length to 32, and train and validation\n",
      "batch sizes of 64. We used Adam optimizer with a learning\n",
      "rate of 0.00002. We ran the model for 40 epochs and picked\n",
      "the model saved after 18th epoch based on the performance on\n",
      "the validation set. Finally, we ranked the predictions based on\n",
      "the predicted probability of each class.\n",
      " 5.2\n",
      "System - 2 (S2)\n",
      "This system is similar to System-1 with the only difference\n",
      "that data being the augmented set and not the original dataset.\n",
      "Since the augmented dataset had the descriptions of the terms,\n",
      "the inputs were considerably longer. Hence, we increased the\n",
      "maximum length to 256 while keeping all the other hyper-\n",
      "parameters the same. After, training the model for 40 epochs\n",
      "we selected the model saved after the 17th epoch as the best\n",
      "model based on validation set performance.\n",
      " 5.3\n",
      "System -3 (S3)\n",
      "We explored the FIBO ontology to understand the hierarchy\n",
      "[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\n",
      "in Figure 2. We used the augmented data described in sec-\n",
      "tion 4.2 to create a labelled dataset having similarity scores.\n",
      "For every term deﬁnition (T) to label deﬁnition (L) mapping\n",
      "which existed in the extended training set, we assigned a sim-\n",
      "ilarity score of 1.0 to the (T,L) pair and picked up 10 train-\n",
      "ing instances randomly ensuring none of their label deﬁnition\n",
      "was same as L. For each of the label deﬁnitions (LL) present\n",
      "in this sample, we extracted its root node and ﬁrst child node.\n",
      "We did the same for the original label deﬁnition (L). Then, we\n",
      "compared these nodes. If the root node and ﬁrst child node of\n",
      "L were different from that of LL then we assigned a similar-\n",
      "ity score of 0 to the (T, LL) pair. If the root nodes were the\n",
      "same, we assigned a similarity score of ’k’ when the ﬁrst child\n",
      "nodes differed and a similarity score of ’2k’ when they were\n",
      "the same (where 0 < k < 1). We empirically ﬁgured out that\n",
      "k=0.4 works the best. As expected, the number of instances\n",
      "with a similarity score equal to 0 increased substantially. We\n",
      "under-sampled such instances and the new training set had\n",
      "30% instances with similarity score 1.0, 12% instances with\n",
      "similarity score ’k’, 28% instances with similarity score ’2k’\n",
      "and 30% instances with similarity score 0. After that, we ﬁne-\n",
      "tuned a FinBERT [Araci, 2019] model using Sentence BERT\n",
      "[Reimers et al., 2019] framework with this newly generated\n",
      "labelled data for 25 epochs with a batch size of 20. Our ob-\n",
      "jective was to minimize the multiple negatives ranking loss\n",
      "and online contrastive loss. We used a margin of 0.5 and co-\n",
      "sine distance as a distance metric while training this model.\n",
      "Finally, we converted all of the 17 labels’ deﬁnitions and term\n",
      "deﬁnitions from the validation set to vectors using this ﬁne-\n",
      "tuned model. For every such term deﬁnition, we performed\n",
      "a semantic search over the label vectors and ranked them in\n",
      "decreasing order of cosine similarity.\n",
      "System 2 and 3 take advantage of term expansion during both\n",
      "model training and scoring phases, which causes certain ob-\n",
      "servations to appear more than once (reference: Table 3). We\n",
      "\n",
      " Deﬁnitions from DBpedia: We used the DBpedia search\n",
      "API4 to extract the description of the terms present in the\n",
      " 3https://spacy.io/\n",
      "4https://lookup.dbpedia.org/api/search\n",
      " 47\n",
      " train and test sets.\n",
      "We present such an example in Fig-\n",
      "ure 1. In addition to the description, the label was also re-\n",
      "tained from the result payload to identify the right descrip-\n",
      "tion for the input terms. We tried token overlap-based simi-\n",
      "larity of input terms with both matching labels and descrip-\n",
      "tions. We decided to use the label to term match for descrip-\n",
      "tion matching after going through a randomly drawn sam-\n",
      "ple. We cleaned both input terms and labels from DBpedia\n",
      "results by converting them to lower case, replacing punctua-\n",
      "tions by space, removing repetitive spaces, and singularizing\n",
      "the text. We calculated the token overlap ratios for cleaned\n",
      "term and DBpedia labels using the formulas mentioned be-\n",
      "low: Ratio1 = length(s1 ∩ s2)/length(s1) , Ratio2 =\n",
      "length(s2/length(s1 where s1 and s2 represents sets of to-\n",
      "kenized cleaned terms and tokenized and cleaned DBpedia\n",
      "labels respectively. We empirically decided to use all the in-\n",
      "stances with Ratio1 = 1 and Ratio2 <= 1.25 for matching\n",
      "a DBpedia label (and hence description) to the input term.\n",
      " Deﬁnitions from Investopedia and FIBO: Inspired by\n",
      "[Saini, 2020], we obtained deﬁnitions of the terms present\n",
      "in Investopedia’s data dictionary5 by crawling it. We down-\n",
      "loaded a glossary of ﬁnancial terms from the website of\n",
      "FIBO. We cleaned all the terms from the train and test set and\n",
      "also the terms present in Investopedia’s data dictionary using\n",
      "the steps described in the above DBpedia section. We then as-\n",
      "signed the Investopedia or FIBO deﬁnition to the terms from\n",
      "the train and test sets where cleaned terms from train and test\n",
      "data matched to cleaned Investopedia terms perfectly.\n",
      "The test set which was provided to us had 326 terms. We\n",
      "augmented the original train and test set with the records\n",
      "where we could either ﬁnd deﬁnition or expansion using the\n",
      "above sources. The train set size increased to 1801 records\n",
      "and the test set size increased to 607 after the data augmen-\n",
      "tation. We present an example of data augmentation for the\n",
      "term ”callable bond” in Table 2. Table 3 states the number\n",
      "of instances we used from each of the sources to augment the\n",
      "data we had.\n",
      " 5\n",
      "System Description\n",
      " We tried to solve this problem as the term classiﬁcation and\n",
      "term similarity problems. Two of our 3 submissions are mod-\n",
      "elled as the term classiﬁcation problem, whereas the third sys-\n",
      "tem is designed to be a phrase/sentence similarity problem be-\n",
      "tween terms (or expanded terms from the augmented dataset)\n",
      "and the deﬁnitions of 17 class labels that were extracted from\n",
      "FIBO / Internet. All the systems rely on semantic similarity\n",
      "and use FinBERT model to generate the term or token embed-\n",
      "ding representations. We divided the given data into training\n",
      "and validation sets having 841 and 209 terms respectively.\n",
      " 5.1\n",
      "System - 1 (S1)\n",
      " This is the simplest of our proposed systems, where we did\n",
      "not use the augmented dataset and stuck to the original set that\n",
      "was shared by organizers. We loaded FinBERT pre-trained\n",
      " 5https://www.investopedia.com/ﬁnancial-term-dictionary-\n",
      "4769738\n",
      " model and ﬁne-tuned it by trying to classify the representa-\n",
      "tion of [CLS] token into one of the 17 labels mentioned pre-\n",
      "viously. Since the original data did not have longer terms,\n",
      "we kept the maximum length to 32, and train and validation\n",
      "batch sizes of 64. We used Adam optimizer with a learning\n",
      "rate of 0.00002. We ran the model for 40 epochs and picked\n",
      "the model saved after 18th epoch based on the performance on\n",
      "the validation set. Finally, we ranked the predictions based on\n",
      "the predicted probability of each class.\n",
      " 5.2\n",
      "System - 2 (S2)\n",
      " This system is similar to System-1 with the only difference\n",
      "that data being the augmented set and not the original dataset.\n",
      "Since the augmented dataset has the descriptions of the terms,\n",
      "the input is considerably longer. Hence, we increased the\n",
      "maximum length to 256 while keeping all the other hyper-\n",
      "parameters the same. After, training the model for 40 epochs\n",
      "we selected the model saved after the 17th epoch as the best\n",
      "model based on validation set performance.\n",
      " 5.3\n",
      "System -3 (S3)\n",
      " We explored the FIBO ontology to understand the hierarchy\n",
      "[Stepiˇsnik Perdih et al., 2021] of the 17 labels as depicted\n",
      "in Figure 2. We used the augmented data described in sec-\n",
      "tion 4.2 to create a labelled dataset having similarity scores.\n",
      "For every term deﬁnition (T) to label deﬁnition (L) mapping\n",
      "which existed in the extended training set, we assigned a sim-\n",
      "ilarity score of 1.0 to the (T,L) pair and picked up 10 train-\n",
      "ing instances randomly ensuring none of their label deﬁnition\n",
      "was same as L. For each of the label deﬁnitions (LL) present\n",
      "in this sample, we extracted its root node and ﬁrst child node.\n",
      "We did the same for the original label deﬁnition (L). Then, we\n",
      "compared these nodes. If the root node and ﬁrst child node of\n",
      "L were different from that of LL then we assigned a similar-\n",
      "ity score of 0 to the (T, LL) pair. If the root nodes were the\n",
      "same, we assigned a similarity score of ’k’ when the ﬁrst child\n",
      "nodes differed and a similarity score of ’2k’ when they were\n",
      "the same (where 0 < k < 1). We empirically ﬁgured out that\n",
      "k=0.4 works the best. As expected, the number of instances\n",
      "with a similarity score equal to 0 increased substantially. We\n",
      "under-sampled such instances and the new training set had\n",
      "30% instances with similarity score 1.0, 12% instances with\n",
      "similarity score ’k’, 28% instances with similarity score ’2k’\n",
      "and 30% instances with similarity score 0. After that, we ﬁne-\n",
      "tuned a FinBERT [Araci, 2019] model using Sentence BERT\n",
      "[Reimers et al., 2019] framework with this newly generated\n",
      "labelled data for 25 epochs with a batch size of 20. Our ob-\n",
      "jective was to minimize the multiple negatives ranking loss\n",
      "and online contrastive loss. We used a margin of 0.5 and co-\n",
      "sine distance as a distance metric while training this model.\n",
      "Finally, we converted all of the 17 labels’ deﬁnitions and term\n",
      "deﬁnitions from the validation set to vectors using this ﬁne-\n",
      "tuned model. For every such term deﬁnition, we performed\n",
      "a semantic search over the label vectors and ranked them in\n",
      "decreasing order of similarity.\n",
      "System 2 and 3 take advantage of term expansion during both\n",
      "model training and scoring phases, which causes certain ob-\n",
      "servations to appear more than once (reference: Table 3). We\n",
      " 48\n",
      "\n",
      "Figure 1: Sample output from DBpedia search API\n",
      "\n",
      "Figure 1: Sample output from DBpedia search API\n",
      "\n",
      "Expanded Term/Term Deﬁnition\n",
      "Label\n",
      "Source\n",
      "\n",
      "Expanded Term/Term Deﬁnition\n",
      "Label\n",
      "Source\n",
      "\n",
      "Callable bond\n",
      "Bonds\n",
      "original and\n",
      "acronym expansion\n",
      "\n",
      "Callable bond\n",
      "Bonds\n",
      "original and\n",
      "acronym expansion\n",
      "\n",
      "bond that includes a stipulation allowing the issuer\n",
      "the right to repurchase and retire the bond at the call price after the call protection period\n",
      "Bonds\n",
      "FIBO\n",
      "\n",
      "bond that includes a stipulation allowing the issuer\n",
      "the right to repurchase and retire the bond at the call price after the call protection period\n",
      "Bonds\n",
      "FIBO\n",
      "\n",
      "A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\n",
      "the issuer of the bond to retain the privilege of redeeming the bond at some point before\n",
      "the bond reaches its date of maturity.\n",
      "\n",
      "A callable bond (also called redeemable bond) is a type of bond (debt security) that allows\n",
      "the issuer of the bond to retain the privilege of redeeming the bond at some point before\n",
      "the bond reaches its date of maturity.\n",
      "\n",
      "Bonds\n",
      "DBpedia\n",
      "\n",
      "Bonds\n",
      "DBpedia\n",
      "\n",
      "Table 2: Result of Data Augmentation of the term ”Callable bond”\n",
      "\n",
      "Table 2: Result of Data Augmentation of the term ”Callable bond”\n",
      "\n",
      "Data Source\n",
      "Count\n",
      "\n",
      "Data Source\n",
      "Count\n",
      "\n",
      "Original modelling data\n",
      "1040\n",
      "\n",
      "Original modelling data\n",
      "1040\n",
      "\n",
      "DBpedia\n",
      "257\n",
      "\n",
      "DBpedia\n",
      "257\n",
      "\n",
      "FIBO\n",
      "236\n",
      "\n",
      "FIBO\n",
      "236\n",
      "\n",
      "Investopedia\n",
      "85\n",
      "\n",
      "Investopedia\n",
      "85\n",
      "\n",
      "Acronym expansion\n",
      "218\n",
      "\n",
      "Acronym expansion\n",
      "218\n",
      "\n",
      "Table 3: Details of various data sources\n",
      "\n",
      "Table 3: Details of various data sources\n",
      "\n",
      "derive the ﬁnal prediction by averaging the output probabili-\n",
      "ties for all the 17 classes for all the occurrences of the term.\n",
      "\n",
      "derive the ﬁnal prediction by averaging the output probabili-\n",
      "ties for all the 17 classes for all the occurrences of the term.\n",
      "\n",
      "6\n",
      "Experimentation and Results\n",
      "\n",
      "6\n",
      "Experimentation and Results\n",
      "\n",
      " We had 1040 observations after removing the duplicates. We\n",
      "did an 80:20 split to create a training and validation set from\n",
      "this. We augmented the given modelling set by incorporating\n",
      "deﬁnitions from DBpedia, FIBO and Investopedia. We used\n",
      "the list of acronyms extracted from the prospectus corpus to\n",
      "create a copy with acronym expansion. This helped us to in-\n",
      "crease the original data to 1836 records (mentioned in Table\n",
      "1). It should be noted that we could not ﬁnd the expansions\n",
      "for all the terms given in the modelling set. Train and valida-\n",
      "tion set sizes for the original modelling set and expanded data\n",
      "were (832 & 208) and (1470 & 366) respectively.\n",
      "We established a baseline by running the scripts provided\n",
      "by the organizers. Then, we considered original modelling\n",
      "data and ﬁne-tuned base BERT-cased model [Devlin et al.,\n",
      "2019] to predict the class label by taking the representa-\n",
      "tion of [CLS] token while passing it through few layers of\n",
      "a feed-forward network. This performed better than base-\n",
      "line. We then tried the same BERT-base model on the ex-\n",
      "panded dataset, which gave us further performance improve-\n",
      "ment. Since the only major change between these runs was\n",
      " the data, the improvement can be attributed to the expanded\n",
      "data.\n",
      "We experimented with a few of the other pre-trained mod-\n",
      "els that are available on the Huggingface model repository\n",
      "[Wolf et al., 2020]. We observed clear improvement when\n",
      "we used the FinBERT model which was trained on data spe-\n",
      "ciﬁc to the ﬁnancial domain. The model performance succes-\n",
      "sively increased when we used a combination of data expan-\n",
      "sion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\n",
      "BERT using Sentence Transformers [Reimers et al., 2019] to\n",
      "capture semantic textual similarity. For this, we used several\n",
      "combinations of term and term deﬁnitions with label and la-\n",
      "bel deﬁnitions.\n",
      "All the hyperparameters for the ﬁnal 3 models have been\n",
      "already mentioned in the system description. After rigorous\n",
      "experimentation, these hyperparameters were selected empir-\n",
      "ically based on validation set performance. The results are\n",
      "presented in Table 4. Since the number of submissions was\n",
      "restricted to 3 for each team, we do not have the performance\n",
      "numbers of the BERT models in the test set. Analysing the\n",
      "results we see that SentenceBERT trained with FinBERT at\n",
      "the backed as mentioned in section-5.3 performed the best.\n",
      " 7\n",
      "Conclusion and Future Works\n",
      " In this work, we attempted to solve the hypernym and syn-\n",
      "onym discovery hosted at FinSim-3. This challenge aimed\n",
      "to enable the better use of ontologies like FIBO using hy-\n",
      "pernyms and synonyms, and we used these ontologies them-\n",
      "selves to develop our systems which perform signiﬁcantly\n",
      "better than the provided baseline systems. This proves the\n",
      "present use of these ontologies. The presented solution is\n",
      "recursive in a sense as it uses knowledge from ontologies\n",
      "to further increase the effectiveness and use of the same.\n",
      "\n",
      " We had 1040 observations after removing the duplicates. We\n",
      "did an 80:20 split to create a training and validation set from\n",
      "this. We augmented the given modelling set by incorporating\n",
      "deﬁnitions from DBpedia, FIBO and Investopedia. We used\n",
      "the list of acronyms extracted from the prospectus corpus to\n",
      "create a copy with acronym expansion. This helped us to in-\n",
      "crease the original data to 1836 records (mentioned in Table\n",
      "1). It should be noted that we could not ﬁnd the expansions\n",
      "for all the terms given in the modelling set. Train and valida-\n",
      "tion set sizes for the original modelling set and expanded data\n",
      "were (832 & 208) and (1469 & 366) respectively.\n",
      "We established a baseline by running the scripts provided\n",
      "by the organizers. Then, we considered original modelling\n",
      "data and ﬁne-tuned base BERT-cased model [Devlin et al.,\n",
      "2019] to predict the class label by taking the representa-\n",
      "tion of [CLS] token while passing it through few layers of\n",
      "a feed-forward network. This performed better than base-\n",
      "line. We then tried the same BERT-base model on the ex-\n",
      "panded dataset, which gave us further performance improve-\n",
      "ment. Since the only change between these runs was the data,\n",
      " the improvement can be attributed to the expanded data.\n",
      "We experimented with a few of the other pre-trained mod-\n",
      "els that are available on the Huggingface model repository\n",
      "[Wolf et al., 2020]. We observed clear improvement when\n",
      "we used the FinBERT model which was trained on data spe-\n",
      "ciﬁc to the ﬁnancial domain. The model performance succes-\n",
      "sively increased when we used a combination of data expan-\n",
      "sion with FinBERT. Furthermore, we tried to ﬁne-tune Fin-\n",
      "BERT using Sentence Transformers [Reimers et al., 2019] to\n",
      "capture semantic textual similarity. For this, we used several\n",
      "combinations of term and term deﬁnitions with label and la-\n",
      "bel deﬁnitions.\n",
      "All the hyperparameters for the ﬁnal 3 models are already\n",
      "given in the system description. After rigorous experimenta-\n",
      "tion, these hyperparameters were selected empirically based\n",
      "on validation set performance. The results are presented in\n",
      "Table 4. Since the number of submissions was restricted to\n",
      "3 for each team, we do not have the performance numbers of\n",
      "the BERT models in the test set. Analysing the results we see\n",
      "that SentenceBERT trained with FinBERT at the backed as\n",
      "mentioned in section-5.3 performed the best.\n",
      " 7\n",
      "Conclusion and Future Works\n",
      "In this work, we attempted to solve the hypernym and syn-\n",
      "onym discovery hosted at FinSim-3. This challenge aimed\n",
      "to enable the better use of ontologies like FIBO using hy-\n",
      "pernyms and synonyms, and we used these ontologies them-\n",
      "selves to develop our systems which perform signiﬁcantly\n",
      "better than the provided baseline systems. This proves the\n",
      "present use of these ontologies. The presented solution is\n",
      "recursive in a sense as it uses knowledge from ontologies\n",
      "to further increase the effectiveness and use of the same.\n",
      "Apart from data augmentation, our solution relies upon se-\n",
      "mantic similarity learnt from pre-trained embedding models\n",
      " 49\n",
      "\n",
      "Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\n",
      "\n",
      "Figure 2: Label Hierarchy from FIBO. Bold (leaf nodes) denotes the labels.\n",
      "\n",
      "Validation set\n",
      "Test set\n",
      "\n",
      "Validation set\n",
      "Test set\n",
      "\n",
      "Model\n",
      "Data\n",
      "Rank\n",
      "Acc.\n",
      "Rank\n",
      "Acc.\n",
      "\n",
      "Model\n",
      "Data\n",
      "Rank\n",
      "Acc.\n",
      "Rank\n",
      "Acc.\n",
      "\n",
      "Base-1\n",
      "Org.\n",
      "2.158\n",
      "0.498\n",
      "1.941\n",
      "0.564\n",
      "\n",
      "Base-1\n",
      "Org.\n",
      "2.158\n",
      "0.498\n",
      "1.941\n",
      "0.564\n",
      "\n",
      "Base-2\n",
      "Org.\n",
      "1.201\n",
      "0.876\n",
      "1.75\n",
      "0.669\n",
      "\n",
      "Base-2\n",
      "Org.\n",
      "1.201\n",
      "0.876\n",
      "1.75\n",
      "0.669\n",
      "\n",
      "BERT\n",
      "Org.\n",
      "1.177\n",
      "0.899\n",
      "-\n",
      "-\n",
      "\n",
      "BERT\n",
      "Org.\n",
      "1.177\n",
      "0.899\n",
      "-\n",
      "-\n",
      "\n",
      "BERT\n",
      "Ext.\n",
      "1.153\n",
      "0.928\n",
      "-\n",
      "-\n",
      "\n",
      "BERT\n",
      "Ext.\n",
      "1.153\n",
      "0.928\n",
      "-\n",
      "-\n",
      "\n",
      "FinBERT(S1)\n",
      "Org.\n",
      "1.117\n",
      "0.928\n",
      "1.257\n",
      "0.886\n",
      "\n",
      "FinBERT(S1)\n",
      "Org.\n",
      "1.117\n",
      "0.928\n",
      "1.257\n",
      "0.886\n",
      "\n",
      "FinBERT(S2)\n",
      "Ext.\n",
      "1.110\n",
      "0.942\n",
      "1.220\n",
      "0.895\n",
      "\n",
      "FinBERT(S2)\n",
      "Ext.\n",
      "1.110\n",
      "0.942\n",
      "1.220\n",
      "0.895\n",
      "\n",
      "SBERT(S3)\n",
      "Ext.\n",
      "1.086\n",
      "0.947\n",
      "1.156\n",
      "0.917\n",
      "\n",
      "SBERT(S3)\n",
      "Ext.\n",
      "1.086\n",
      "0.947\n",
      "1.156\n",
      "0.917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df['Anth'].iloc[i-1])\n",
    "    print(df['Arxiv'].iloc[i-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39371040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anth     Fidelity Investments, AI CoE, Bengaluru, India...\n",
       "Arxiv    Fidelity Investments, AI CoE, Bengaluru, India...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1d8f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a485d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_Anth'] = df['Anth'].apply(clean_n)\n",
    "df['clean_Arxiv'] = df['Arxiv'].apply(clean_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01e742c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anth</th>\n",
       "      <th>Arxiv</th>\n",
       "      <th>clean_Anth</th>\n",
       "      <th>clean_Arxiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Term Expansion and FinBERT ﬁne-tuning for Hype...</td>\n",
       "      <td>Term Expansion and FinBERT ﬁne-tuning for Hype...</td>\n",
       "      <td>Term Expansion and FinBERT ﬁne-tuning for Hype...</td>\n",
       "      <td>Term Expansion and FinBERT ﬁne-tuning for Hype...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ankush Chopra∗† , Sohom Ghosh†\\n</td>\n",
       "      <td>Ankush Chopra∗† , Sohom Ghosh†\\n</td>\n",
       "      <td>Ankush Chopra∗† , Sohom Ghosh†</td>\n",
       "      <td>Ankush Chopra∗† , Sohom Ghosh†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fidelity Investments, AI CoE, Bengaluru, India...</td>\n",
       "      <td>Fidelity Investments, AI CoE, Bengaluru, India...</td>\n",
       "      <td>Fidelity Investments, AI CoE, Bengaluru, India...</td>\n",
       "      <td>Fidelity Investments, AI CoE, Bengaluru, India...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract\\n</td>\n",
       "      <td>Abstract\\n</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hypernym and synonym matching are one of the\\...</td>\n",
       "      <td>Hypernym and synonym matching are one of the\\...</td>\n",
       "      <td>Hypernym and synonym matching are one of the ...</td>\n",
       "      <td>Hypernym and synonym matching are one of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n</td>\n",
       "      <td>BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n</td>\n",
       "      <td>BERT Ext. 1.153 0.928</td>\n",
       "      <td>BERT Ext. 1.153 0.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n</td>\n",
       "      <td>FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n</td>\n",
       "      <td>FinBERT(S1) Org. 1.117 0.928 1.257 0.886</td>\n",
       "      <td>FinBERT(S1) Org. 1.117 0.928 1.257 0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n</td>\n",
       "      <td>FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n</td>\n",
       "      <td>FinBERT(S2) Ext. 1.110 0.942 1.220 0.895</td>\n",
       "      <td>FinBERT(S2) Ext. 1.110 0.942 1.220 0.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n</td>\n",
       "      <td>SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n</td>\n",
       "      <td>SBERT(S3) Ext. 1.086 0.947 1.156 0.917</td>\n",
       "      <td>SBERT(S3) Ext. 1.086 0.947 1.156 0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Table 4: Results on validation and test set. O...</td>\n",
       "      <td>Table 4: Results on validation and test set. O...</td>\n",
       "      <td>Table 4: Results on validation and test set. O...</td>\n",
       "      <td>Table 4: Results on validation and test set. O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Anth  \\\n",
       "0   Term Expansion and FinBERT ﬁne-tuning for Hype...   \n",
       "1                    Ankush Chopra∗† , Sohom Ghosh†\\n   \n",
       "2   Fidelity Investments, AI CoE, Bengaluru, India...   \n",
       "3                                          Abstract\\n   \n",
       "4    Hypernym and synonym matching are one of the\\...   \n",
       "..                                                ...   \n",
       "72                   BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n   \n",
       "73    FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n   \n",
       "74    FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n   \n",
       "75      SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n   \n",
       "76  Table 4: Results on validation and test set. O...   \n",
       "\n",
       "                                                Arxiv  \\\n",
       "0   Term Expansion and FinBERT ﬁne-tuning for Hype...   \n",
       "1                    Ankush Chopra∗† , Sohom Ghosh†\\n   \n",
       "2   Fidelity Investments, AI CoE, Bengaluru, India...   \n",
       "3                                          Abstract\\n   \n",
       "4    Hypernym and synonym matching are one of the\\...   \n",
       "..                                                ...   \n",
       "72                   BERT\\nExt.\\n1.153\\n0.928\\n-\\n-\\n   \n",
       "73    FinBERT(S1)\\nOrg.\\n1.117\\n0.928\\n1.257\\n0.886\\n   \n",
       "74    FinBERT(S2)\\nExt.\\n1.110\\n0.942\\n1.220\\n0.895\\n   \n",
       "75      SBERT(S3)\\nExt.\\n1.086\\n0.947\\n1.156\\n0.917\\n   \n",
       "76  Table 4: Results on validation and test set. O...   \n",
       "\n",
       "                                           clean_Anth  \\\n",
       "0   Term Expansion and FinBERT ﬁne-tuning for Hype...   \n",
       "1                     Ankush Chopra∗† , Sohom Ghosh†    \n",
       "2   Fidelity Investments, AI CoE, Bengaluru, India...   \n",
       "3                                           Abstract    \n",
       "4    Hypernym and synonym matching are one of the ...   \n",
       "..                                                ...   \n",
       "72                             BERT Ext. 1.153 0.928    \n",
       "73          FinBERT(S1) Org. 1.117 0.928 1.257 0.886    \n",
       "74          FinBERT(S2) Ext. 1.110 0.942 1.220 0.895    \n",
       "75            SBERT(S3) Ext. 1.086 0.947 1.156 0.917    \n",
       "76  Table 4: Results on validation and test set. O...   \n",
       "\n",
       "                                          clean_Arxiv  \n",
       "0   Term Expansion and FinBERT ﬁne-tuning for Hype...  \n",
       "1                     Ankush Chopra∗† , Sohom Ghosh†   \n",
       "2   Fidelity Investments, AI CoE, Bengaluru, India...  \n",
       "3                                           Abstract   \n",
       "4    Hypernym and synonym matching are one of the ...  \n",
       "..                                                ...  \n",
       "72                             BERT Ext. 1.153 0.928   \n",
       "73          FinBERT(S1) Org. 1.117 0.928 1.257 0.886   \n",
       "74          FinBERT(S2) Ext. 1.110 0.942 1.220 0.895   \n",
       "75            SBERT(S3) Ext. 1.086 0.947 1.156 0.917   \n",
       "76  Table 4: Results on validation and test set. O...  \n",
       "\n",
       "[77 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b7da1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_Anth'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "195c5d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Abstract ', 'Abstract ')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_Anth'].iloc[3], df['clean_Arxiv'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2628aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "952050c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.0660406   0.03597628  0.08232541 ... -0.02758392 -0.06031669\n",
      "  -0.01588463]\n",
      " [-0.0660406   0.03597628  0.08232541 ... -0.02758392 -0.06031669\n",
      "  -0.01588463]], shape=(2, 512), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999999, 0.9999999],\n",
       "       [0.9999999, 0.9999999]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_both = embed([df['clean_Anth'].iloc[3], df['clean_Arxiv'].iloc[3]])\n",
    "# e_Arxiv = embed()\n",
    "\n",
    "# print(e_Arxiv, e_Anth)\n",
    "print(e_both)\n",
    "cosine_similarity(e_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8eab09b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06068841 -0.08901937  0.009689   ... -0.00118487 -0.00935884\n",
      "   0.01548933]\n",
      " [ 0.06970452 -0.03407925  0.04807003 ... -0.08015236  0.01355954\n",
      "  -0.05697582]\n",
      " [ 0.0460197  -0.00676592 -0.0265883  ...  0.05534033 -0.07345361\n",
      "  -0.08141246]\n",
      " ...\n",
      " [-0.05944481 -0.0490467   0.02642503 ... -0.01587217  0.02697126\n",
      "   0.00699797]\n",
      " [-0.01973367 -0.05383429  0.02668439 ... -0.05047641  0.03337703\n",
      "   0.0107846 ]\n",
      " [ 0.03363169 -0.08258406 -0.00992958 ...  0.01526111  0.01070202\n",
      "  -0.0352058 ]], shape=(77, 512), dtype=float32) tf.Tensor(\n",
      "[[ 0.06068841 -0.08901937  0.009689   ... -0.00118487 -0.00935884\n",
      "   0.01548933]\n",
      " [ 0.06970452 -0.03407925  0.04807003 ... -0.08015236  0.01355954\n",
      "  -0.05697582]\n",
      " [ 0.0460197  -0.00676592 -0.0265883  ...  0.05534033 -0.07345361\n",
      "  -0.08141246]\n",
      " ...\n",
      " [-0.05944481 -0.0490467   0.02642503 ... -0.01587217  0.02697126\n",
      "   0.00699797]\n",
      " [-0.01973367 -0.05383429  0.02668439 ... -0.05047641  0.03337703\n",
      "   0.0107846 ]\n",
      " [ 0.03363169 -0.08258406 -0.00992958 ...  0.01526111  0.01070202\n",
      "  -0.0352058 ]], shape=(77, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "e_Anth = embed(df['clean_Anth'])\n",
    "e_Arxiv = embed(df['clean_Arxiv'])\n",
    "\n",
    "print(e_Arxiv, e_Anth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34c6c774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000002 ,  0.0072812 ,  0.0939659 , ...,  0.04023684,\n",
       "         0.11238029,  0.17212136],\n",
       "       [ 0.0072812 ,  1.        ,  0.15659754, ...,  0.23801717,\n",
       "         0.32555997, -0.06540015],\n",
       "       [ 0.0939659 ,  0.15659754,  1.        , ...,  0.11673675,\n",
       "         0.15190855,  0.01112158],\n",
       "       ...,\n",
       "       [ 0.04023684,  0.23801717,  0.11673675, ...,  1.        ,\n",
       "         0.6585305 ,  0.06733611],\n",
       "       [ 0.11238029,  0.32555997,  0.15190855, ...,  0.6585305 ,\n",
       "         1.0000001 ,  0.05017748],\n",
       "       [ 0.17212136, -0.06540016,  0.01112157, ...,  0.0673361 ,\n",
       "         0.05017751,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result  = cosine_similarity(e_Anth, e_Arxiv)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "686e4e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 77)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9418e9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([77, 512])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_Arxiv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "# embeddings = embed([s_Anth, s_Arx])\n",
    "\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87e60641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1a2a0ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[0.99847585]]\n",
      "[[1.]]\n",
      "[[0.9903998]]\n",
      "[[0.9999999]]\n",
      "[[0.99277234]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.9999999]]\n",
      "[[0.26552123]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[0.99999994]]\n",
      "[[1.]]\n",
      "[[1.0000002]]\n",
      "[[1.0000001]]\n",
      "[[0.9999999]]\n",
      "[[0.9082912]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.99999994]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[0.99999994]]\n",
      "[[0.9999999]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.9460838]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.99888074]]\n",
      "[[1.0000001]]\n",
      "[[1.0000002]]\n",
      "[[1.0000002]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[0.9999999]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.99999994]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[0.9958594]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[0.99999994]]\n",
      "[[1.0000001]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.0000002]]\n",
      "[[1.]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# cannot take the average, some parts are larger then others. Titles are 1. And small piece of text. Will skew the result.\n",
    "for i in range(77):\n",
    "    print(cosine_similarity(e_Anth[i].reshape(1,-1), e_Arxiv[i].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7276d0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(77, 512), dtype=float32, numpy=\n",
       "array([[ 0.06068841, -0.08901937,  0.009689  , ..., -0.00118487,\n",
       "        -0.00935884,  0.01548933],\n",
       "       [ 0.06970452, -0.03407925,  0.04807003, ..., -0.08015236,\n",
       "         0.01355954, -0.05697582],\n",
       "       [ 0.0460197 , -0.00676592, -0.0265883 , ...,  0.05534033,\n",
       "        -0.07345361, -0.08141246],\n",
       "       ...,\n",
       "       [-0.05944481, -0.0490467 ,  0.02642503, ..., -0.01587217,\n",
       "         0.02697126,  0.00699797],\n",
       "       [-0.01973367, -0.05383429,  0.02668439, ..., -0.05047641,\n",
       "         0.03337703,  0.0107846 ],\n",
       "       [ 0.03363169, -0.08258406, -0.00992958, ...,  0.01526111,\n",
       "         0.01070202, -0.0352058 ]], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_Anth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4dabd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38bf5dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 512), dtype=float32, numpy=\n",
       "array([[ 6.97045177e-02, -3.40792537e-02,  4.80700321e-02,\n",
       "         1.10333441e-02,  9.67413634e-02, -4.06522155e-02,\n",
       "         5.80488555e-02,  1.21750701e-02,  3.73702087e-02,\n",
       "         3.01103964e-02,  1.46108782e-02, -1.76529419e-02,\n",
       "         2.12727226e-02, -1.84823908e-02,  4.91663404e-02,\n",
       "        -4.81386855e-03,  3.57749052e-02, -5.06638624e-02,\n",
       "        -3.00153997e-02,  4.16631028e-02,  2.80601811e-02,\n",
       "         3.03535871e-02, -3.44741829e-02,  4.79683690e-02,\n",
       "        -1.54933454e-02, -1.26457773e-02,  2.48310398e-02,\n",
       "         1.55036226e-02,  2.29931939e-02,  6.50966167e-02,\n",
       "        -5.92529355e-03,  7.20450878e-02, -2.10878719e-02,\n",
       "        -6.03301488e-02,  8.38561505e-02,  5.64505309e-02,\n",
       "        -5.87900020e-02,  2.18304154e-02, -9.02495235e-02,\n",
       "         2.74714287e-02, -4.52572256e-02, -5.38781025e-02,\n",
       "         5.10093905e-02,  9.02677886e-03,  9.32785310e-03,\n",
       "         1.37680694e-02, -7.02025555e-03,  2.62791826e-03,\n",
       "        -5.47175072e-02,  2.41855532e-02, -6.85425252e-02,\n",
       "        -1.26991607e-02, -6.21590205e-02, -7.14912340e-02,\n",
       "        -6.43757880e-02, -3.44753042e-02,  8.36104676e-02,\n",
       "         3.31308767e-02,  4.10073400e-02, -2.83907428e-02,\n",
       "         1.58792883e-02, -3.17198187e-02, -7.93911517e-02,\n",
       "         1.34197157e-02, -1.16215721e-02, -2.73023313e-03,\n",
       "         1.85968690e-02,  6.36691898e-02, -5.62176295e-02,\n",
       "        -1.25064291e-02, -2.58840993e-02,  1.53870136e-02,\n",
       "         9.59992595e-03,  8.54586512e-02, -3.65951695e-02,\n",
       "         4.05444019e-02,  5.05253300e-02, -1.41261192e-02,\n",
       "        -4.39400878e-03, -2.42375606e-03, -5.75955585e-03,\n",
       "         1.00021912e-02,  2.14467812e-02,  5.49184456e-02,\n",
       "         1.34967277e-02, -3.43261920e-02,  1.70261338e-02,\n",
       "         1.23644096e-03, -1.80651657e-02, -5.93851246e-02,\n",
       "        -3.45208533e-02,  7.49183446e-02,  6.09599203e-02,\n",
       "         2.39865649e-02, -4.12368514e-02, -3.49022611e-03,\n",
       "         3.70511785e-02,  5.56015298e-02,  5.90677150e-02,\n",
       "         2.51107197e-02, -3.88959335e-04,  1.45355854e-02,\n",
       "        -5.61256334e-02,  8.59657302e-03,  2.77927332e-02,\n",
       "         8.00960138e-02, -7.50899240e-02, -3.52383628e-02,\n",
       "         2.62663197e-02,  1.08571313e-02, -2.63876598e-02,\n",
       "        -8.58198255e-02,  4.35125530e-02, -5.19635454e-02,\n",
       "         1.70895748e-03, -3.75114903e-02, -4.94239666e-02,\n",
       "         1.00224940e-02,  1.06019760e-02, -3.02989520e-02,\n",
       "         5.71031384e-02,  4.26550917e-02, -1.30934874e-03,\n",
       "        -4.14250568e-02, -7.31038153e-02, -1.17161516e-02,\n",
       "         3.21700834e-02,  2.59128097e-03,  1.80471614e-02,\n",
       "         2.69628037e-02, -3.35071534e-02, -4.56484072e-02,\n",
       "         7.34410286e-02, -1.05598513e-02, -5.01534976e-02,\n",
       "         4.42197584e-02, -3.59123237e-02,  8.71589184e-02,\n",
       "         1.80683024e-02,  3.36116143e-02, -5.88421747e-02,\n",
       "         1.64588820e-02, -2.59174947e-02, -6.35339469e-02,\n",
       "         3.65670696e-02, -2.74360739e-02,  2.98279282e-02,\n",
       "        -2.87083238e-02, -6.42989054e-02,  8.06149840e-03,\n",
       "         8.92568305e-02,  6.85468968e-03, -1.85191426e-02,\n",
       "         4.96748043e-03,  5.92649989e-02,  2.97511499e-02,\n",
       "         6.11743703e-02, -2.10407991e-02,  1.01493648e-03,\n",
       "        -4.17408869e-02, -6.38943911e-02, -4.27230261e-02,\n",
       "         4.83937934e-02, -4.10826206e-02, -8.96596685e-02,\n",
       "        -2.04964355e-02, -1.75325964e-02, -4.49831076e-02,\n",
       "        -3.58094424e-02,  1.72382686e-02, -2.36319238e-03,\n",
       "        -5.37648387e-02,  7.33804852e-02, -5.18513136e-02,\n",
       "         3.21179144e-02, -2.02092882e-02,  2.00915411e-02,\n",
       "        -4.32864344e-03, -8.76454413e-02,  6.56739399e-02,\n",
       "        -2.38830913e-02, -6.09281547e-02,  2.93656755e-02,\n",
       "        -6.09456226e-02,  1.07690645e-03,  6.79363217e-03,\n",
       "        -1.26673980e-02, -1.87839649e-03, -3.00363619e-02,\n",
       "         6.51072562e-02, -1.38894487e-02, -4.21711467e-02,\n",
       "         3.48400176e-02, -6.45648465e-02, -7.28558376e-02,\n",
       "         7.25892782e-02,  9.90847647e-02, -1.36561245e-02,\n",
       "         2.79052276e-02, -2.93467939e-02, -8.81536976e-02,\n",
       "        -1.27700716e-02, -4.62010652e-02,  1.46555603e-02,\n",
       "         3.44238281e-02, -2.59892805e-03, -3.06976736e-02,\n",
       "         3.13486084e-02,  4.38431352e-02,  1.53316772e-02,\n",
       "        -7.28583261e-02, -2.77304389e-02, -1.75362565e-02,\n",
       "        -1.27192456e-02, -5.06290235e-02, -1.01599939e-01,\n",
       "         2.79559642e-02,  4.72328477e-02,  1.96236633e-02,\n",
       "        -2.96242163e-02, -3.79726104e-02,  1.67260356e-02,\n",
       "         7.27104722e-03, -2.87592486e-02,  1.57461781e-02,\n",
       "        -3.57975326e-02,  3.91234942e-02,  1.55679779e-02,\n",
       "        -2.70762760e-02, -4.40018401e-02, -6.01989478e-02,\n",
       "         4.99560125e-02,  2.39672586e-02,  1.04189485e-01,\n",
       "         5.31047620e-02, -8.31065178e-02, -8.23270231e-02,\n",
       "        -2.39463691e-02,  4.12935428e-02,  2.54143458e-02,\n",
       "         5.38329817e-02, -4.04019319e-02,  2.04363875e-02,\n",
       "        -3.99897397e-02,  5.94709106e-02, -7.71617796e-03,\n",
       "        -4.80712857e-03, -3.98801453e-02, -3.97901535e-02,\n",
       "         6.48443699e-02, -5.98389842e-02, -7.77968019e-02,\n",
       "        -1.92639511e-03, -1.74794365e-02, -1.44590558e-02,\n",
       "        -5.75134158e-02,  1.36862900e-02, -6.20713010e-02,\n",
       "        -5.51132746e-02, -2.26711426e-02,  4.25806604e-02,\n",
       "        -7.40974844e-02,  1.10644577e-02,  3.12103685e-02,\n",
       "        -8.55158828e-03, -9.72870458e-03, -3.59937288e-02,\n",
       "         3.31848115e-02, -3.94491814e-02, -6.13941811e-02,\n",
       "        -9.41683576e-02, -5.52058555e-02, -3.09533626e-02,\n",
       "         3.61802429e-02, -7.79823139e-02,  5.20291217e-02,\n",
       "         1.74186658e-02,  2.50125825e-02, -6.44755214e-02,\n",
       "         6.02891296e-02, -2.84144208e-02,  1.80471689e-02,\n",
       "         3.02803674e-04,  6.06544055e-02,  3.36442282e-03,\n",
       "        -6.66885898e-02, -5.75450389e-03,  6.29454702e-02,\n",
       "        -3.67750563e-02,  1.69220529e-02,  1.03004903e-01,\n",
       "        -1.18274018e-02,  4.53710221e-02,  6.54235780e-02,\n",
       "        -7.00657293e-02, -3.32816727e-02, -3.67584005e-02,\n",
       "         1.71228442e-02,  2.94200573e-02,  1.08003858e-02,\n",
       "         5.10847196e-02, -8.40868149e-03,  5.04219756e-02,\n",
       "         1.23445701e-03, -3.64133455e-02, -5.69173172e-02,\n",
       "        -5.26030883e-02, -2.73226742e-02,  4.13161479e-02,\n",
       "         2.44718622e-02,  7.94598386e-02,  7.19338581e-02,\n",
       "        -3.85021493e-02, -1.78410602e-03,  5.07675596e-02,\n",
       "        -2.21213941e-02,  9.06589907e-03,  8.09356570e-02,\n",
       "         4.61344514e-03, -5.19063994e-02, -1.73762813e-02,\n",
       "         3.13862488e-02,  8.41894895e-02, -1.03695113e-02,\n",
       "        -2.90777721e-02,  1.27369910e-02, -2.10529435e-02,\n",
       "         3.69437076e-02,  6.29751310e-02, -3.74877825e-02,\n",
       "         2.00642273e-02, -4.05735560e-02,  3.47826257e-02,\n",
       "         7.42863566e-02,  4.78244498e-02, -3.17964214e-03,\n",
       "        -2.77933409e-03,  6.73506483e-02,  3.36953700e-02,\n",
       "        -3.15092169e-02,  7.66347721e-02,  3.49405482e-02,\n",
       "        -4.30069165e-03, -7.18019381e-02, -4.23703110e-03,\n",
       "         4.11998257e-02, -1.78835224e-02,  9.29887034e-03,\n",
       "        -1.12472810e-02,  9.31376926e-05, -3.29853706e-02,\n",
       "        -3.92829068e-02,  4.73451801e-02,  3.75027582e-02,\n",
       "         6.77003665e-03,  5.79524748e-02, -7.59147555e-02,\n",
       "         4.46229316e-02, -1.48486355e-02,  1.72406416e-02,\n",
       "        -1.99831929e-02, -2.85517555e-02, -5.82146756e-02,\n",
       "        -4.99253981e-02,  3.91367115e-02, -8.30149204e-02,\n",
       "         5.61979525e-02,  2.89027747e-02, -6.12150691e-02,\n",
       "         6.45243600e-02,  2.40122285e-02,  5.31685166e-02,\n",
       "        -2.69295182e-02,  8.61917157e-03,  4.62680906e-02,\n",
       "        -1.75368902e-03, -2.48726886e-02, -3.10136937e-02,\n",
       "        -2.41712295e-02,  9.14953053e-02, -9.80458781e-03,\n",
       "         1.36980014e-02,  5.95122464e-02,  7.36016482e-02,\n",
       "         4.71725315e-02, -3.09884548e-02, -1.28833512e-02,\n",
       "        -6.34699035e-03, -1.29180439e-02, -5.29299155e-02,\n",
       "         4.31245528e-02, -8.05313960e-02,  7.65536949e-02,\n",
       "         8.55492577e-02, -7.00644590e-03,  2.09710766e-02,\n",
       "        -2.53276676e-02,  2.63471287e-02, -1.02977231e-02,\n",
       "        -5.26823662e-02,  2.52462947e-03, -5.11072874e-02,\n",
       "        -7.56765604e-02,  4.07145992e-02,  4.13135551e-02,\n",
       "         3.01830657e-02,  4.27376889e-02, -2.18432602e-02,\n",
       "        -6.87060505e-03, -6.42959028e-02,  1.18319625e-02,\n",
       "         5.07112592e-03, -6.13714661e-03, -1.20548643e-02,\n",
       "        -5.02436608e-02, -6.54653739e-03,  6.47010878e-02,\n",
       "         2.45529357e-02,  7.80905485e-02, -3.85899507e-02,\n",
       "         3.90771823e-03, -1.51900286e-02,  1.02813272e-02,\n",
       "         4.51066718e-02, -1.63913239e-02, -9.09261256e-02,\n",
       "        -2.47427188e-02, -8.43563005e-02, -8.66800081e-03,\n",
       "         9.05999262e-03, -6.92357942e-02,  3.01613547e-02,\n",
       "         3.17752361e-02, -6.76256046e-02, -3.75248902e-02,\n",
       "         1.45651232e-02,  6.57137576e-03, -8.87748133e-03,\n",
       "        -5.52256629e-02, -8.44510198e-02,  8.35970938e-02,\n",
       "        -4.80772480e-02, -7.82800540e-02,  7.67615950e-03,\n",
       "         7.56692979e-03, -5.29770888e-02, -1.48717761e-02,\n",
       "         1.97813381e-03,  5.70478803e-03, -1.52726639e-02,\n",
       "         1.52001763e-02, -4.83896630e-03, -5.28665297e-02,\n",
       "        -1.40726864e-02, -5.04911765e-02,  1.04302457e-02,\n",
       "        -2.86712777e-02,  3.73027250e-02, -7.47180879e-02,\n",
       "        -3.19129899e-02,  3.86127494e-02,  5.43224104e-02,\n",
       "        -8.66798237e-02, -2.14494597e-02, -2.09317096e-02,\n",
       "         4.21944074e-02,  5.00622168e-02, -2.35503949e-02,\n",
       "         3.46799269e-02,  3.58216353e-02,  2.87742093e-02,\n",
       "        -5.44508472e-02, -6.82415739e-02,  4.24111709e-02,\n",
       "        -2.26291753e-02,  4.88181263e-02,  5.47379181e-02,\n",
       "         2.54759174e-02, -1.78693589e-02,  7.21281916e-02,\n",
       "         6.32332563e-02, -3.05547249e-02, -2.49126721e-02,\n",
       "         4.70511541e-02,  2.87048072e-02, -5.89202642e-02,\n",
       "         3.36954417e-03,  4.43987362e-02,  2.52380073e-02,\n",
       "         4.75175958e-03,  1.77504290e-02, -4.70503196e-02,\n",
       "        -5.31876087e-02, -4.45192084e-02,  1.57383755e-02,\n",
       "         1.44182164e-02,  4.67208885e-02, -2.13630795e-02,\n",
       "        -3.89128029e-02, -6.24226518e-02,  1.72728971e-02,\n",
       "        -3.97552326e-02,  3.35489586e-03, -6.57012388e-02,\n",
       "         5.46899587e-02, -7.25522190e-02,  2.21962556e-02,\n",
       "        -5.21640815e-02,  5.38990200e-02, -8.01523626e-02,\n",
       "         1.35595417e-02, -5.69758192e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "e_Anth[1].reshape(1,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

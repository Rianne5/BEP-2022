{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_sim(embed, tab_old, result=None, max_iter=1, r_state = 2):\n",
    "    \"\"\"\n",
    "    Wanted statistics?: number of words, similarity, number of pages, percentage of words exactly the same,\n",
    "    in = table\n",
    "    \"\"\" \n",
    "    # for each random row in table, replace=False all rows only sampled once\n",
    "    sample = tab_old.sample(n=max_iter, replace=False, random_state=r_state)\n",
    "    not_sample = tab_old.drop(sample.index)\n",
    "    \n",
    "    #download two papers from table\n",
    "    loc_name_Arxiv = 'pdf sim/Arxiv.pdf'\n",
    "    loc_name_Anth = 'pdf sim/Anth.pdf'\n",
    "    \n",
    "    \n",
    "    sample[['Succes', 'w_Anth', 'w_Arxiv', 'w_both', 'pages_Anth', 'pages_Arxiv', 'cosine', 'len_blocks', 'len_Anth', 'len_Arxiv', 'len_both', 'ref_Anth', 'ref_Arxiv', 'Jaccard' ]] =np.nan\n",
    "    \n",
    "    \n",
    "    for index,series in sample.iterrows():\n",
    "        sample.loc[[index],['Succes']]=0\n",
    "\n",
    "\n",
    "        #open paper in jupiter/blocks\n",
    "        try: #sometimes download not available. Then download works but does not give a good pdf so reading does not work\n",
    "            urllib.request.urlretrieve(series['pdf_url_Arxiv'], loc_name_Arxiv)\n",
    "            urllib.request.urlretrieve(series['url']+'.pdf', loc_name_Anth)\n",
    "            print('start with two papers')\n",
    "            doc_Arxiv = fitz.open(loc_name_Arxiv)\n",
    "            doc_Anth = fitz.open(loc_name_Anth)\n",
    "\n",
    "            b_Anth = get_blocks(doc_Anth)\n",
    "            b_Arxiv = get_blocks(doc_Arxiv)\n",
    "            \n",
    "            #try removing sidebar from arxiv\n",
    "            # if we DO find something\n",
    "            if [i for i, x in enumerate(b_Arxiv) if x.find('arXiv:')>= 0] !=[]:\n",
    "                i_sidebar = [i for i, x in enumerate(b_Arxiv) if x.find('arXiv:')>= 0][0]\n",
    "                if len(b_Arxiv[i_sidebar])< 50: #dont remove references by accident normal is length 41\n",
    "                    x = b_Arxiv.pop(i_sidebar)\n",
    "                \n",
    "#             print('remove_bar skipped')\n",
    "#             remove_bar = doc_Arxiv.load_page(0).get_text(\"text\",clip=rect)\n",
    "#             try:\n",
    "#                 b_Arxiv.remove(remove_bar)\n",
    "#                 print('bar removed')\n",
    "#             except:\n",
    "#                 print('Unable to remove bar from arxiv')\n",
    "            \n",
    "#             elem_both_list = set()\n",
    "            elem_both_list = set(b_Anth)&set(b_Arxiv)\n",
    "            #sometimes same occurs twice in paper then remove this from elem_both_list\n",
    "            dup = [x for x in elem_both_list if b_Anth.count(x)>1]\n",
    "            dup = set(dup + [x for x in elem_both_list if b_Anth.count(x)>1])\n",
    "            for el in dup:\n",
    "                elem_both_list.remove(el)\n",
    "                \n",
    "        \n",
    "            #number of citations\n",
    "            if [i for i, x in enumerate(b_Anth) if x.find('References')>= 0] ==[]:\n",
    "                ref_Anth='NF'\n",
    "                ref_Arxiv='NF'\n",
    "            elif [i for i, x in enumerate(b_Anth) if x.find('References')>= 0] ==[]:\n",
    "                ref_Anth='NF'\n",
    "                ref_Arxiv='NF'\n",
    "            else:\n",
    "                start_ref_Anth = [i for i, x in enumerate(b_Anth) if x.find('References')>= 0][0]\n",
    "                \n",
    "                start_ref_Arxiv = [i for i, x in enumerate(b_Arxiv) if x.find('References')>= 0][0]\n",
    "                \n",
    "#                 ref_Anth = max(sum([x.count('[') for x in b_Anth[start_ref_Anth:]]),len(b_Anth[start_ref_Anth:]))\n",
    "#                 ref_Arxiv = max(sum([x.count('[') for x in b_Anth[start_ref_Arxiv:]]),len(b_Anth[start_ref_Arxiv:]))\n",
    "                \n",
    "                ref_Anth,ref_Arxiv = max([len(b_Anth[start_ref_Anth:]),len(b_Arxiv[start_ref_Arxiv:])],\n",
    "                                         [sum([x.count('[') for x in b_Anth[start_ref_Anth:]]),\n",
    "                                          sum([x.count('[') for x in b_Arxiv[start_ref_Arxiv:]])])\n",
    "                \n",
    "            \n",
    "            #join blocks\n",
    "            p_Anth = join_blocks(b_Anth, elem_both_list)\n",
    "            p_Arxiv = join_blocks(b_Arxiv, elem_both_list)\n",
    "    \n",
    "            #similarity\n",
    "            weight_avg, w_Anth, w_Arxiv, len_blocks, len_Anth, len_Arxiv, jacc = sim(p_Anth,p_Arxiv,embedding=embed)\n",
    "            \n",
    "            # add statistics to table 'sample'\n",
    "            sample.loc[[index],['cosine', 'w_Anth', 'w_Arxiv', 'len_blocks', 'len_Anth', 'len_Arxiv', 'ref_Anth', 'ref_Arxiv', 'Jaccard']]= weight_avg, w_Anth, w_Arxiv, len_blocks, len_Anth, len_Arxiv, ref_Anth, ref_Arxiv, jacc\n",
    "            sample.loc[[index],['w_both','len_both','pages_Anth', 'pages_Arxiv']] = [len(clean_n(\"\".join(elem_both_list)).split()),len(elem_both_list),doc_Anth.page_count, doc_Arxiv.page_count]\n",
    "            \n",
    "            sample.loc[[index],['Succes']]=1\n",
    "            \n",
    "#             #remove papers\n",
    "#             if os.path.exists(\"pdf sim/Anth.pdf\"):\n",
    "#                   os.remove(\"pdf sim/Anth.pdf\")\n",
    "#             else:\n",
    "#                 print(\"The anthology file does not exist\") \n",
    "#             if os.path.exists(\"pdf sim/Arxiv.pdf\"):\n",
    "#                   os.remove(\"pdf sim/Arxiv.pdf\")\n",
    "#             else:\n",
    "#                 print(\"The arxiv file does not exist\") \n",
    "\n",
    "        except: #error with downloading \n",
    "            print('Exception found some error')\n",
    "            next \n",
    "              \n",
    "        time.sleep(3)\n",
    "        \n",
    "    #end loop\n",
    "    # join tables\n",
    "    if type(result) ==pd.DataFrame:\n",
    "        sample = sample.merge(result,how='outer')\n",
    "    return not_sample, sample\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_blocks(doc):\n",
    "    blocks = []\n",
    "    for page in doc.pages():\n",
    "        b_p = page.get_text('blocks')\n",
    "        for b in b_p:\n",
    "            if b[6] == 0: #only text not metadata pictures.\n",
    "                blocks.append(b[4])\n",
    "                \n",
    "    return blocks\n",
    "\n",
    "def join_blocks(blocks, elem_both_list):\n",
    "    \"\"\"Joins same and diff paragraphs until it same comes after difference \n",
    "    then add to list and start new paragraph with the shared block\"\"\"\n",
    "    parag = []\n",
    "    #negative so diff before first same get added to first paragraph\n",
    "    is_diff = -100 #dummy variable to know when to append paragraph. Join same + diff, if same again start new paragraph\n",
    "    join = '' #join outside of loop\n",
    "    for b in blocks:\n",
    "        if b == ' ':\n",
    "            next\n",
    "        if b in elem_both_list: # if block is same\n",
    "            if is_diff >= 1: #last block was different, start new paragraph\n",
    "                parag.append(join)\n",
    "                is_diff = 0\n",
    "                join = b\n",
    "            elif is_diff == 0: #same followed by same, new paragraph and see if diff follows\n",
    "                # is_diff stays 0\n",
    "                parag.append(join)\n",
    "                join = b\n",
    "            else: #  first block of same, not new paragraph \n",
    "                is_diff = 0\n",
    "                join+=b\n",
    "#                 join+=' '\n",
    "                # is_diff is 0 and stays 0\n",
    "        else: #block is different to the other paper\n",
    "            is_diff+= 1 \n",
    "            join+=' '\n",
    "            join+=b\n",
    "            \n",
    "    #add last paragraph to list named parag since it has not been added\n",
    "    parag.append(join)\n",
    "            \n",
    "    return parag\n",
    "\n",
    "def join_diff(blocks, elem_both_list):\n",
    "    \"\"\"Only join different blocks not same ones\"\"\"\n",
    "    parag = []\n",
    "    is_diff = 0 #dummy variable to know when to append paragraph. Join same + diff, if same again start new paragraph\n",
    "    join = ''\n",
    "    for b in blocks:\n",
    "        if b in elem_both_list: # same: if not empty add, else go to next\n",
    "            if is_diff==1:\n",
    "                parag.append(join)\n",
    "                join = ''\n",
    "                is_diff=0\n",
    "            else:next               \n",
    "        else: # not in elem list\n",
    "            join+=b #add element to join string\n",
    "            is_diff=1\n",
    "    parag.append(join)\n",
    "            \n",
    "    return parag\n",
    "    \n",
    "def sim(anth, arxiv, embedding):\n",
    "#     print('start sim')\n",
    "#     print('Length Anthology: {}\\nLength Arxiv:{}'.format(len(anth),len(arxiv)))\n",
    "    len_Anth = len(anth)\n",
    "    len_Arxiv = len(arxiv)\n",
    "    \n",
    "    set_Anth = set(clean_n(\" \".join(anth)).split())\n",
    "    set_Arxiv = set(clean_n(\" \".join(arxiv)).split())\n",
    "    \n",
    "    # jacc = Intersection/Union\n",
    "    jacc = len(set_Anth & set_Arxiv)/len(set_Anth | set_Arxiv)\n",
    "    \n",
    "    \n",
    "    #action if not same length make it one big string\n",
    "    if len(anth) != len(arxiv):\n",
    "        u_anth = \" \".join(anth)\n",
    "        u_arxiv = \" \".join(arxiv)\n",
    "        #could be just length of words??\n",
    "#         print('Updated length Anthology: {}\\nUpdated length Arxiv:{}'.format(len(u_anth),len(u_arxiv)))\n",
    "        df = pd.DataFrame()\n",
    "        df['Anth'] = [u_anth]\n",
    "        df['Arxiv'] = [u_arxiv]\n",
    "        len_blocks = 1\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "        df['Anth'] = anth\n",
    "        df['Arxiv'] = arxiv\n",
    "        len_blocks= len(anth)\n",
    "\n",
    "    df['Anth'] = df['Anth'].apply(clean_n)\n",
    "    df['Arxiv'] = df['Arxiv'].apply(clean_n)\n",
    "#     print(df)\n",
    "    \n",
    "    df['words Anth'] = df['Anth'].apply(lambda x: len(x.split()))\n",
    "    df['words Arxiv'] = df['Arxiv'].apply(lambda x: len(x.split()))\n",
    "#     print('start cosin')\n",
    "    df['cosine'] = df.apply(lambda x:cosine_similarity(embedding([x.Anth]),embedding([x.Arxiv])), axis=1)\n",
    "    df['cosine'] = df['cosine'].astype(float)\n",
    "\n",
    "    #nr of words per paper\n",
    "    w_Anth = sum(df['words Anth'])\n",
    "    w_Arxiv = sum(df['words Arxiv'])\n",
    "    \n",
    "    df['weight'] = df['words Anth']+df['words Arxiv']\n",
    "    \n",
    "    df2 = df[df['words Anth']>50] #shorter pieces are often just headings, \n",
    "    \n",
    "    weight_avg = round(np.average( df2['cosine'], weights = df2['weight']),6)\n",
    "        \n",
    "#     return  df, weight_avg #possible to evaluate to blocks and weighted average.\n",
    "#     print(weight_avg, w_Anth, w_Arxiv, len_blocks, len_Anth, len_Arxiv)\n",
    "    \n",
    "    return weight_avg, w_Anth, w_Arxiv, len_blocks, len_Anth, len_Arxiv, jacc  # or return sim score and len words and diff words.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
